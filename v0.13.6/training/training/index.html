<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Training · Flux</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-36890222-9', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/flux.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="Flux logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">Flux</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Building Models</span><ul><li><a class="tocitem" href="../../models/overview/">Overview</a></li><li><a class="tocitem" href="../../models/basics/">Basics</a></li><li><a class="tocitem" href="../../models/recurrence/">Recurrence</a></li><li><a class="tocitem" href="../../models/layers/">Model Reference</a></li><li><a class="tocitem" href="../../models/losses/">Loss Functions</a></li><li><a class="tocitem" href="../../models/regularisation/">Regularisation</a></li><li><a class="tocitem" href="../../models/advanced/">Advanced Model Building</a></li><li><a class="tocitem" href="../../models/nnlib/">Neural Network primitives from NNlib.jl</a></li><li><a class="tocitem" href="../../models/functors/">Recursive transformations from Functors.jl</a></li></ul></li><li><span class="tocitem">Handling Data</span><ul><li><a class="tocitem" href="../../data/onehot/">One-Hot Encoding with OneHotArrays.jl</a></li><li><a class="tocitem" href="../../data/mlutils/">Working with data using MLUtils.jl</a></li></ul></li><li><span class="tocitem">Training Models</span><ul><li><a class="tocitem" href="../optimisers/">Optimisers</a></li><li class="is-active"><a class="tocitem" href>Training</a><ul class="internal"><li><a class="tocitem" href="#Loss-Functions"><span>Loss Functions</span></a></li><li><a class="tocitem" href="#Model-parameters"><span>Model parameters</span></a></li><li><a class="tocitem" href="#Datasets"><span>Datasets</span></a></li><li><a class="tocitem" href="#Callbacks"><span>Callbacks</span></a></li><li><a class="tocitem" href="#Custom-Training-loops"><span>Custom Training loops</span></a></li></ul></li></ul></li><li><a class="tocitem" href="../../gpu/">GPU Support</a></li><li><a class="tocitem" href="../../saving/">Saving &amp; Loading</a></li><li><a class="tocitem" href="../../ecosystem/">The Julia Ecosystem</a></li><li><a class="tocitem" href="../../utilities/">Utility Functions</a></li><li><a class="tocitem" href="../../performance/">Performance Tips</a></li><li><a class="tocitem" href="../../datasets/">Datasets</a></li><li><a class="tocitem" href="../../community/">Community</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Training Models</a></li><li class="is-active"><a href>Training</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Training</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/FluxML/Flux.jl/blob/master/docs/src/training/training.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h1><p>To actually train a model we need four things:</p><ul><li>A <em>objective function</em>, that evaluates how well a model is doing given some input data.</li><li>The trainable parameters of the model.</li><li>A collection of data points that will be provided to the objective function.</li><li>An <a href="../optimisers/">optimiser</a> that will update the model parameters appropriately.</li></ul><p>Training a model is typically an iterative process, where we go over the data set, calculate the objective function over the data points, and optimise that. This can be visualised in the form of a simple loop.</p><pre><code class="language-julia hljs">for d in datapoints

  # `d` should produce a collection of arguments
  # to the loss function

  # Calculate the gradients of the parameters
  # with respect to the loss function
  grads = Flux.gradient(parameters) do
    loss(d...)
  end

  # Update the parameters based on the chosen
  # optimiser (opt)
  Flux.Optimise.update!(opt, parameters, grads)
end</code></pre><p>To make it easy, Flux defines <code>train!</code>:</p><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.train!" href="#Flux.Optimise.train!"><code>Flux.Optimise.train!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">train!(loss, pars::Params, data, opt::AbstractOptimiser; [cb])</code></pre><p>Uses a <code>loss</code> function and training <code>data</code> to improve the  model&#39;s parameters according to a particular optimisation rule <code>opt</code>.</p><p>For each <code>d in data</code>, first the gradient of the <code>loss</code> is computed like this:</p><pre><code class="nohighlight hljs">    gradient(() -&gt; loss(d...), pars)  # if d isa Tuple
    gradient(() -&gt; loss(d), pars)     # otherwise</code></pre><p>Here <code>pars</code> is produced by calling <a href="#Flux.params"><code>Flux.params</code></a> on your model. (Or just on the layers you want to train, like <code>train!(loss, params(model[1:end-2]), data, opt)</code>.) This is the &quot;implicit&quot; style of parameter handling.</p><p>This gradient is then used by optimizer <code>opt</code> to update the parameters:</p><pre><code class="nohighlight hljs">    update!(opt, pars, grads)</code></pre><p>The optimiser should be from the <code>Flux.Optimise</code> module (see <a href="../optimisers/#Optimisers">Optimisers</a>). Different optimisers can be combined using <a href="../optimisers/#Flux.Optimise.Optimiser"><code>Flux.Optimise.Optimiser</code></a>.</p><p>This training loop iterates through <code>data</code> once. You can use <a href="#Flux.Optimise.@epochs"><code>@epochs</code></a> to do this several times, or  use for instance <code>Iterators.repeat</code> to make a longer <code>data</code> iterator.</p><p><strong>Callbacks</strong></p><p><a href="#Callbacks">Callbacks</a> are given with the keyword argument <code>cb</code>. For example, this will print &quot;training&quot; every 10 seconds (using <a href="../../utilities/#Flux.throttle"><code>Flux.throttle</code></a>):</p><pre><code class="nohighlight hljs">    train!(loss, params, data, opt, cb = throttle(() -&gt; println(&quot;training&quot;), 10))</code></pre><p>The callback can call <a href="../../utilities/#Flux.Optimise.stop"><code>Flux.stop</code></a> to interrupt the training loop.</p><p>Multiple callbacks can be passed to <code>cb</code> as array.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/8bc0c35932c4a871ac73b42e39146cd9bbb1d446/src/optimise/train.jl#L85-L122">source</a></section></article><p>There are plenty of examples in the <a href="https://github.com/FluxML/model-zoo">model zoo</a>, and more information can be found on <a href="../../models/advanced/">Custom Training Loops</a>.</p><h2 id="Loss-Functions"><a class="docs-heading-anchor" href="#Loss-Functions">Loss Functions</a><a id="Loss-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-Functions" title="Permalink"></a></h2><p>The objective function must return a number representing how far the model is from its target – the <em>loss</em> of the model. The <code>loss</code> function that we defined in <a href="../../models/basics/">basics</a> will work as an objective. In addition to custom losses, a model can be trained in conjunction with the commonly used losses that are grouped under the <code>Flux.Losses</code> module. We can also define an objective in terms of some model:</p><pre><code class="language-julia hljs">m = Chain(
  Dense(784 =&gt; 32, σ),
  Dense(32 =&gt; 10), softmax)

loss(x, y) = Flux.Losses.mse(m(x), y)
ps = Flux.params(m)

# later
Flux.train!(loss, ps, data, opt)</code></pre><p>The objective will almost always be defined in terms of some <em>cost function</em> that measures the distance of the prediction <code>m(x)</code> from the target <code>y</code>. Flux has several of these built-in, like <code>mse</code> for mean squared error or <code>crossentropy</code> for cross-entropy loss, but you can calculate it however you want. For a list of all built-in loss functions, check out the <a href="../../models/losses/">losses reference</a>.</p><p>At first glance, it may seem strange that the model that we want to train is not part of the input arguments of <code>Flux.train!</code> too. However the target of the optimizer is not the model itself, but the objective function that represents the departure between modelled and observed data. In other words, the model is implicitly defined in the objective function, and there is no need to give it explicitly. Passing the objective function instead of the model and a cost function separately provides more flexibility and the possibility of optimizing the calculations.</p><h2 id="Model-parameters"><a class="docs-heading-anchor" href="#Model-parameters">Model parameters</a><a id="Model-parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Model-parameters" title="Permalink"></a></h2><p>The model to be trained must have a set of tracked parameters that are used to calculate the gradients of the objective function. In the <a href="../../models/basics/">basics</a> section it is explained how to create models with such parameters. The second argument of the function <code>Flux.train!</code> must be an object containing those parameters, which can be obtained from a model <code>m</code> as <code>Flux.params(m)</code>.</p><p>Such an object contains a reference to the model&#39;s parameters, not a copy, such that after their training, the model behaves according to their updated values.</p><p>Handling all the parameters on a layer-by-layer basis is explained in the <a href="../../models/basics/">Layer Helpers</a> section. For freezing model parameters, see the <a href="../../models/advanced/">Advanced Usage Guide</a>.</p><article class="docstring"><header><a class="docstring-binding" id="Flux.params" href="#Flux.params"><code>Flux.params</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">params(model)
params(layers...)</code></pre><p>Given a model or specific layers from a model, create a <code>Params</code> object pointing to its trainable parameters.</p><p>This can be used with the <code>gradient</code> function, see <a href="../../models/basics/#Taking-Gradients">Taking Gradients</a>, or as input to the <a href="#Flux.Optimise.train!"><code>Flux.train!</code></a> function.</p><p>The behaviour of <code>params</code> on custom types can be customized using <a href="../../models/functors/#Functors.@functor"><code>Functors.@functor</code></a> or <a href="../optimisers/#Optimisers.trainable"><code>Flux.trainable</code></a>.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using Flux: params

julia&gt; params(Chain(Dense(ones(2,3)), softmax))  # unpacks Flux models
Params([[1.0 1.0 1.0; 1.0 1.0 1.0], [0.0, 0.0]])

julia&gt; bn = BatchNorm(2, relu)
BatchNorm(2, relu)  # 4 parameters, plus 4 non-trainable

julia&gt; params(bn)  # only the trainable parameters
Params([Float32[0.0, 0.0], Float32[1.0, 1.0]])

julia&gt; params([1, 2, 3], [4])  # one or more arrays of numbers
Params([[1, 2, 3], [4]])

julia&gt; params([[1, 2, 3], [4]])  # unpacks array of arrays
Params([[1, 2, 3], [4]])

julia&gt; params(1, [2 2], (alpha=[3,3,3], beta=Ref(4), gamma=sin))  # ignores scalars, unpacks NamedTuples
Params([[2 2], [3, 3, 3]])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/8bc0c35932c4a871ac73b42e39146cd9bbb1d446/src/functor.jl#L52-L84">source</a></section></article><h2 id="Datasets"><a class="docs-heading-anchor" href="#Datasets">Datasets</a><a id="Datasets-1"></a><a class="docs-heading-anchor-permalink" href="#Datasets" title="Permalink"></a></h2><p>The <code>data</code> argument of <code>train!</code> provides a collection of data to train with (usually a set of inputs <code>x</code> and target outputs <code>y</code>). For example, here&#39;s a dummy dataset with only one data point:</p><pre><code class="language-julia hljs">x = rand(784)
y = rand(10)
data = [(x, y)]</code></pre><p><code>Flux.train!</code> will call <code>loss(x, y)</code>, calculate gradients, update the weights and then move on to the next data point if there is one. We can train the model on the same data three times:</p><pre><code class="language-julia hljs">data = [(x, y), (x, y), (x, y)]
# Or equivalently
using IterTools: ncycle
data = ncycle([(x, y)], 3)</code></pre><p>It&#39;s common to load the <code>x</code>s and <code>y</code>s separately. Here you can use <code>zip</code>:</p><pre><code class="language-julia hljs">xs = [rand(784), rand(784), rand(784)]
ys = [rand( 10), rand( 10), rand( 10)]
data = zip(xs, ys)</code></pre><p>Training data can be conveniently  partitioned for mini-batch training using the <a href="../../data/mlutils/#MLUtils.DataLoader"><code>Flux.Data.DataLoader</code></a> type:</p><pre><code class="language-julia hljs">X = rand(28, 28, 60000)
Y = rand(0:9, 60000)
data = DataLoader((X, Y), batchsize=128) </code></pre><p>Note that, by default, <code>train!</code> only loops over the data once (a single &quot;epoch&quot;). A convenient way to run multiple epochs from the REPL is provided by <code>@epochs</code>.</p><pre><code class="language-julia hljs">julia&gt; using Flux: @epochs

julia&gt; @epochs 2 println(&quot;hello&quot;)
[ Info: Epoch 1
hello
[ Info: Epoch 2
hello

julia&gt; @epochs 2 Flux.train!(...)
# Train for two epochs</code></pre><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.@epochs" href="#Flux.Optimise.@epochs"><code>Flux.Optimise.@epochs</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia hljs">@epochs N body</code></pre><p>Run <code>body</code> <code>N</code> times. Mainly useful for quickly doing multiple epochs of training in a REPL.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>The macro <code>@epochs</code> will be removed from Flux 0.14. Please just write an ordinary <code>for</code> loop.</p></div></div><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; Flux.@epochs 2 println(&quot;hello&quot;)
[ Info: Epoch 1
hello
[ Info: Epoch 2
hello</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/8bc0c35932c4a871ac73b42e39146cd9bbb1d446/src/optimise/train.jl#L147-L164">source</a></section></article><h2 id="Callbacks"><a class="docs-heading-anchor" href="#Callbacks">Callbacks</a><a id="Callbacks-1"></a><a class="docs-heading-anchor-permalink" href="#Callbacks" title="Permalink"></a></h2><p><code>train!</code> takes an additional argument, <code>cb</code>, that&#39;s used for callbacks so that you can observe the training process. For example:</p><pre><code class="language-julia hljs">train!(objective, ps, data, opt, cb = () -&gt; println(&quot;training&quot;))</code></pre><p>Callbacks are called for every batch of training data. You can slow this down using <code>Flux.throttle(f, timeout)</code> which prevents <code>f</code> from being called more than once every <code>timeout</code> seconds.</p><p>A more typical callback might look like this:</p><pre><code class="language-julia hljs">test_x, test_y = # ... create single batch of test data ...
evalcb() = @show(loss(test_x, test_y))
throttled_cb = throttle(evalcb, 5)
Flux.@epochs 20 Flux.train!(objective, ps, data, opt, cb = throttled_cb)</code></pre><p>Calling <code>Flux.stop()</code> in a callback will exit the training loop early.</p><pre><code class="language-julia hljs">cb = function ()
  accuracy() &gt; 0.9 &amp;&amp; Flux.stop()
end</code></pre><h2 id="Custom-Training-loops"><a class="docs-heading-anchor" href="#Custom-Training-loops">Custom Training loops</a><a id="Custom-Training-loops-1"></a><a class="docs-heading-anchor-permalink" href="#Custom-Training-loops" title="Permalink"></a></h2><p>The <code>Flux.train!</code> function can be very convenient, especially for simple problems. For some problems, however, it&#39;s much cleaner to write your own custom training loop. An example follows that works similar to the default <code>Flux.train</code> but with no callbacks. You don&#39;t need callbacks if you just code the calls to your functions directly into the loop. E.g. in the places marked with comments.</p><pre><code class="language-julia hljs">function my_custom_train!(loss, ps, data, opt)
  # training_loss is declared local so it will be available for logging outside the gradient calculation.
  local training_loss
  ps = Params(ps)
  for d in data
    gs = gradient(ps) do
      training_loss = loss(d...)
      # Code inserted here will be differentiated, unless you need that gradient information
      # it is better to do the work outside this block.
      return training_loss
    end
    # Insert whatever code you want here that needs training_loss, e.g. logging.
    # logging_callback(training_loss)
    # Insert whatever code you want here that needs gradients.
    # e.g. logging histograms with TensorBoardLogger.jl to check for exploding gradients.
    update!(opt, ps, gs)
    # Here you might like to check validation set accuracy, and break out to do early stopping.
  end
end</code></pre><p>You could simplify this further, for example by hard-coding in the loss function.</p><p>Another possibility is to use <a href="https://fluxml.ai/Zygote.jl/dev/adjoints/#Pullbacks-1"><code>Zygote.pullback</code></a> to access the training loss and the gradient simultaneously.</p><pre><code class="language-julia hljs">function my_custom_train!(loss, ps, data, opt)
  ps = Params(ps)
  for d in data
    # back is a method that computes the product of the gradient so far with its argument.
    train_loss, back = Zygote.pullback(() -&gt; loss(d...), ps)
    # Insert whatever code you want here that needs training_loss, e.g. logging.
    # logging_callback(training_loss)
    # Apply back() to the correct type of 1.0 to get the gradient of loss.
    gs = back(one(train_loss))
    # Insert whatever code you want here that needs gradient.
    # E.g. logging with TensorBoardLogger.jl as histogram so you can see if it is becoming huge.
    update!(opt, ps, gs)
    # Here you might like to check validation set accuracy, and break out to do early stopping.
  end
end</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../optimisers/">« Optimisers</a><a class="docs-footer-nextpage" href="../../gpu/">GPU Support »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Tuesday 13 September 2022 14:11">Tuesday 13 September 2022</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
