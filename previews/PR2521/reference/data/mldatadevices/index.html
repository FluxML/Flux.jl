<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Transfer Data to GPU – MLDataDevices.jl · Flux</title><meta name="title" content="Transfer Data to GPU – MLDataDevices.jl · Flux"/><meta property="og:title" content="Transfer Data to GPU – MLDataDevices.jl · Flux"/><meta property="twitter:title" content="Transfer Data to GPU – MLDataDevices.jl · Flux"/><meta name="description" content="Documentation for Flux."/><meta property="og:description" content="Documentation for Flux."/><meta property="twitter:description" content="Documentation for Flux."/><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-36890222-9', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../../assets/documenter.js"></script><script src="../../../search_index.js"></script><script src="../../../siteinfo.js"></script><script src="../../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../../assets/themeswap.js"></script><link href="../../../assets/flux.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../../"><img class="docs-light-only" src="../../../assets/logo.png" alt="Flux logo"/><img class="docs-dark-only" src="../../../assets/logo-dark.png" alt="Flux logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../../">Welcome</a></li><li><span class="tocitem">Guide</span><ul><li><a class="tocitem" href="../../../guide/models/quickstart/">Quick Start</a></li><li><a class="tocitem" href="../../../guide/models/overview/">Fitting a Line</a></li><li><a class="tocitem" href="../../../guide/models/basics/">Gradients and Layers</a></li><li><a class="tocitem" href="../../../guide/models/custom_layers/">Custom Layers</a></li><li><a class="tocitem" href="../../../guide/training/training/">Training</a></li><li><a class="tocitem" href="../../../guide/models/recurrence/">Recurrence</a></li><li><a class="tocitem" href="../../../guide/gpu/">GPU Support</a></li><li><a class="tocitem" href="../../../guide/saving/">Saving &amp; Loading</a></li><li><a class="tocitem" href="../../../guide/performance/">Performance Tips</a></li></ul></li><li><a class="tocitem" href="../../../ecosystem/">Ecosystem</a></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../../models/layers/">Built-in Layers</a></li><li><a class="tocitem" href="../../models/activation/">Activation Functions</a></li><li><a class="tocitem" href="../../utilities/">Weight Initialisation</a></li><li><a class="tocitem" href="../../models/losses/">Loss Functions</a></li><li><a class="tocitem" href="../../training/reference/">Training API</a></li><li><a class="tocitem" href="../../training/optimisers/">Optimisation Rules</a></li><li><a class="tocitem" href="../../outputsize/">Shape Inference</a></li><li><a class="tocitem" href="../../destructure/">Flat vs. Nested</a></li><li><a class="tocitem" href="../../training/callbacks/">Callback Helpers</a></li><li><a class="tocitem" href="../../training/zygote/">Gradients – Zygote.jl</a></li><li class="is-active"><a class="tocitem" href>Transfer Data to GPU – MLDataDevices.jl</a></li><li><a class="tocitem" href="../mlutils/">Batching Data – MLUtils.jl</a></li><li><a class="tocitem" href="../onehot/">OneHotArrays.jl</a></li><li><a class="tocitem" href="../../models/nnlib/">Low-level Operations – NNlib.jl</a></li><li><a class="tocitem" href="../../models/functors/">Nested Structures – Functors.jl</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../../tutorials/linear_regression/">Linear Regression</a></li><li><a class="tocitem" href="../../../tutorials/logistic_regression/">Logistic Regression</a></li><li><a class="tocitem" href="../../../tutorials/model_zoo/">Model Zoo</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Reference</a></li><li class="is-active"><a href>Transfer Data to GPU – MLDataDevices.jl</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Transfer Data to GPU – MLDataDevices.jl</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/FluxML/Flux.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/FluxML/Flux.jl/blob/master/docs/src/reference/data/mldatadevices.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Transferring-data-across-devices"><a class="docs-heading-anchor" href="#Transferring-data-across-devices">Transferring data across devices</a><a id="Transferring-data-across-devices-1"></a><a class="docs-heading-anchor-permalink" href="#Transferring-data-across-devices" title="Permalink"></a></h1><p>Flux relies on the <a href="https://github.com/LuxDL/MLDataDevices.jl/blob/main/src/public.jl">MLDataDevices.jl</a> package to manage devices and transfer data across them. You don&#39;t have to explicitly use the package, as Flux re-exports the necessary functions and types.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLDataDevices.cpu_device" href="#MLDataDevices.cpu_device"><code>MLDataDevices.cpu_device</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">cpu_device() -&gt; CPUDevice()</code></pre><p>Return a <code>CPUDevice</code> object which can be used to transfer data to CPU.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/LuxDL/Lux.jl/blob/v1.5.0/src/public.jl#L205-L209">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLDataDevices.default_device_rng" href="#MLDataDevices.default_device_rng"><code>MLDataDevices.default_device_rng</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">default_device_rng(::AbstractDevice)</code></pre><p>Returns the default RNG for the device. This can be used to directly generate parameters and states on the device using <a href="https://github.com/LuxDL/WeightInitializers.jl">WeightInitializers.jl</a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/LuxDL/Lux.jl/blob/v1.5.0/src/public.jl#L241-L247">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLDataDevices.functional" href="#MLDataDevices.functional"><code>MLDataDevices.functional</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">functional(x::AbstractDevice) -&gt; Bool
functional(::Type{&lt;:AbstractDevice}) -&gt; Bool</code></pre><p>Checks if the device is functional. This is used to determine if the device can be used for computation. Note that even if the backend is loaded (as checked via <a href="#MLDataDevices.loaded"><code>MLDataDevices.loaded</code></a>), the device may not be functional.</p><p>Note that while this function is not exported, it is considered part of the public API.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/LuxDL/Lux.jl/blob/v1.5.0/src/public.jl#L39-L48">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLDataDevices.get_device" href="#MLDataDevices.get_device"><code>MLDataDevices.get_device</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">get_device(x) -&gt; dev::AbstractDevice | Exception | Nothing</code></pre><p>If all arrays (on the leaves of the structure) are on the same device, we return that device. Otherwise, we throw an error. If the object is device agnostic, we return <code>nothing</code>.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Trigger Packages must be loaded for this to return the correct device.</p></div></div><p><strong>Special Retuened Values</strong></p><ul><li><code>nothing</code> – denotes that the object is device agnostic. For example, scalar, abstract range, etc.</li><li><code>UnknownDevice()</code> – denotes that the device type is unknown</li></ul><p>See also <a href="#MLDataDevices.get_device_type"><code>get_device_type</code></a> for a faster alternative that can be used for dispatch based on device type.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/LuxDL/Lux.jl/blob/v1.5.0/src/public.jl#L266-L282">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLDataDevices.gpu_device" href="#MLDataDevices.gpu_device"><code>MLDataDevices.gpu_device</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">gpu_device(device_id::Union{Nothing, Integer}=nothing;
    force::Bool=false) -&gt; AbstractDevice</code></pre><p>Selects GPU device based on the following criteria:</p><ol><li>If <code>gpu_backend</code> preference is set and the backend is functional on the system, then that device is selected.</li><li>Otherwise, an automatic selection algorithm is used. We go over possible device backends in the order specified by <code>supported_gpu_backends()</code> and select the first functional backend.</li><li>If no GPU device is functional and  <code>force</code> is <code>false</code>, then <code>cpu_device()</code> is invoked.</li><li>If nothing works, an error is thrown.</li></ol><p><strong>Arguments</strong></p><ul><li><code>device_id::Union{Nothing, Integer}</code>: The device id to select. If <code>nothing</code>, then we return the last selected device or if none was selected then we run the autoselection and choose the current device using <code>CUDA.device()</code> or <code>AMDGPU.device()</code> or similar. If <code>Integer</code>, then we select the device with the given id. Note that this is <code>1</code>-indexed, in contrast to the <code>0</code>-indexed <code>CUDA.jl</code>. For example, <code>id = 4</code> corresponds to <code>CUDA.device!(3)</code>.</li></ul><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p><code>device_id</code> is only applicable for <code>CUDA</code> and <code>AMDGPU</code> backends. For <code>Metal</code>, <code>oneAPI</code> and <code>CPU</code> backends, <code>device_id</code> is ignored and a warning is printed.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p><code>gpu_device</code> won&#39;t select a CUDA device unless both CUDA.jl and cuDNN.jl are loaded. This is to ensure that deep learning operations work correctly. Nonetheless, if cuDNN is not loaded you can still manually create a <code>CUDADevice</code> object and use it (e.g. <code>dev = CUDADevice()</code>).</p></div></div><p><strong>Keyword Arguments</strong></p><ul><li><code>force::Bool</code>: If <code>true</code>, then an error is thrown if no functional GPU device is found.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/LuxDL/Lux.jl/blob/v1.5.0/src/public.jl#L91-L131">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLDataDevices.gpu_backend!" href="#MLDataDevices.gpu_backend!"><code>MLDataDevices.gpu_backend!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">gpu_backend!() = gpu_backend!(&quot;&quot;)
gpu_backend!(backend) = gpu_backend!(string(backend))
gpu_backend!(backend::AbstractGPUDevice)
gpu_backend!(backend::String)</code></pre><p>Creates a <code>LocalPreferences.toml</code> file with the desired GPU backend.</p><p>If <code>backend == &quot;&quot;</code>, then the <code>gpu_backend</code> preference is deleted. Otherwise, <code>backend</code> is validated to be one of the possible backends and the preference is set to <code>backend</code>.</p><p>If a new backend is successfully set, then the Julia session must be restarted for the change to take effect.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/LuxDL/Lux.jl/blob/v1.5.0/src/public.jl#L163-L176">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLDataDevices.get_device_type" href="#MLDataDevices.get_device_type"><code>MLDataDevices.get_device_type</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">get_device_type(x) -&gt; Type{&lt;:AbstractDevice} | Exception | Type{Nothing}</code></pre><p>Similar to <a href="#MLDataDevices.get_device"><code>get_device</code></a> but returns the type of the device instead of the device itself. This value is often a compile time constant and is recommended to be used instead of <a href="#MLDataDevices.get_device"><code>get_device</code></a> where ever defining dispatches based on the device type.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Trigger Packages must be loaded for this to return the correct device.</p></div></div><p><strong>Special Retuened Values</strong></p><ul><li><code>Nothing</code> – denotes that the object is device agnostic. For example, scalar, abstract range, etc.</li><li><code>UnknownDevice</code> – denotes that the device type is unknown</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/LuxDL/Lux.jl/blob/v1.5.0/src/public.jl#L285-L299">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLDataDevices.isleaf" href="#MLDataDevices.isleaf"><code>MLDataDevices.isleaf</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">isleaf(x) -&gt; Bool</code></pre><p>Returns <code>true</code> if <code>x</code> is a leaf node in the data structure.</p><p>Defining <code>MLDataDevices.isleaf(x::T) = true</code> for custom types can be used to customize the behavior the data movement behavior when an object with nested structure containing the type is transferred to a device.</p><p><code>Adapt.adapt_structure(::AbstractDevice, x::T)</code> or <code>Adapt.adapt_structure(::AbstractDevice, x::T)</code> will be called during data movement if <code>isleaf(x::T) == true</code>.</p><p>If <code>MLDataDevices.isleaf(x::T)</code> is not defined, then it will fall back to <code>Functors.isleaf(x)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/LuxDL/Lux.jl/blob/v1.5.0/src/public.jl#L406-L420">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLDataDevices.loaded" href="#MLDataDevices.loaded"><code>MLDataDevices.loaded</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">loaded(x::AbstractDevice) -&gt; Bool
loaded(::Type{&lt;:AbstractDevice}) -&gt; Bool</code></pre><p>Checks if the trigger package for the device is loaded. Trigger packages are as follows:</p><ul><li><code>CUDA.jl</code> and <code>cuDNN.jl</code> (or just <code>LuxCUDA.jl</code>) for NVIDIA CUDA Support.</li><li><code>AMDGPU.jl</code> for AMD GPU ROCM Support.</li><li><code>Metal.jl</code> for Apple Metal GPU Support.</li><li><code>oneAPI.jl</code> for Intel oneAPI GPU Support.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/LuxDL/Lux.jl/blob/v1.5.0/src/public.jl#L52-L62">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLDataDevices.reset_gpu_device!" href="#MLDataDevices.reset_gpu_device!"><code>MLDataDevices.reset_gpu_device!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">reset_gpu_device!()</code></pre><p>Resets the selected GPU device. This is useful when automatic GPU selection needs to be run again.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/LuxDL/Lux.jl/blob/v1.5.0/src/public.jl#L71-L76">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLDataDevices.set_device!" href="#MLDataDevices.set_device!"><code>MLDataDevices.set_device!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">set_device!(T::Type{&lt;:AbstractDevice}, dev_or_id)</code></pre><p>Set the device for the given type. This is a no-op for <code>CPUDevice</code>. For <code>CUDADevice</code> and <code>AMDGPUDevice</code>, it prints a warning if the corresponding trigger package is not loaded.</p><p>Currently, <code>MetalDevice</code> and <code>oneAPIDevice</code> don&#39;t support setting the device.</p><p><strong>Arguments</strong></p><ul><li><code>T::Type{&lt;:AbstractDevice}</code>: The device type to set.</li><li><code>dev_or_id</code>: Can be the device from the corresponding package. For example for CUDA it can be a <code>CuDevice</code>. If it is an integer, it is the device id to set. This is <code>1</code>-indexed.</li></ul><div class="admonition is-danger"><header class="admonition-header">Danger</header><div class="admonition-body"><p>This specific function should be considered experimental at this point and is currently provided to support distributed training in Lux. As such please use <code>Lux.DistributedUtils</code> instead of using this function.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/LuxDL/Lux.jl/blob/v1.5.0/src/public.jl#L319-L332">source</a></section><section><div><pre><code class="language-julia hljs">set_device!(T::Type{&lt;:AbstractDevice}, ::Nothing, rank::Integer)</code></pre><p>Set the device for the given type. This is a no-op for <code>CPUDevice</code>. For <code>CUDADevice</code> and <code>AMDGPUDevice</code>, it prints a warning if the corresponding trigger package is not loaded.</p><p>Currently, <code>MetalDevice</code> and <code>oneAPIDevice</code> don&#39;t support setting the device.</p><p><strong>Arguments</strong></p><ul><li><code>T::Type{&lt;:AbstractDevice}</code>: The device type to set.</li><li><code>rank::Integer</code>: Local Rank of the process. This is applicable for distributed training and must be <code>0</code>-indexed.</li></ul><div class="admonition is-danger"><header class="admonition-header">Danger</header><div class="admonition-body"><p>This specific function should be considered experimental at this point and is currently provided to support distributed training in Lux. As such please use <code>Lux.DistributedUtils</code> instead of using this function.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/LuxDL/Lux.jl/blob/v1.5.0/src/public.jl#L348-L360">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLDataDevices.supported_gpu_backends" href="#MLDataDevices.supported_gpu_backends"><code>MLDataDevices.supported_gpu_backends</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">supported_gpu_backends() -&gt; Tuple{String, ...}</code></pre><p>Return a tuple of supported GPU backends.</p><div class="admonition is-warning"><header class="admonition-header">Warning</header><div class="admonition-body"><p>This is not the list of functional backends on the system, but rather backends which <code>MLDataDevices.jl</code> supports.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/LuxDL/Lux.jl/blob/v1.5.0/src/public.jl#L79-L88">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="MLDataDevices.DeviceIterator" href="#MLDataDevices.DeviceIterator"><code>MLDataDevices.DeviceIterator</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">DeviceIterator(dev::AbstractDevice, iterator)</code></pre><p>Create a <code>DeviceIterator</code> that iterates through the provided <code>iterator</code> via <code>iterate</code>. Upon each iteration, the current batch is copied to the device <code>dev</code>, and the previous iteration is marked as freeable from GPU memory (via <code>unsafe_free!</code>) (no-op for a CPU device).</p><p>The conversion follows the same semantics as <code>dev(&lt;item from iterator&gt;)</code>.</p><div class="admonition is-success"><header class="admonition-header">Similarity to `CUDA.CuIterator`</header><div class="admonition-body"><p>The design inspiration was taken from <code>CUDA.CuIterator</code> and was generalized to work with other backends and more complex iterators (using <code>Functors</code>).</p></div></div><div class="admonition is-success"><header class="admonition-header">`MLUtils.DataLoader`</header><div class="admonition-body"><p>Calling <code>dev(::MLUtils.DataLoader)</code> will automatically convert the dataloader to use the same semantics as <code>DeviceIterator</code>. This is generally preferred over looping over the dataloader directly and transferring the data to the device.</p></div></div><p><strong>Examples</strong></p><p>The following was run on a computer with an NVIDIA GPU.</p><pre><code class="language-julia-repl hljs">julia&gt; using MLDataDevices, MLUtils

julia&gt; X = rand(Float64, 3, 33);

julia&gt; dataloader = DataLoader(X; batchsize=13, shuffle=false);

julia&gt; for (i, x) in enumerate(dataloader)
           @show i, summary(x)
       end
(i, summary(x)) = (1, &quot;3×13 Matrix{Float64}&quot;)
(i, summary(x)) = (2, &quot;3×13 Matrix{Float64}&quot;)
(i, summary(x)) = (3, &quot;3×7 Matrix{Float64}&quot;)

julia&gt; for (i, x) in enumerate(CUDADevice()(dataloader))
           @show i, summary(x)
       end
(i, summary(x)) = (1, &quot;3×13 CuArray{Float32, 2, CUDA.DeviceMemory}&quot;)
(i, summary(x)) = (2, &quot;3×13 CuArray{Float32, 2, CUDA.DeviceMemory}&quot;)
(i, summary(x)) = (3, &quot;3×7 CuArray{Float32, 2, CUDA.DeviceMemory}&quot;)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/LuxDL/Lux.jl/blob/v1.5.0/src/iterator.jl#L1-L46">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../training/zygote/">« Gradients – Zygote.jl</a><a class="docs-footer-nextpage" href="../mlutils/">Batching Data – MLUtils.jl »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.7.0 on <span class="colophon-date" title="Saturday 9 November 2024 09:23">Saturday 9 November 2024</span>. Using Julia version 1.11.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
