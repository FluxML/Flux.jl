<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Optimisers · Flux</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-36890222-9', 'auto');
ga('send', 'pageview', {'page': location.pathname + location.search + location.hash});
</script><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/flux.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="Flux logo"/></a><div class="docs-package-name"><span class="docs-autofit">Flux</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Building Models</span><ul><li><a class="tocitem" href="../../models/overview/">Overview</a></li><li><a class="tocitem" href="../../models/basics/">Basics</a></li><li><a class="tocitem" href="../../models/recurrence/">Recurrence</a></li><li><a class="tocitem" href="../../models/layers/">Model Reference</a></li><li><a class="tocitem" href="../../models/losses/">Loss Functions</a></li><li><a class="tocitem" href="../../models/regularisation/">Regularisation</a></li><li><a class="tocitem" href="../../models/advanced/">Advanced Model Building</a></li><li><a class="tocitem" href="../../models/nnlib/">NNlib</a></li><li><a class="tocitem" href="../../models/functors/">Functors</a></li></ul></li><li><span class="tocitem">Handling Data</span><ul><li><a class="tocitem" href="../../data/onehot/">One-Hot Encoding</a></li><li><a class="tocitem" href="../../data/mlutils/">MLUtils</a></li></ul></li><li><span class="tocitem">Training Models</span><ul><li class="is-active"><a class="tocitem" href>Optimisers</a><ul class="internal"><li><a class="tocitem" href="#Optimiser-Reference"><span>Optimiser Reference</span></a></li><li><a class="tocitem" href="#Optimiser-Interface"><span>Optimiser Interface</span></a></li><li><a class="tocitem" href="#Composing-Optimisers"><span>Composing Optimisers</span></a></li><li><a class="tocitem" href="#Scheduling-Optimisers"><span>Scheduling Optimisers</span></a></li><li><a class="tocitem" href="#Decays"><span>Decays</span></a></li><li><a class="tocitem" href="#Gradient-Clipping"><span>Gradient Clipping</span></a></li><li class="toplevel"><a class="tocitem" href="#Optimisers.jl"><span>Optimisers.jl</span></a></li></ul></li><li><a class="tocitem" href="../training/">Training</a></li></ul></li><li><a class="tocitem" href="../../gpu/">GPU Support</a></li><li><a class="tocitem" href="../../saving/">Saving &amp; Loading</a></li><li><a class="tocitem" href="../../ecosystem/">The Julia Ecosystem</a></li><li><a class="tocitem" href="../../utilities/">Utility Functions</a></li><li><a class="tocitem" href="../../performance/">Performance Tips</a></li><li><a class="tocitem" href="../../datasets/">Datasets</a></li><li><a class="tocitem" href="../../community/">Community</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Training Models</a></li><li class="is-active"><a href>Optimisers</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Optimisers</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/FluxML/Flux.jl/blob/master/docs/src/training/optimisers.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Optimisers"><a class="docs-heading-anchor" href="#Optimisers">Optimisers</a><a id="Optimisers-1"></a><a class="docs-heading-anchor-permalink" href="#Optimisers" title="Permalink"></a></h1><p>Consider a <a href="../../models/basics/">simple linear regression</a>. We create some dummy data, calculate a loss, and backpropagate to calculate gradients for the parameters <code>W</code> and <code>b</code>.</p><pre><code class="language-julia">using Flux

W = rand(2, 5)
b = rand(2)

predict(x) = (W * x) .+ b
loss(x, y) = sum((predict(x) .- y).^2)

x, y = rand(5), rand(2) # Dummy data
l = loss(x, y) # ~ 3

θ = Flux.params(W, b)
grads = gradient(() -&gt; loss(x, y), θ)</code></pre><p>We want to update each parameter, using the gradient, in order to improve (reduce) the loss. Here&#39;s one way to do that:</p><pre><code class="language-julia">η = 0.1 # Learning Rate
for p in (W, b)
  p .-= η * grads[p]
end</code></pre><p>Running this will alter the parameters <code>W</code> and <code>b</code> and our loss should go down. Flux provides a more general way to do optimiser updates like this.</p><pre><code class="language-julia">using Flux: update!

opt = Descent(0.1) # Gradient descent with learning rate 0.1

for p in (W, b)
  update!(opt, p, grads[p])
end</code></pre><p>An optimiser <code>update!</code> accepts a parameter and a gradient, and updates the parameter according to the chosen rule. We can also pass <code>opt</code> to our <a href="../training/">training loop</a>, which will update all parameters of the model in a loop. However, we can now easily replace <code>Descent</code> with a more advanced optimiser such as <code>Adam</code>.</p><h2 id="Optimiser-Reference"><a class="docs-heading-anchor" href="#Optimiser-Reference">Optimiser Reference</a><a id="Optimiser-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Optimiser-Reference" title="Permalink"></a></h2><p>All optimisers return an object that, when passed to <code>train!</code>, will update the parameters passed to it.</p><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.update!" href="#Flux.Optimise.update!"><code>Flux.Optimise.update!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">update!(opt, p, g)
update!(opt, ps::Params, gs)</code></pre><p>Perform an update step of the parameters <code>ps</code> (or the single parameter <code>p</code>) according to optimizer <code>opt</code>  and the gradients <code>gs</code> (the gradient <code>g</code>).</p><p>As a result, the parameters are mutated and the optimizer&#39;s internal state may change. The gradient could be mutated as well.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/8b3e23842c19559a653ce5701d5b5db8f06f5d02/src/optimise/train.jl#L5-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.Descent" href="#Flux.Optimise.Descent"><code>Flux.Optimise.Descent</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Descent(η = 0.1)</code></pre><p>Classic gradient descent optimiser with learning rate <code>η</code>. For each parameter <code>p</code> and its gradient <code>δp</code>, this runs <code>p -= η*δp</code></p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = Descent()

opt = Descent(0.3)

ps = Flux.params(model)

gs = gradient(ps) do
    loss(x, y)
end

Flux.Optimise.update!(opt, ps, gs)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/8b3e23842c19559a653ce5701d5b5db8f06f5d02/src/optimise/optimisers.jl#L10-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.Momentum" href="#Flux.Optimise.Momentum"><code>Flux.Optimise.Momentum</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Momentum(η = 0.01, ρ = 0.9)</code></pre><p>Gradient descent optimizer with learning rate <code>η</code> and momentum <code>ρ</code>.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Momentum (<code>ρ</code>): Controls the acceleration of gradient descent in the                 prominent direction, in effect damping oscillations.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = Momentum()

opt = Momentum(0.01, 0.99)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/8b3e23842c19559a653ce5701d5b5db8f06f5d02/src/optimise/optimisers.jl#L45-L62">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.Nesterov" href="#Flux.Optimise.Nesterov"><code>Flux.Optimise.Nesterov</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Nesterov(η = 0.001, ρ = 0.9)</code></pre><p>Gradient descent optimizer with learning rate <code>η</code> and Nesterov momentum <code>ρ</code>.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Nesterov momentum (<code>ρ</code>): Controls the acceleration of gradient descent in the                          prominent direction, in effect damping oscillations.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = Nesterov()

opt = Nesterov(0.003, 0.95)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/8b3e23842c19559a653ce5701d5b5db8f06f5d02/src/optimise/optimisers.jl#L78-L95">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.RMSProp" href="#Flux.Optimise.RMSProp"><code>Flux.Optimise.RMSProp</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">RMSProp(η = 0.001, ρ = 0.9, ϵ = 1.0e-8)</code></pre><p>Optimizer using the <a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">RMSProp</a> algorithm. Often a good choice for recurrent networks. Parameters other than learning rate generally don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Momentum (<code>ρ</code>): Controls the acceleration of gradient descent in the                 prominent direction, in effect damping oscillations.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = RMSProp()

opt = RMSProp(0.002, 0.95)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/8b3e23842c19559a653ce5701d5b5db8f06f5d02/src/optimise/optimisers.jl#L112-L132">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.Adam" href="#Flux.Optimise.Adam"><code>Flux.Optimise.Adam</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Adam(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = 1.0e-8)</code></pre><p><a href="https://arxiv.org/abs/1412.6980">Adam</a> optimiser.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = Adam()

opt = Adam(0.001, (0.9, 0.8))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/8b3e23842c19559a653ce5701d5b5db8f06f5d02/src/optimise/optimisers.jl#L149-L166">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.RAdam" href="#Flux.Optimise.RAdam"><code>Flux.Optimise.RAdam</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">RAdam(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = 1.0e-8)</code></pre><p><a href="https://arxiv.org/abs/1908.03265">Rectified Adam</a> optimizer.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = RAdam()

opt = RAdam(0.001, (0.9, 0.8))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/8b3e23842c19559a653ce5701d5b5db8f06f5d02/src/optimise/optimisers.jl#L191-L208">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.AdaMax" href="#Flux.Optimise.AdaMax"><code>Flux.Optimise.AdaMax</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AdaMax(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = 1.0e-8)</code></pre><p><a href="https://arxiv.org/abs/1412.6980">AdaMax</a> is a variant of Adam based on the ∞-norm.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = AdaMax()

opt = AdaMax(0.001, (0.9, 0.995))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/8b3e23842c19559a653ce5701d5b5db8f06f5d02/src/optimise/optimisers.jl#L241-L258">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.AdaGrad" href="#Flux.Optimise.AdaGrad"><code>Flux.Optimise.AdaGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AdaGrad(η = 0.1, ϵ = 1.0e-8)</code></pre><p><a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">AdaGrad</a> optimizer. It has parameter specific learning rates based on how frequently it is updated. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = AdaGrad()

opt = AdaGrad(0.001)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/8b3e23842c19559a653ce5701d5b5db8f06f5d02/src/optimise/optimisers.jl#L328-L345">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.AdaDelta" href="#Flux.Optimise.AdaDelta"><code>Flux.Optimise.AdaDelta</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AdaDelta(ρ = 0.9, ϵ = 1.0e-8)</code></pre><p><a href="https://arxiv.org/abs/1212.5701">AdaDelta</a> is a version of AdaGrad adapting its learning rate based on a window of past gradient updates. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Rho (<code>ρ</code>): Factor by which the gradient is decayed at each time step.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = AdaDelta()

opt = AdaDelta(0.89)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/8b3e23842c19559a653ce5701d5b5db8f06f5d02/src/optimise/optimisers.jl#L361-L377">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.AMSGrad" href="#Flux.Optimise.AMSGrad"><code>Flux.Optimise.AMSGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AMSGrad(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = 1.0e-8)</code></pre><p>The <a href="https://openreview.net/forum?id=ryQu7f-RZ">AMSGrad</a> version of the Adam optimiser. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = AMSGrad()

opt = AMSGrad(0.001, (0.89, 0.995))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/8b3e23842c19559a653ce5701d5b5db8f06f5d02/src/optimise/optimisers.jl#L397-L415">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.NAdam" href="#Flux.Optimise.NAdam"><code>Flux.Optimise.NAdam</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">NAdam(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = 1.0e-8)</code></pre><p><a href="https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ">NAdam</a> is a Nesterov variant of Adam. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = NAdam()

opt = NAdam(0.002, (0.89, 0.995))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/8b3e23842c19559a653ce5701d5b5db8f06f5d02/src/optimise/optimisers.jl#L438-L456">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.AdamW" href="#Flux.Optimise.AdamW"><code>Flux.Optimise.AdamW</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">AdamW(η = 0.001, β::Tuple = (0.9, 0.999), decay = 0)</code></pre><p><a href="https://arxiv.org/abs/1711.05101">AdamW</a> is a variant of Adam fixing (as in repairing) its weight decay regularization.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li><code>decay</code>: Decay applied to weights during optimisation.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = AdamW()

opt = AdamW(0.001, (0.89, 0.995), 0.1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/8b3e23842c19559a653ce5701d5b5db8f06f5d02/src/optimise/optimisers.jl#L482-L501">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.OAdam" href="#Flux.Optimise.OAdam"><code>Flux.Optimise.OAdam</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">OAdam(η = 0.0001, β::Tuple = (0.5, 0.9), ϵ = 1.0e-8)</code></pre><p><a href="https://arxiv.org/abs/1711.00141">OAdam</a> (Optimistic Adam) is a variant of Adam adding an &quot;optimistic&quot; term suitable for adversarial training.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = OAdam()

opt = OAdam(0.001, (0.9, 0.995))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/8b3e23842c19559a653ce5701d5b5db8f06f5d02/src/optimise/optimisers.jl#L283-L301">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.AdaBelief" href="#Flux.Optimise.AdaBelief"><code>Flux.Optimise.AdaBelief</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">AdaBelief(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = 1.0e-8)</code></pre><p>The <a href="https://arxiv.org/abs/2010.07468">AdaBelief</a> optimiser is a variant of the well-known Adam optimiser.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia">opt = AdaBelief()

opt = AdaBelief(0.001, (0.9, 0.8))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/8b3e23842c19559a653ce5701d5b5db8f06f5d02/src/optimise/optimisers.jl#L505-L523">source</a></section></article><h2 id="Optimiser-Interface"><a class="docs-heading-anchor" href="#Optimiser-Interface">Optimiser Interface</a><a id="Optimiser-Interface-1"></a><a class="docs-heading-anchor-permalink" href="#Optimiser-Interface" title="Permalink"></a></h2><p>Flux&#39;s optimisers are built around a <code>struct</code> that holds all the optimiser parameters along with a definition of how to apply the update rule associated with it. We do this via the <code>apply!</code> function which takes the optimiser as the first argument followed by the parameter and its corresponding gradient.</p><p>In this manner Flux also allows one to create custom optimisers to be used seamlessly. Let&#39;s work this with a simple example.</p><pre><code class="language-julia">mutable struct Momentum
  eta
  rho
  velocity
end

Momentum(eta::Real, rho::Real) = Momentum(eta, rho, IdDict())</code></pre><p>The <code>Momentum</code> type will act as our optimiser in this case. Notice that we have added all the parameters as fields, along with the velocity which we will use as our state dictionary. Each parameter in our models will get an entry in there. We can now define the rule applied when this optimiser is invoked.</p><pre><code class="language-julia">function Flux.Optimise.apply!(o::Momentum, x, Δ)
  η, ρ = o.eta, o.rho
  v = get!(o.velocity, x, zero(x))::typeof(x)
  @. v = ρ * v - η * Δ
  @. Δ = -v
end</code></pre><p>This is the basic definition of a Momentum update rule given by:</p><p class="math-container">\[v = ρ * v - η * Δ
w = w - v\]</p><p>The <code>apply!</code> defines the update rules for an optimiser <code>opt</code>, given the parameters and gradients. It returns the updated gradients. Here, every parameter <code>x</code> is retrieved from the running state <code>v</code> and subsequently updates the state of the optimiser.</p><p>Flux internally calls on this function via the <code>update!</code> function. It shares the API with <code>apply!</code> but ensures that multiple parameters are handled gracefully.</p><h2 id="Composing-Optimisers"><a class="docs-heading-anchor" href="#Composing-Optimisers">Composing Optimisers</a><a id="Composing-Optimisers-1"></a><a class="docs-heading-anchor-permalink" href="#Composing-Optimisers" title="Permalink"></a></h2><p>Flux defines a special kind of optimiser simply called <code>Optimiser</code> which takes in arbitrary optimisers as input. Its behaviour is similar to the usual optimisers, but differs in that it acts by calling the optimisers listed in it sequentially. Each optimiser produces a modified gradient that will be fed into the next, and the resultant update will be applied to the parameter as usual. A classic use case is where adding decays is desirable. Flux defines some basic decays including <code>ExpDecay</code>, <code>InvDecay</code> etc.</p><pre><code class="language-julia">opt = Optimiser(ExpDecay(1, 0.1, 1000, 1e-4), Descent())</code></pre><p>Here we apply exponential decay to the <code>Descent</code> optimiser. The defaults of <code>ExpDecay</code> say that its learning rate will be decayed every 1000 steps. It is then applied like any optimiser.</p><pre><code class="language-julia">w = randn(10, 10)
w1 = randn(10,10)
ps = Params([w, w1])

loss(x) = Flux.Losses.mse(w * x, w1 * x)

loss(rand(10)) # around 9

for t = 1:10^5
  θ = Params([w, w1])
  θ̄ = gradient(() -&gt; loss(rand(10)), θ)
  Flux.Optimise.update!(opt, θ, θ̄)
end

loss(rand(10)) # around 0.9</code></pre><p>In this manner it is possible to compose optimisers for some added flexibility.</p><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.Optimiser" href="#Flux.Optimise.Optimiser"><code>Flux.Optimise.Optimiser</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Optimiser(a, b, c...)</code></pre><p>Combine several optimisers into one; each optimiser produces a modified gradient that will be fed into the next, and this is finally applied to the parameter as usual.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/8b3e23842c19559a653ce5701d5b5db8f06f5d02/src/optimise/optimisers.jl#L561-L567">source</a></section></article><h2 id="Scheduling-Optimisers"><a class="docs-heading-anchor" href="#Scheduling-Optimisers">Scheduling Optimisers</a><a id="Scheduling-Optimisers-1"></a><a class="docs-heading-anchor-permalink" href="#Scheduling-Optimisers" title="Permalink"></a></h2><p>In practice, it is fairly common to schedule the learning rate of an optimiser to obtain faster convergence. There are a variety of popular scheduling policies, and you can find implementations of them in <a href="https://darsnack.github.io/ParameterSchedulers.jl/dev/README.html">ParameterSchedulers.jl</a>. The documentation for ParameterSchedulers.jl provides a more detailed overview of the different scheduling policies, and how to use them with Flux optimizers. Below, we provide a brief snippet illustrating a <a href="https://arxiv.org/pdf/1608.03983.pdf">cosine annealing</a> schedule with a momentum optimiser.</p><p>First, we import ParameterSchedulers.jl and initalize a cosine annealing schedule to varying the learning rate between <code>1e-4</code> and <code>1e-2</code> every 10 steps. We also create a new <a href="#Flux.Optimise.Momentum"><code>Momentum</code></a> optimiser.</p><pre><code class="language-julia">using ParameterSchedulers

opt = Momentum()
schedule = Cos(λ0 = 1e-4, λ1 = 1e-2, period = 10)
for (eta, epoch) in zip(schedule, 1:100)
  opt.eta = eta
  # your training code here
end</code></pre><p><code>schedule</code> can also be indexed (e.g. <code>schedule(100)</code>) or iterated like any iterator in Julia.</p><p>ParameterSchedulers.jl schedules are stateless (they don&#39;t store their iteration state). If you want a <em>stateful</em> schedule, you can use <code>ParameterSchedulers.Stateful</code>:</p><pre><code class="language-julia">using ParameterSchedulers: Stateful, next!

schedule = Stateful(Cos(λ0 = 1e-4, λ1 = 1e-2, period = 10))
for epoch in 1:100
  opt.eta = next!(schedule)
  # your training code here
end</code></pre><p>ParameterSchedulers.jl allows for many more scheduling policies including arbitrary functions, looping any function with a given period, or sequences of many schedules. See the ParameterSchedulers.jl documentation for more info.</p><h2 id="Decays"><a class="docs-heading-anchor" href="#Decays">Decays</a><a id="Decays-1"></a><a class="docs-heading-anchor-permalink" href="#Decays" title="Permalink"></a></h2><p>Similar to optimisers, Flux also defines some simple decays that can be used in conjunction with other optimisers, or standalone.</p><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.ExpDecay" href="#Flux.Optimise.ExpDecay"><code>Flux.Optimise.ExpDecay</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ExpDecay(η = 0.001, decay = 0.1, decay_step = 1000, clip = 1e-4, start = 1)</code></pre><p>Discount the learning rate <code>η</code> by the factor <code>decay</code> every <code>decay_step</code> steps till a minimum of <code>clip</code>.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li><code>decay</code>: Factor by which the learning rate is discounted.</li><li><code>decay_step</code>: Schedule decay operations by setting the number of steps between               two decay operations.</li><li><code>clip</code>: Minimum value of learning rate.</li><li>&#39;start&#39;: Step at which the decay starts.</li></ul><p>See also the <a href="#Scheduling-Optimisers">Scheduling Optimisers</a> section of the docs for more general scheduling techniques.</p><p><strong>Examples</strong></p><p><code>ExpDecay</code> is typically composed  with other optimizers  as the last transformation of the gradient:</p><pre><code class="language-julia">opt = Optimiser(Adam(), ExpDecay(1.0))</code></pre><p>Note: you may want to start with <code>η=1</code> in <code>ExpDecay</code> when combined with other optimizers (<code>Adam</code> in this case) that have their own learning rate.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/8b3e23842c19559a653ce5701d5b5db8f06f5d02/src/optimise/optimisers.jl#L622-L650">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.InvDecay" href="#Flux.Optimise.InvDecay"><code>Flux.Optimise.InvDecay</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">InvDecay(γ = 0.001)</code></pre><p>Apply inverse time decay to an optimiser, so that the effective step size at iteration <code>n</code> is <code>eta / (1 + γ * n)</code> where <code>eta</code> is the initial step size. The wrapped optimiser&#39;s step size is not modified.</p><p>See also the <a href="#Scheduling-Optimisers">Scheduling Optimisers</a> section of the docs for more general scheduling techniques.</p><p><strong>Examples</strong></p><p><code>InvDecay</code> is typically composed  with other optimizers  as the last transformation of the gradient:</p><pre><code class="language-julia"># Inverse decay of the learning rate
# with starting value 0.001 and decay coefficient 0.01.
opt = Optimiser(Adam(1f-3), InvDecay(1f-2))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/8b3e23842c19559a653ce5701d5b5db8f06f5d02/src/optimise/optimisers.jl#L586-L606">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.WeightDecay" href="#Flux.Optimise.WeightDecay"><code>Flux.Optimise.WeightDecay</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">WeightDecay(λ = 0)</code></pre><p>Decay weights by <span>$λ$</span>.  Typically composed  with other optimizers as the first transformation to the gradient, making it equivalent to adding <span>$L_2$</span> regularization  with coefficient  <span>$λ$</span> to the loss.</p><p><strong>Examples</strong></p><pre><code class="language-julia">opt = Optimiser(WeightDecay(1f-4), Adam())</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/8b3e23842c19559a653ce5701d5b5db8f06f5d02/src/optimise/optimisers.jl#L673-L686">source</a></section></article><h2 id="Gradient-Clipping"><a class="docs-heading-anchor" href="#Gradient-Clipping">Gradient Clipping</a><a id="Gradient-Clipping-1"></a><a class="docs-heading-anchor-permalink" href="#Gradient-Clipping" title="Permalink"></a></h2><p>Gradient clipping is useful for training recurrent neural networks, which have a tendency to suffer from the exploding gradient problem. An example usage is</p><pre><code class="language-julia">opt = Optimiser(ClipValue(1e-3), Adam(1e-3))</code></pre><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.ClipValue" href="#Flux.Optimise.ClipValue"><code>Flux.Optimise.ClipValue</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ClipValue(thresh)</code></pre><p>Clip gradients when their absolute value exceeds <code>thresh</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/8b3e23842c19559a653ce5701d5b5db8f06f5d02/src/optimise/optimisers.jl#L698-L702">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.ClipNorm" href="#Flux.Optimise.ClipNorm"><code>Flux.Optimise.ClipNorm</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ClipNorm(thresh)</code></pre><p>Clip gradients when their L2 norm exceeds <code>thresh</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/8b3e23842c19559a653ce5701d5b5db8f06f5d02/src/optimise/optimisers.jl#L709-L713">source</a></section></article><h1 id="Optimisers.jl"><a class="docs-heading-anchor" href="#Optimisers.jl">Optimisers.jl</a><a id="Optimisers.jl-1"></a><a class="docs-heading-anchor-permalink" href="#Optimisers.jl" title="Permalink"></a></h1><p>Flux re-exports some utility functions from <a href="https://github.com/FluxML/Optimisers.jl"><code>Optimisers.jl</code></a> and the complete <code>Optimisers</code> package under the <code>Flux.Optimisers</code> namespace.</p><article class="docstring"><header><a class="docstring-binding" id="Optimisers.destructure" href="#Optimisers.destructure"><code>Optimisers.destructure</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">destructure(model) -&gt; vector, reconstructor</code></pre><p>Copies all <a href="#Optimisers.trainable"><code>trainable</code></a>, <a href="training/@ref"><code>isnumeric</code></a> parameters in the model to a vector, and returns also a function which reverses this transformation. Differentiable.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl">julia&gt; v, re = destructure((x=[1.0, 2.0], y=(sin, [3.0 + 4.0im])))
(ComplexF64[1.0 + 0.0im, 2.0 + 0.0im, 3.0 + 4.0im], Restructure(NamedTuple, ..., 3))

julia&gt; re([3, 5, 7+11im])
(x = [3.0, 5.0], y = (sin, ComplexF64[7.0 + 11.0im]))</code></pre><p>If <code>model</code> contains various number types, they are promoted to make <code>vector</code>, and are usually restored by <code>Restructure</code>. Such restoration follows the rules  of <code>ChainRulesCore.ProjectTo</code>, and thus will restore floating point precision, but will permit more exotic numbers like <code>ForwardDiff.Dual</code>.</p><p>If <code>model</code> contains only GPU arrays, then <code>vector</code> will also live on the GPU. At present, a mixture of GPU and ordinary CPU arrays is undefined behaviour.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.trainable" href="#Optimisers.trainable"><code>Optimisers.trainable</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">trainable(x::Layer) -&gt; NamedTuple</code></pre><p>This should be overloaded to make optimisers ignore some fields of every <code>Layer</code>, which would otherwise contain trainable parameters. (Elements such as functions and sizes are always ignored.)</p><p>The default is <code>Functors.children(x)</code>, usually a NamedTuple of all fields, and <code>trainable(x)</code> must contain a subset of these.</p></div></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../data/mlutils/">« MLUtils</a><a class="docs-footer-nextpage" href="../training/">Training »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Monday 1 August 2022 05:12">Monday 1 August 2022</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
