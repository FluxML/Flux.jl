var documenterSearchIndex = {"docs":
[{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"CurrentModule = Flux","category":"page"},{"location":"training/optimisers/#Optimisers","page":"Optimisation Rules ðŸ“š","title":"Optimisers","text":"","category":"section"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"Consider a simple linear regression. We create some dummy data, calculate a loss, and backpropagate to calculate gradients for the parameters W and b.","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"using Flux\n\nW = rand(2, 5)\nb = rand(2)\n\npredict(x) = (W * x) .+ b\nloss(x, y) = sum((predict(x) .- y).^2)\n\nx, y = rand(5), rand(2) # Dummy data\nl = loss(x, y) # ~ 3\n\nÎ¸ = Flux.params(W, b)\ngrads = gradient(() -> loss(x, y), Î¸)","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"We want to update each parameter, using the gradient, in order to improve (reduce) the loss. Here's one way to do that:","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"Î· = 0.1 # Learning Rate\nfor p in (W, b)\n  p .-= Î· * grads[p]\nend","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"Running this will alter the parameters W and b and our loss should go down. Flux provides a more general way to do optimiser updates like this.","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"using Flux: update!\n\nopt = Descent(0.1) # Gradient descent with learning rate 0.1\n\nfor p in (W, b)\n  update!(opt, p, grads[p])\nend","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"An optimiser update! accepts a parameter and a gradient, and updates the parameter according to the chosen rule. We can also pass opt to our training loop, which will update all parameters of the model in a loop. However, we can now easily replace Descent with a more advanced optimiser such as Adam.","category":"page"},{"location":"training/optimisers/#Optimiser-Reference","page":"Optimisation Rules ðŸ“š","title":"Optimiser Reference","text":"","category":"section"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"All optimisers return an object that, when passed to train!, will update the parameters passed to it.","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"Flux.Optimise.update!\nDescent\nMomentum\nNesterov\nRMSProp\nAdam\nRAdam\nAdaMax\nAdaGrad\nAdaDelta\nAMSGrad\nNAdam\nAdamW\nOAdam\nAdaBelief","category":"page"},{"location":"training/optimisers/#Optimisers.update!","page":"Optimisation Rules ðŸ“š","title":"Optimisers.update!","text":"Optimisers.update!(tree, model, gradient) -> (tree, model)\n\nUses the optimiser and the gradient to change the trainable parameters in the model. Returns the improved model, and the optimiser states needed for the next update. The initial tree of states comes from setup.\n\nThis is used in exactly the same manner as update, but because it may mutate arrays within the old model (and the old state), it will be faster for models of ordinary Arrays or CuArrays. However, you should not rely on the old model being fully updated but rather use the returned model.\n\nExample\n\njulia> using StaticArrays, Zygote, Optimisers\n\njulia> m = (x = [1f0, 2f0], y = SA[4f0, 5f0]);  # partly mutable model\n\njulia> t = Optimisers.setup(Momentum(1/30, 0.9), m);\n\njulia> g = gradient(m -> sum(abs2.(m.x .+ m.y)), m)[1]\n(x = Float32[10.0, 14.0], y = Float32[10.0, 14.0])\n\njulia> t2, m2 = Optimisers.update!(t, m, g);\n\njulia> m2  # after update or update!, this is the new model\n(x = Float32[0.6666666, 1.5333333], y = Float32[3.6666667, 4.5333333])\n\njulia> m2.x === m.x  # update! has re-used this array, for efficiency\ntrue\n\njulia> m  # original should be discarded, may be mutated but no guarantee\n(x = Float32[0.6666666, 1.5333333], y = Float32[4.0, 5.0])\n\njulia> t == t2  # original state is in fact guaranteed to be mutated\ntrue\n\n\n\n\n\n","category":"function"},{"location":"training/optimisers/#Flux.Optimise.Descent","page":"Optimisation Rules ðŸ“š","title":"Flux.Optimise.Descent","text":"Descent(Î· = 0.1)\n\nClassic gradient descent optimiser with learning rate Î·. For each parameter p and its gradient Î´p, this runs p -= Î·*Î´p\n\nParameters\n\nLearning rate (Î·): Amount by which gradients are discounted before updating                      the weights.\n\nExamples\n\nopt = Descent()\n\nopt = Descent(0.3)\n\nps = Flux.params(model)\n\ngs = gradient(ps) do\n    loss(x, y)\nend\n\nFlux.Optimise.update!(opt, ps, gs)\n\n\n\n\n\n","category":"type"},{"location":"training/optimisers/#Flux.Optimise.Momentum","page":"Optimisation Rules ðŸ“š","title":"Flux.Optimise.Momentum","text":"Momentum(Î· = 0.01, Ï = 0.9)\n\nGradient descent optimizer with learning rate Î· and momentum Ï.\n\nParameters\n\nLearning rate (Î·): Amount by which gradients are discounted before updating                      the weights.\nMomentum (Ï): Controls the acceleration of gradient descent in the                 prominent direction, in effect damping oscillations.\n\nExamples\n\nopt = Momentum()\n\nopt = Momentum(0.01, 0.99)\n\n\n\n\n\n","category":"type"},{"location":"training/optimisers/#Flux.Optimise.Nesterov","page":"Optimisation Rules ðŸ“š","title":"Flux.Optimise.Nesterov","text":"Nesterov(Î· = 0.001, Ï = 0.9)\n\nGradient descent optimizer with learning rate Î· and Nesterov momentum Ï.\n\nParameters\n\nLearning rate (Î·): Amount by which gradients are discounted before updating                      the weights.\nNesterov momentum (Ï): Controls the acceleration of gradient descent in the                          prominent direction, in effect damping oscillations.\n\nExamples\n\nopt = Nesterov()\n\nopt = Nesterov(0.003, 0.95)\n\n\n\n\n\n","category":"type"},{"location":"training/optimisers/#Flux.Optimise.RMSProp","page":"Optimisation Rules ðŸ“š","title":"Flux.Optimise.RMSProp","text":"RMSProp(Î· = 0.001, Ï = 0.9, Ïµ = 1.0e-8)\n\nOptimizer using the RMSProp algorithm. Often a good choice for recurrent networks. Parameters other than learning rate generally don't need tuning.\n\nParameters\n\nLearning rate (Î·): Amount by which gradients are discounted before updating                      the weights.\nMomentum (Ï): Controls the acceleration of gradient descent in the                 prominent direction, in effect damping oscillations.\n\nExamples\n\nopt = RMSProp()\n\nopt = RMSProp(0.002, 0.95)\n\n\n\n\n\n","category":"type"},{"location":"training/optimisers/#Flux.Optimise.Adam","page":"Optimisation Rules ðŸ“š","title":"Flux.Optimise.Adam","text":"Adam(Î· = 0.001, Î²::Tuple = (0.9, 0.999), Ïµ = 1.0e-8)\n\nAdam optimiser.\n\nParameters\n\nLearning rate (Î·): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (Î²::Tuple): Exponential decay for the first (Î²1) and the                                  second (Î²2) momentum estimate.\n\nExamples\n\nopt = Adam()\n\nopt = Adam(0.001, (0.9, 0.8))\n\n\n\n\n\n","category":"type"},{"location":"training/optimisers/#Flux.Optimise.RAdam","page":"Optimisation Rules ðŸ“š","title":"Flux.Optimise.RAdam","text":"RAdam(Î· = 0.001, Î²::Tuple = (0.9, 0.999), Ïµ = 1.0e-8)\n\nRectified Adam optimizer.\n\nParameters\n\nLearning rate (Î·): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (Î²::Tuple): Exponential decay for the first (Î²1) and the                                  second (Î²2) momentum estimate.\n\nExamples\n\nopt = RAdam()\n\nopt = RAdam(0.001, (0.9, 0.8))\n\n\n\n\n\n","category":"type"},{"location":"training/optimisers/#Flux.Optimise.AdaMax","page":"Optimisation Rules ðŸ“š","title":"Flux.Optimise.AdaMax","text":"AdaMax(Î· = 0.001, Î²::Tuple = (0.9, 0.999), Ïµ = 1.0e-8)\n\nAdaMax is a variant of Adam based on the âˆž-norm.\n\nParameters\n\nLearning rate (Î·): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (Î²::Tuple): Exponential decay for the first (Î²1) and the                                  second (Î²2) momentum estimate.\n\nExamples\n\nopt = AdaMax()\n\nopt = AdaMax(0.001, (0.9, 0.995))\n\n\n\n\n\n","category":"type"},{"location":"training/optimisers/#Flux.Optimise.AdaGrad","page":"Optimisation Rules ðŸ“š","title":"Flux.Optimise.AdaGrad","text":"AdaGrad(Î· = 0.1, Ïµ = 1.0e-8)\n\nAdaGrad optimizer. It has parameter specific learning rates based on how frequently it is updated. Parameters don't need tuning.\n\nParameters\n\nLearning rate (Î·): Amount by which gradients are discounted before updating                      the weights.\n\nExamples\n\nopt = AdaGrad()\n\nopt = AdaGrad(0.001)\n\n\n\n\n\n","category":"type"},{"location":"training/optimisers/#Flux.Optimise.AdaDelta","page":"Optimisation Rules ðŸ“š","title":"Flux.Optimise.AdaDelta","text":"AdaDelta(Ï = 0.9, Ïµ = 1.0e-8)\n\nAdaDelta is a version of AdaGrad adapting its learning rate based on a window of past gradient updates. Parameters don't need tuning.\n\nParameters\n\nRho (Ï): Factor by which the gradient is decayed at each time step.\n\nExamples\n\nopt = AdaDelta()\n\nopt = AdaDelta(0.89)\n\n\n\n\n\n","category":"type"},{"location":"training/optimisers/#Flux.Optimise.AMSGrad","page":"Optimisation Rules ðŸ“š","title":"Flux.Optimise.AMSGrad","text":"AMSGrad(Î· = 0.001, Î²::Tuple = (0.9, 0.999), Ïµ = 1.0e-8)\n\nThe AMSGrad version of the Adam optimiser. Parameters don't need tuning.\n\nParameters\n\nLearning rate (Î·): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (Î²::Tuple): Exponential decay for the first (Î²1) and the                                  second (Î²2) momentum estimate.\n\nExamples\n\nopt = AMSGrad()\n\nopt = AMSGrad(0.001, (0.89, 0.995))\n\n\n\n\n\n","category":"type"},{"location":"training/optimisers/#Flux.Optimise.NAdam","page":"Optimisation Rules ðŸ“š","title":"Flux.Optimise.NAdam","text":"NAdam(Î· = 0.001, Î²::Tuple = (0.9, 0.999), Ïµ = 1.0e-8)\n\nNAdam is a Nesterov variant of Adam. Parameters don't need tuning.\n\nParameters\n\nLearning rate (Î·): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (Î²::Tuple): Exponential decay for the first (Î²1) and the                                  second (Î²2) momentum estimate.\n\nExamples\n\nopt = NAdam()\n\nopt = NAdam(0.002, (0.89, 0.995))\n\n\n\n\n\n","category":"type"},{"location":"training/optimisers/#Flux.Optimise.AdamW","page":"Optimisation Rules ðŸ“š","title":"Flux.Optimise.AdamW","text":"AdamW(Î· = 0.001, Î²::Tuple = (0.9, 0.999), decay = 0)\n\nAdamW is a variant of Adam fixing (as in repairing) its weight decay regularization.\n\nParameters\n\nLearning rate (Î·): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (Î²::Tuple): Exponential decay for the first (Î²1) and the                                  second (Î²2) momentum estimate.\ndecay: Decay applied to weights during optimisation.\n\nExamples\n\nopt = AdamW()\n\nopt = AdamW(0.001, (0.89, 0.995), 0.1)\n\n\n\n\n\n","category":"function"},{"location":"training/optimisers/#Flux.Optimise.OAdam","page":"Optimisation Rules ðŸ“š","title":"Flux.Optimise.OAdam","text":"OAdam(Î· = 0.0001, Î²::Tuple = (0.5, 0.9), Ïµ = 1.0e-8)\n\nOAdam (Optimistic Adam) is a variant of Adam adding an \"optimistic\" term suitable for adversarial training.\n\nParameters\n\nLearning rate (Î·): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (Î²::Tuple): Exponential decay for the first (Î²1) and the                                  second (Î²2) momentum estimate.\n\nExamples\n\nopt = OAdam()\n\nopt = OAdam(0.001, (0.9, 0.995))\n\n\n\n\n\n","category":"type"},{"location":"training/optimisers/#Flux.Optimise.AdaBelief","page":"Optimisation Rules ðŸ“š","title":"Flux.Optimise.AdaBelief","text":"AdaBelief(Î· = 0.001, Î²::Tuple = (0.9, 0.999), Ïµ = 1.0e-8)\n\nThe AdaBelief optimiser is a variant of the well-known Adam optimiser.\n\nParameters\n\nLearning rate (Î·): Amount by which gradients are discounted before updating                      the weights.\nDecay of momentums (Î²::Tuple): Exponential decay for the first (Î²1) and the                                  second (Î²2) momentum estimate.\n\nExamples\n\nopt = AdaBelief()\n\nopt = AdaBelief(0.001, (0.9, 0.8))\n\n\n\n\n\n","category":"type"},{"location":"training/optimisers/#Optimiser-Interface","page":"Optimisation Rules ðŸ“š","title":"Optimiser Interface","text":"","category":"section"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"Flux's optimisers are built around a struct that holds all the optimiser parameters along with a definition of how to apply the update rule associated with it. We do this via the apply! function which takes the optimiser as the first argument followed by the parameter and its corresponding gradient.","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"In this manner Flux also allows one to create custom optimisers to be used seamlessly. Let's work on this with a simple example.","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"mutable struct Momentum\n  eta\n  rho\n  velocity\nend\n\nMomentum(eta::Real, rho::Real) = Momentum(eta, rho, IdDict())","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"The Momentum type will act as our optimiser in this case. Notice that we have added all the parameters as fields, along with the velocity which we will use as our state dictionary. Each parameter in our models will get an entry in there. We can now define the rule applied when this optimiser is invoked.","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"function Flux.Optimise.apply!(o::Momentum, x, Î”)\n  Î·, Ï = o.eta, o.rho\n  v = get!(o.velocity, x, zero(x))::typeof(x)\n  @. v = Ï * v - Î· * Î”\n  @. Î” = -v\nend","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"This is the basic definition of a Momentum update rule given by:","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"v = Ï * v - Î· * Î”\nw = w - v","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"The apply! defines the update rules for an optimiser opt, given the parameters and gradients. It returns the updated gradients. Here, every parameter x is retrieved from the running state v and subsequently updates the state of the optimiser.","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"Flux internally calls on this function via the update! function. It shares the API with apply! but ensures that multiple parameters are handled gracefully.","category":"page"},{"location":"training/optimisers/#Composing-Optimisers","page":"Optimisation Rules ðŸ“š","title":"Composing Optimisers","text":"","category":"section"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"Flux defines a special kind of optimiser simply called Optimiser which takes in arbitrary optimisers as input. Its behaviour is similar to the usual optimisers, but differs in that it acts by calling the optimisers listed in it sequentially. Each optimiser produces a modified gradient that will be fed into the next, and the resultant update will be applied to the parameter as usual. A classic use case is where adding decays is desirable. Flux defines some basic decays including ExpDecay, InvDecay etc.","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"opt = Optimiser(ExpDecay(1, 0.1, 1000, 1e-4), Descent())","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"Here we apply exponential decay to the Descent optimiser. The defaults of ExpDecay say that its learning rate will be decayed every 1000 steps. It is then applied like any optimiser.","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"w = randn(10, 10)\nw1 = randn(10,10)\nps = Params([w, w1])\n\nloss(x) = Flux.Losses.mse(w * x, w1 * x)\n\nloss(rand(10)) # around 9\n\nfor t = 1:10^5\n  Î¸ = Params([w, w1])\n  Î¸Ì„ = gradient(() -> loss(rand(10)), Î¸)\n  Flux.Optimise.update!(opt, Î¸, Î¸Ì„)\nend\n\nloss(rand(10)) # around 0.9","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"It is possible to compose optimisers for some added flexibility.","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"Flux.Optimise.Optimiser","category":"page"},{"location":"training/optimisers/#Flux.Optimise.Optimiser","page":"Optimisation Rules ðŸ“š","title":"Flux.Optimise.Optimiser","text":"Optimiser(a, b, c...)\n\nCombine several optimisers into one; each optimiser produces a modified gradient that will be fed into the next, and this is finally applied to the parameter as usual.\n\n\n\n\n\n","category":"type"},{"location":"training/optimisers/#Scheduling-Optimisers","page":"Optimisation Rules ðŸ“š","title":"Scheduling Optimisers","text":"","category":"section"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"In practice, it is fairly common to schedule the learning rate of an optimiser to obtain faster convergence. There are a variety of popular scheduling policies, and you can find implementations of them in ParameterSchedulers.jl. The documentation for ParameterSchedulers.jl provides a more detailed overview of the different scheduling policies, and how to use them with Flux optimizers. Below, we provide a brief snippet illustrating a cosine annealing schedule with a momentum optimiser.","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"First, we import ParameterSchedulers.jl and initialize a cosine annealing schedule to vary the learning rate between 1e-4 and 1e-2 every 10 steps. We also create a new Momentum optimiser.","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"using ParameterSchedulers\n\nopt = Momentum()\nschedule = Cos(Î»0 = 1e-4, Î»1 = 1e-2, period = 10)\nfor (eta, epoch) in zip(schedule, 1:100)\n  opt.eta = eta\n  # your training code here\nend","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"schedule can also be indexed (e.g. schedule(100)) or iterated like any iterator in Julia.","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"ParameterSchedulers.jl schedules are stateless (they don't store their iteration state). If you want a stateful schedule, you can use ParameterSchedulers.Stateful:","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"using ParameterSchedulers: Stateful, next!\n\nschedule = Stateful(Cos(Î»0 = 1e-4, Î»1 = 1e-2, period = 10))\nfor epoch in 1:100\n  opt.eta = next!(schedule)\n  # your training code here\nend","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"ParameterSchedulers.jl allows for many more scheduling policies including arbitrary functions, looping any function with a given period, or sequences of many schedules. See the ParameterSchedulers.jl documentation for more info.","category":"page"},{"location":"training/optimisers/#Decays","page":"Optimisation Rules ðŸ“š","title":"Decays","text":"","category":"section"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"Similar to optimisers, Flux also defines some simple decays that can be used in conjunction with other optimisers, or standalone.","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"ExpDecay\nInvDecay\nWeightDecay","category":"page"},{"location":"training/optimisers/#Flux.Optimise.ExpDecay","page":"Optimisation Rules ðŸ“š","title":"Flux.Optimise.ExpDecay","text":"ExpDecay(Î· = 0.001, decay = 0.1, decay_step = 1000, clip = 1e-4, start = 1)\n\nDiscount the learning rate Î· by the factor decay every decay_step steps till a minimum of clip.\n\nParameters\n\nLearning rate (Î·): Amount by which gradients are discounted before updating                      the weights.\ndecay: Factor by which the learning rate is discounted.\ndecay_step: Schedule decay operations by setting the number of steps between               two decay operations.\nclip: Minimum value of learning rate.\n'start': Step at which the decay starts.\n\nSee also the Scheduling Optimisers section of the docs for more general scheduling techniques.\n\nExamples\n\nExpDecay is typically composed  with other optimizers  as the last transformation of the gradient:\n\nopt = Optimiser(Adam(), ExpDecay(1.0))\n\nNote: you may want to start with Î·=1 in ExpDecay when combined with other optimizers (Adam in this case) that have their own learning rate.\n\n\n\n\n\n","category":"type"},{"location":"training/optimisers/#Flux.Optimise.InvDecay","page":"Optimisation Rules ðŸ“š","title":"Flux.Optimise.InvDecay","text":"InvDecay(Î³ = 0.001)\n\nApply inverse time decay to an optimiser, so that the effective step size at iteration n is eta / (1 + Î³ * n) where eta is the initial step size. The wrapped optimiser's step size is not modified.\n\nSee also the Scheduling Optimisers section of the docs for more general scheduling techniques.\n\nExamples\n\nInvDecay is typically composed  with other optimizers  as the last transformation of the gradient:\n\n# Inverse decay of the learning rate\n# with starting value 0.001 and decay coefficient 0.01.\nopt = Optimiser(Adam(1f-3), InvDecay(1f-2))\n\n\n\n\n\n","category":"type"},{"location":"training/optimisers/#Flux.Optimise.WeightDecay","page":"Optimisation Rules ðŸ“š","title":"Flux.Optimise.WeightDecay","text":"WeightDecay(Î» = 0)\n\nDecay weights by Î».  Typically composed  with other optimizers as the first transformation to the gradient, making it equivalent to adding L_2 regularization  with coefficient  Î» to the loss.\n\nExamples\n\nopt = Optimiser(WeightDecay(1f-4), Adam())\n\n\n\n\n\n","category":"type"},{"location":"training/optimisers/#Gradient-Clipping","page":"Optimisation Rules ðŸ“š","title":"Gradient Clipping","text":"","category":"section"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"Gradient clipping is useful for training recurrent neural networks, which have a tendency to suffer from the exploding gradient problem. An example usage is","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"opt = Optimiser(ClipValue(1e-3), Adam(1e-3))","category":"page"},{"location":"training/optimisers/","page":"Optimisation Rules ðŸ“š","title":"Optimisation Rules ðŸ“š","text":"ClipValue\nClipNorm","category":"page"},{"location":"training/optimisers/#Flux.Optimise.ClipValue","page":"Optimisation Rules ðŸ“š","title":"Flux.Optimise.ClipValue","text":"ClipValue(thresh)\n\nClip gradients when their absolute value exceeds thresh.\n\n\n\n\n\n","category":"type"},{"location":"training/optimisers/#Flux.Optimise.ClipNorm","page":"Optimisation Rules ðŸ“š","title":"Flux.Optimise.ClipNorm","text":"ClipNorm(thresh)\n\nClip gradients when their L2 norm exceeds thresh.\n\n\n\n\n\n","category":"type"},{"location":"training/zygote/#Automatic-Differentiation-using-Zygote.jl","page":"Zygote.jl ðŸ“š (gradient, ...)","title":"Automatic Differentiation using Zygote.jl","text":"","category":"section"},{"location":"training/zygote/","page":"Zygote.jl ðŸ“š (gradient, ...)","title":"Zygote.jl ðŸ“š (gradient, ...)","text":"Flux re-exports the gradient from Zygote, and uses this function within train! to differentiate the model. Zygote has its own documentation, in particular listing some important limitations.","category":"page"},{"location":"training/zygote/#Implicit-style","page":"Zygote.jl ðŸ“š (gradient, ...)","title":"Implicit style","text":"","category":"section"},{"location":"training/zygote/","page":"Zygote.jl ðŸ“š (gradient, ...)","title":"Zygote.jl ðŸ“š (gradient, ...)","text":"Flux uses primarily what Zygote calls \"implicit\" gradients, described here in its documentation. ","category":"page"},{"location":"training/zygote/","page":"Zygote.jl ðŸ“š (gradient, ...)","title":"Zygote.jl ðŸ“š (gradient, ...)","text":"Zygote.gradient\nZygote.Params\nZygote.Grads\nZygote.jacobian(loss, ::Params)","category":"page"},{"location":"training/zygote/#Zygote.gradient","page":"Zygote.jl ðŸ“š (gradient, ...)","title":"Zygote.gradient","text":"gradient(() -> loss(), ps::Params) -> Grads\n\nGradient with implicit parameters. Takes a zero-argument function, and returns a dictionary-like container, whose keys are arrays x in ps.\n\njulia> x = [1 2 3; 4 5 6]; y = [7, 8]; z = [1, 10, 100];\n\njulia> g = gradient(Params([x, y])) do\n         sum(x .* y .* z')\n       end\nGrads(...)\n\njulia> g[x]\n2Ã—3 Matrix{Float64}:\n 7.0  70.0  700.0\n 8.0  80.0  800.0\n\njulia> haskey(g, z)  # only x and y are parameters\nfalse\n\n\n\n\n\n","category":"function"},{"location":"training/zygote/#Zygote.Params","page":"Zygote.jl ðŸ“š (gradient, ...)","title":"Zygote.Params","text":"Params([A, B])\n\nContainer for implicit parameters, used when differentiating a zero-argument funtion () -> loss(A, B) with respect to A, B.\n\n\n\n\n\n","category":"type"},{"location":"training/zygote/#Zygote.Grads","page":"Zygote.jl ðŸ“š (gradient, ...)","title":"Zygote.Grads","text":"Grads(...)\n\nDictionary-like container returned when taking gradients with respect to implicit parameters. For an array W, appearing  within Params([W, A, B...]), the gradient is g[W].\n\n\n\n\n\n","category":"type"},{"location":"training/zygote/#Zygote.jacobian-Tuple{Any, Params}","page":"Zygote.jl ðŸ“š (gradient, ...)","title":"Zygote.jacobian","text":"jacobian(loss, ::Params)\n\nLike gradient with implicit parameters, this method takes a zero-argument function and returns an IdDict-like object, now containing the Jacobian for each parameter.\n\nExamples\n\njulia> xs = [1 2; 3 4]; ys = [5,7,9];\n\njulia> Jxy = jacobian(() -> ys[1:2] .+ sum(xs.^2), Params([xs, ys]))\nGrads(...)\n\njulia> Jxy[ys]\n2Ã—3 Matrix{Int64}:\n 1  0  0\n 0  1  0\n\njulia> Jxy[xs]\n2Ã—4 Matrix{Int64}:\n 2  6  4  8\n 2  6  4  8\n\n\n\n\n\n","category":"method"},{"location":"training/zygote/#Explicit-style","page":"Zygote.jl ðŸ“š (gradient, ...)","title":"Explicit style","text":"","category":"section"},{"location":"training/zygote/","page":"Zygote.jl ðŸ“š (gradient, ...)","title":"Zygote.jl ðŸ“š (gradient, ...)","text":"The other way of using Zygote, and using most other AD packages, is to explicitly provide a function and its arguments.","category":"page"},{"location":"training/zygote/","page":"Zygote.jl ðŸ“š (gradient, ...)","title":"Zygote.jl ðŸ“š (gradient, ...)","text":"Zygote.gradient(f, args...)\nZygote.withgradient(f, args...)\nZygote.jacobian(f, args...)\nZygote.withgradient","category":"page"},{"location":"training/zygote/#Zygote.gradient-Tuple{Any, Vararg{Any, N} where N}","page":"Zygote.jl ðŸ“š (gradient, ...)","title":"Zygote.gradient","text":"gradient(f, args...)\n\nReturns a tuple containing âˆ‚f/âˆ‚x for each argument x, the derivative (for scalar x) or the gradient.\n\nf(args...) must be a real number, see jacobian for array output.\n\nSee also withgradient to keep the value f(args...), and pullback for value and back-propagator.\n\njulia> gradient(*, 2.0, 3.0, 5.0)\n(15.0, 10.0, 6.0)\n\njulia> gradient(x -> sum(abs2,x), [7.0, 11.0, 13.0])\n([14.0, 22.0, 26.0],)\n\njulia> gradient([7, 11], 0, 1) do x, y, d\n         p = size(x, d)\n         sum(x.^p .+ y)\n       end\n([14.0, 22.0], 2.0, nothing)\n\n\n\n\n\n","category":"method"},{"location":"training/zygote/#Zygote.withgradient-Tuple{Any, Vararg{Any, N} where N}","page":"Zygote.jl ðŸ“š (gradient, ...)","title":"Zygote.withgradient","text":"withgradient(f, args...)\nwithgradient(f, ::Params)\n\nReturns both the value of the function and the gradient, as a named tuple. \n\njulia> y, âˆ‡ = withgradient(/, 1, 2)\n(val = 0.5, grad = (0.5, -0.25))\n\njulia> âˆ‡ == gradient(/, 1, 2)\ntrue\n\n\n\n\n\n","category":"method"},{"location":"training/zygote/#Zygote.jacobian-Tuple{Any, Vararg{Any, N} where N}","page":"Zygote.jl ðŸ“š (gradient, ...)","title":"Zygote.jacobian","text":"jacobian(f, args...) -> Tuple\n\nFor each array a âˆˆ args this returns a matrix with Ja[k,i] = âˆ‚y[k]/âˆ‚a[i] where y = f(args...) is usually a vector. Arrays of higher dimension are treated like vec(a), or vec(y) for output.\n\nFor scalar x::Number âˆˆ args, the result is a vector Jx[k] = âˆ‚y[k]/âˆ‚x, while for scalar y all results have just one row.\n\nWith any other argument type, no result is produced, even if gradient would work.\n\nThis reverse-mode Jacobian needs to evaluate the pullback once for each element of y. Doing so is usually only efficient when length(y) is small compared to length(a), otherwise forward mode is likely to be better.\n\nSee also withjacobian, hessian, hessian_reverse.\n\nExamples\n\njulia> jacobian(a -> 100*a[1:3].^2, 1:7)[1]  # first index (rows) is output\n3Ã—7 Matrix{Int64}:\n 200    0    0  0  0  0  0\n   0  400    0  0  0  0  0\n   0    0  600  0  0  0  0\n\njulia> jacobian((a,x) -> a.^2 .* x, [1,2,3], 1)  # scalar argument has vector jacobian\n([2 0 0; 0 4 0; 0 0 6], [1, 4, 9])\n\njulia> jacobian((a,d) -> prod(a, dims=d), [1 2; 3 4; 5 6], 2)\n([2 0 â€¦ 0 0; 0 4 â€¦ 3 0; 0 0 â€¦ 0 5], [0, 0, 0])\n\nwarning: Warning\nFor arguments of any type except Number & AbstractArray, the result is nothing.\n\njulia> jacobian((a,s) -> a.^length(s), [1,2,3], \"str\")\n([3 0 0; 0 12 0; 0 0 27], nothing)\n\njulia> jacobian((a,t) -> sum(a .* t[1]) + t[2], [1,2,3], (4,5))\n([4 4 4], nothing)\n\njulia> gradient((a,t) -> sum(a .* t[1]) + t[2], [1,2,3], (4,5))  # gradient undersands the tuple\n([4 4 4], (6, 1))\n\n\n\n\n\n","category":"method"},{"location":"training/zygote/#Zygote.withgradient","page":"Zygote.jl ðŸ“š (gradient, ...)","title":"Zygote.withgradient","text":"withgradient(f, args...)\nwithgradient(f, ::Params)\n\nReturns both the value of the function and the gradient, as a named tuple. \n\njulia> y, âˆ‡ = withgradient(/, 1, 2)\n(val = 0.5, grad = (0.5, -0.25))\n\njulia> âˆ‡ == gradient(/, 1, 2)\ntrue\n\n\n\n\n\n","category":"function"},{"location":"training/zygote/#ChainRules","page":"Zygote.jl ðŸ“š (gradient, ...)","title":"ChainRules","text":"","category":"section"},{"location":"training/zygote/","page":"Zygote.jl ðŸ“š (gradient, ...)","title":"Zygote.jl ðŸ“š (gradient, ...)","text":"Sometimes it is necessary to exclude some code, or a whole function, from automatic differentiation. This can be done using ChainRules:","category":"page"},{"location":"training/zygote/","page":"Zygote.jl ðŸ“š (gradient, ...)","title":"Zygote.jl ðŸ“š (gradient, ...)","text":"ChainRulesCore.ignore_derivatives\nChainRulesCore.@non_differentiable","category":"page"},{"location":"training/zygote/#ChainRulesCore.ignore_derivatives","page":"Zygote.jl ðŸ“š (gradient, ...)","title":"ChainRulesCore.ignore_derivatives","text":"ignore_derivatives(f::Function)\n\nTells the AD system to ignore the gradients of the wrapped closure. The primal computation (forward pass) is executed normally.\n\nignore_derivatives() do\n    value = rand()\n    push!(collection, value)\nend\n\nUsing this incorrectly could lead to incorrect gradients. For example, the following function will have zero gradients with respect to its argument:\n\nfunction wrong_grads(x)\n    y = ones(3)\n    ignore_derivatives() do\n        push!(y, x)\n    end\n    return sum(y)\nend\n\n\n\n\n\nignore_derivatives(x)\n\nTells the AD system to ignore the gradients of the argument. Can be used to avoid unnecessary computation of gradients.\n\nignore_derivatives(x) * w\n\n\n\n\n\n","category":"function"},{"location":"training/zygote/#ChainRulesCore.@non_differentiable","page":"Zygote.jl ðŸ“š (gradient, ...)","title":"ChainRulesCore.@non_differentiable","text":"@non_differentiable(signature_expression)\n\nA helper to make it easier to declare that a method is not differentiable. This is a short-hand for defining an frule and rrule that return NoTangent() for all partials (even for the function sÌ„elf-partial itself)\n\nKeyword arguments should not be included.\n\njulia> @non_differentiable Base.:(==)(a, b)\n\njulia> _, pullback = rrule(==, 2.0, 3.0);\n\njulia> pullback(1.0)\n(NoTangent(), NoTangent(), NoTangent())\n\nYou can place type-constraints in the signature:\n\njulia> @non_differentiable Base.length(xs::Union{Number, Array})\n\njulia> frule((ZeroTangent(), 1), length, [2.0, 3.0])\n(2, NoTangent())\n\nwarning: Warning\nThis helper macro covers only the simple common cases. It does not support where-clauses. For these you can declare the rrule and frule directly\n\n\n\n\n\n","category":"macro"},{"location":"training/zygote/","page":"Zygote.jl ðŸ“š (gradient, ...)","title":"Zygote.jl ðŸ“š (gradient, ...)","text":"To manually supply the gradient for one function, you should define a method of rrule. ChainRules has detailed documentation on how this works.","category":"page"},{"location":"training/zygote/","page":"Zygote.jl ðŸ“š (gradient, ...)","title":"Zygote.jl ðŸ“š (gradient, ...)","text":"ChainRulesCore.rrule","category":"page"},{"location":"training/zygote/#ChainRulesCore.rrule","page":"Zygote.jl ðŸ“š (gradient, ...)","title":"ChainRulesCore.rrule","text":"rrule([::RuleConfig,] f, x...)\n\nExpressing x as the tuple (xâ‚, xâ‚‚, ...) and the output tuple of f(x...) as Î©, return the tuple:\n\n(Î©, (Î©Ì„â‚, Î©Ì„â‚‚, ...) -> (sÌ„elf, xÌ„â‚, xÌ„â‚‚, ...))\n\nWhere the second return value is the the propagation rule or pullback. It takes in cotangents corresponding to the outputs (xÌ„â‚, xÌ„â‚‚, ...), and sÌ„elf, the internal values of the function itself (for closures)\n\nIf no method matching rrule(f, xs...) has been defined, then return nothing.\n\nExamples:\n\nunary input, unary output scalar function:\n\njulia> x = rand();\n\njulia> sinx, sin_pullback = rrule(sin, x);\n\njulia> sinx == sin(x)\ntrue\n\njulia> sin_pullback(1) == (NoTangent(), cos(x))\ntrue\n\nbinary input, unary output scalar function:\n\njulia> x, y = rand(2);\n\njulia> hypotxy, hypot_pullback = rrule(hypot, x, y);\n\njulia> hypotxy == hypot(x, y)\ntrue\n\njulia> hypot_pullback(1) == (NoTangent(), (x / hypot(x, y)), (y / hypot(x, y)))\ntrue\n\nThe optional RuleConfig option allows specifying rrules only for AD systems that support given features. If not needed, then it can be omitted and the rrule without it will be hit as a fallback. This is the case for most rules.\n\nSee also: frule, @scalar_rule, RuleConfig\n\n\n\n\n\n","category":"function"},{"location":"destructure/#man-destructure","page":"Flat vs. Nested ðŸ“š","title":"Flat vs. Nested Structures","text":"","category":"section"},{"location":"destructure/","page":"Flat vs. Nested ðŸ“š","title":"Flat vs. Nested ðŸ“š","text":"A Flux model is a nested structure, with parameters stored within many layers. Sometimes you may want a flat representation of them, to interact with functions expecting just one vector. This is provided by destructure:","category":"page"},{"location":"destructure/","page":"Flat vs. Nested ðŸ“š","title":"Flat vs. Nested ðŸ“š","text":"julia> model = Chain(Dense(2=>1, tanh), Dense(1=>1))\nChain(\n  Dense(2 => 1, tanh),                  # 3 parameters\n  Dense(1 => 1),                        # 2 parameters\n)                   # Total: 4 arrays, 5 parameters, 276 bytes.\n\njulia> flat, rebuild = Flux.destructure(model)\n(Float32[0.863101, 1.2454957, 0.0, -1.6345707, 0.0], Restructure(Chain, ..., 5))\n\njulia> rebuild(zeros(5))  # same structure, new parameters\nChain(\n  Dense(2 => 1, tanh),                  # 3 parameters  (all zero)\n  Dense(1 => 1),                        # 2 parameters  (all zero)\n)                   # Total: 4 arrays, 5 parameters, 276 bytes.","category":"page"},{"location":"destructure/","page":"Flat vs. Nested ðŸ“š","title":"Flat vs. Nested ðŸ“š","text":"Both destructure and the Restructure function can be used within gradient computations. For instance, this computes the Hessian âˆ‚Â²L/âˆ‚Î¸áµ¢âˆ‚Î¸â±¼ of some loss function, with respect to all parameters of the Flux model. The resulting matrix has off-diagonal entries, which cannot really be expressed in a nested structure:","category":"page"},{"location":"destructure/","page":"Flat vs. Nested ðŸ“š","title":"Flat vs. Nested ðŸ“š","text":"julia> x = rand(Float32, 2, 16);\n\njulia> grad = gradient(m -> sum(abs2, m(x)), model)  # nested gradient\n((layers = ((weight = Float32[10.339018 11.379145], bias = Float32[22.845667], Ïƒ = nothing), (weight = Float32[-29.565302;;], bias = Float32[-37.644184], Ïƒ = nothing)),),)\n\njulia> function loss(v::Vector)\n         m = rebuild(v)\n         y = m(x)\n         sum(abs2, y)\n       end;\n\njulia> gradient(loss, flat)  # flat gradient, same numbers\n(Float32[10.339018, 11.379145, 22.845667, -29.565302, -37.644184],)\n\njulia> Zygote.hessian(loss, flat)  # second derivative\n5Ã—5 Matrix{Float32}:\n  -7.13131   -5.54714  -11.1393  -12.6504   -8.13492\n  -5.54714   -7.11092  -11.0208  -13.9231   -9.36316\n -11.1393   -11.0208   -13.7126  -27.9531  -22.741\n -12.6504   -13.9231   -27.9531   18.0875   23.03\n  -8.13492   -9.36316  -22.741    23.03     32.0\n\njulia> Flux.destructure(grad)  # acts on non-models, too\n(Float32[10.339018, 11.379145, 22.845667, -29.565302, -37.644184], Restructure(Tuple, ..., 5))","category":"page"},{"location":"destructure/#All-Parameters","page":"Flat vs. Nested ðŸ“š","title":"All Parameters","text":"","category":"section"},{"location":"destructure/","page":"Flat vs. Nested ðŸ“š","title":"Flat vs. Nested ðŸ“š","text":"The function destructure now lives in Optimisers.jl. (Be warned this package is unrelated to the Flux.Optimisers sub-module! The confusion is temporary.)","category":"page"},{"location":"destructure/","page":"Flat vs. Nested ðŸ“š","title":"Flat vs. Nested ðŸ“š","text":"Optimisers.destructure\nOptimisers.trainable\nOptimisers.isnumeric","category":"page"},{"location":"destructure/#Optimisers.destructure","page":"Flat vs. Nested ðŸ“š","title":"Optimisers.destructure","text":"destructure(model) -> vector, reconstructor\n\nCopies all trainable, isnumeric parameters in the model to a vector, and returns also a function which reverses this transformation. Differentiable.\n\nExample\n\njulia> v, re = destructure((x=[1.0, 2.0], y=(sin, [3.0 + 4.0im])))\n(ComplexF64[1.0 + 0.0im, 2.0 + 0.0im, 3.0 + 4.0im], Restructure(NamedTuple, ..., 3))\n\njulia> re([3, 5, 7+11im])\n(x = [3.0, 5.0], y = (sin, ComplexF64[7.0 + 11.0im]))\n\nIf model contains various number types, they are promoted to make vector, and are usually restored by Restructure. Such restoration follows the rules  of ChainRulesCore.ProjectTo, and thus will restore floating point precision, but will permit more exotic numbers like ForwardDiff.Dual.\n\nIf model contains only GPU arrays, then vector will also live on the GPU. At present, a mixture of GPU and ordinary CPU arrays is undefined behaviour.\n\n\n\n\n\n","category":"function"},{"location":"destructure/#Optimisers.trainable","page":"Flat vs. Nested ðŸ“š","title":"Optimisers.trainable","text":"trainable(x::Layer) -> NamedTuple\n\nThis should be overloaded to make optimisers ignore some fields of every Layer, which would otherwise contain trainable parameters. (Elements such as functions and sizes are always ignored.)\n\nThe default is Functors.children(x), usually a NamedTuple of all fields, and trainable(x) must contain a subset of these.\n\n\n\n\n\n","category":"function"},{"location":"destructure/#Optimisers.isnumeric","page":"Flat vs. Nested ðŸ“š","title":"Optimisers.isnumeric","text":"isnumeric(x) -> Bool\n\nReturns true on any parameter to be adjusted by Optimisers.jl, namely arrays of non-integer numbers. Returns false on all other types.\n\nRequires also that Functors.isleaf(x) == true, to focus on e.g. the parent of a transposed matrix, not the wrapper.\n\n\n\n\n\n","category":"function"},{"location":"destructure/#All-Layers","page":"Flat vs. Nested ðŸ“š","title":"All Layers","text":"","category":"section"},{"location":"destructure/","page":"Flat vs. Nested ðŸ“š","title":"Flat vs. Nested ðŸ“š","text":"Another kind of flat view of a nested model is provided by the modules command. This extracts a list of all layers:","category":"page"},{"location":"destructure/","page":"Flat vs. Nested ðŸ“š","title":"Flat vs. Nested ðŸ“š","text":"Flux.modules","category":"page"},{"location":"destructure/#Flux.modules","page":"Flat vs. Nested ðŸ“š","title":"Flux.modules","text":"modules(m)\n\nReturn an iterator over non-leaf objects that can be reached by recursing m over the children given by functor.\n\nUseful for applying a function (e.g. a regularizer) over specific modules or subsets of the parameters (e.g. the weights but not the biases).\n\nExamples\n\njulia> m1 = Chain(Dense(28^2, 64), BatchNorm(64, relu));\n\njulia> m2 = Chain(m1, Dense(64, 10))\nChain(\n  Chain(\n    Dense(784 => 64),                   # 50_240 parameters\n    BatchNorm(64, relu),                # 128 parameters, plus 128\n  ),\n  Dense(64 => 10),                      # 650 parameters\n)         # Total: 6 trainable arrays, 51_018 parameters,\n          # plus 2 non-trainable, 128 parameters, summarysize 200.312 KiB.\n\njulia> Flux.modules(m2)\n7-element Vector{Any}:\n Chain(Chain(Dense(784 => 64), BatchNorm(64, relu)), Dense(64 => 10))  # 51_018 parameters, plus 128 non-trainable\n (Chain(Dense(784 => 64), BatchNorm(64, relu)), Dense(64 => 10))\n Chain(Dense(784 => 64), BatchNorm(64, relu))  # 50_368 parameters, plus 128 non-trainable\n (Dense(784 => 64), BatchNorm(64, relu))\n Dense(784 => 64)    # 50_240 parameters\n BatchNorm(64, relu)  # 128 parameters, plus 128 non-trainable\n Dense(64 => 10)     # 650 parameters\n\njulia> L2(m) = sum(sum(abs2, l.weight) for l in Flux.modules(m) if l isa Dense)\nL2 (generic function with 1 method)\n\njulia> L2(m2) isa Float32\ntrue\n\n\n\n\n\n","category":"function"},{"location":"models/regularisation/#Regularisation","page":"Regularisation","title":"Regularisation","text":"","category":"section"},{"location":"models/regularisation/","page":"Regularisation","title":"Regularisation","text":"Applying regularisation to model parameters is straightforward. We just need to apply an appropriate regulariser to each model parameter and add the result to the overall loss.","category":"page"},{"location":"models/regularisation/","page":"Regularisation","title":"Regularisation","text":"For example, say we have a simple regression.","category":"page"},{"location":"models/regularisation/","page":"Regularisation","title":"Regularisation","text":"julia> using Flux\n\njulia> using Flux.Losses: logitcrossentropy\n\njulia> m = Dense(10 => 5)\nDense(10 => 5)      # 55 parameters\n\njulia> loss(x, y) = logitcrossentropy(m(x), y);","category":"page"},{"location":"models/regularisation/","page":"Regularisation","title":"Regularisation","text":"We can apply L2 regularisation by taking the squared norm of the parameters , m.weight and m.bias.","category":"page"},{"location":"models/regularisation/","page":"Regularisation","title":"Regularisation","text":"julia> penalty() = sum(abs2, m.weight) + sum(abs2, m.bias);\n\njulia> loss(x, y) = logitcrossentropy(m(x), y) + penalty();","category":"page"},{"location":"models/regularisation/","page":"Regularisation","title":"Regularisation","text":"When working with layers, Flux provides the params function to grab all parameters at once. We can easily penalise everything with sum:","category":"page"},{"location":"models/regularisation/","page":"Regularisation","title":"Regularisation","text":"julia> Flux.params(m)\nParams([Float32[0.34704182 -0.48532376 â€¦ -0.06914271 -0.38398427; 0.5201164 -0.033709668 â€¦ -0.36169025 -0.5552353; â€¦ ; 0.46534058 0.17114447 â€¦ -0.4809643 0.04993277; -0.47049698 -0.6206029 â€¦ -0.3092334 -0.47857067], Float32[0.0, 0.0, 0.0, 0.0, 0.0]])\n\njulia> sqnorm(x) = sum(abs2, x);\n\njulia> sum(sqnorm, Flux.params(m))\n8.34994f0","category":"page"},{"location":"models/regularisation/","page":"Regularisation","title":"Regularisation","text":"Here's a larger example with a multi-layer perceptron.","category":"page"},{"location":"models/regularisation/","page":"Regularisation","title":"Regularisation","text":"julia> m = Chain(Dense(28^2 => 128, relu), Dense(128 => 32, relu), Dense(32 => 10))\nChain(\n  Dense(784 => 128, relu),              # 100_480 parameters\n  Dense(128 => 32, relu),               # 4_128 parameters\n  Dense(32 => 10),                      # 330 parameters\n)                   # Total: 6 arrays, 104_938 parameters, 410.289 KiB.\n\njulia> sqnorm(x) = sum(abs2, x);\n\njulia> loss(x, y) = logitcrossentropy(m(x), y) + sum(sqnorm, Flux.params(m));\n\njulia> loss(rand(28^2), rand(10))\n300.76693683244997","category":"page"},{"location":"models/regularisation/","page":"Regularisation","title":"Regularisation","text":"One can also easily add per-layer regularisation via the activations function:","category":"page"},{"location":"models/regularisation/","page":"Regularisation","title":"Regularisation","text":"julia> using Flux: activations\n\njulia> c = Chain(Dense(10 => 5, Ïƒ), Dense(5 => 2), softmax)\nChain(\n  Dense(10 => 5, Ïƒ),                    # 55 parameters\n  Dense(5 => 2),                        # 12 parameters\n  NNlib.softmax,\n)                   # Total: 4 arrays, 67 parameters, 524 bytes.\n\njulia> activations(c, rand(10))\n([0.3274892431795043, 0.5360197770386552, 0.3447464835514667, 0.5273025865532305, 0.7513168089280781], [-0.3533774181890544, -0.010937055274926138], [0.4152168057978045, 0.5847831942021956])\n\njulia> sum(sqnorm, ans)\n1.9953131077618562","category":"page"},{"location":"models/regularisation/","page":"Regularisation","title":"Regularisation","text":"Flux.activations","category":"page"},{"location":"models/regularisation/#Flux.activations","page":"Regularisation","title":"Flux.activations","text":"activations(c::Chain, input)\n\nLike calling a Chain, but saves the result of each layer as an output.\n\nExamples\n\njulia> using Flux: activations\n\njulia> c = Chain(x -> x + 1, x -> x * 2, x -> x ^ 3);\n\njulia> activations(c, 1)\n(2, 4, 64)\n\n\n\n\n\n","category":"function"},{"location":"utilities/#Random-Weight-Initialisation","page":"Weight Initialisation ðŸ“š","title":"Random Weight Initialisation","text":"","category":"section"},{"location":"utilities/","page":"Weight Initialisation ðŸ“š","title":"Weight Initialisation ðŸ“š","text":"Flux initialises convolutional layers and recurrent cells with glorot_uniform by default. Most layers accept a function as an init keyword, which replaces this default. For example:","category":"page"},{"location":"utilities/","page":"Weight Initialisation ðŸ“š","title":"Weight Initialisation ðŸ“š","text":"julia> conv = Conv((3, 3), 3 => 2, relu; init=Flux.glorot_normal)\nConv((3, 3), 3 => 2, relu)  # 56 parameters\n\njulia> conv.bias\n2-element Vector{Float32}:\n 0.0\n 0.0","category":"page"},{"location":"utilities/","page":"Weight Initialisation ðŸ“š","title":"Weight Initialisation ðŸ“š","text":"Note that init creates the weight array, but not the bias vector.","category":"page"},{"location":"utilities/","page":"Weight Initialisation ðŸ“š","title":"Weight Initialisation ðŸ“š","text":"Many of the initialisation functions accept keywords such as gain,  and a random number generator. To make it easy to pass these to layers, there are methods which return a function:","category":"page"},{"location":"utilities/","page":"Weight Initialisation ðŸ“š","title":"Weight Initialisation ðŸ“š","text":"julia> Dense(4 => 5, tanh; init=Flux.glorot_uniform(gain=2))\nDense(4 => 5, tanh)  # 25 parameters\n\njulia> Dense(4 => 5, tanh; init=Flux.randn32(MersenneTwister(1)))\nDense(4 => 5, tanh)  # 25 parameters","category":"page"},{"location":"utilities/#Initialisation-functions","page":"Weight Initialisation ðŸ“š","title":"Initialisation functions","text":"","category":"section"},{"location":"utilities/","page":"Weight Initialisation ðŸ“š","title":"Weight Initialisation ðŸ“š","text":"Flux.glorot_uniform\nFlux.glorot_normal\nFlux.kaiming_uniform\nFlux.kaiming_normal\nFlux.truncated_normal\nFlux.orthogonal\nFlux.sparse_init\nFlux.identity_init\nFlux.ones32\nFlux.zeros32\nFlux.rand32\nFlux.randn32","category":"page"},{"location":"utilities/#Flux.glorot_uniform","page":"Weight Initialisation ðŸ“š","title":"Flux.glorot_uniform","text":"glorot_uniform([rng = default_rng_value()], size...; gain = 1) -> Array\nglorot_uniform([rng]; kw...) -> Function\n\nReturn an Array{Float32} of the given size containing random numbers drawn from a uniform distribution on the interval -x x, where x = gain * sqrt(6 / (fan_in + fan_out)).\n\nThis method is described in [1] and also known as Xavier initialization.\n\nExamples\n\njulia> Flux.glorot_uniform(3, 4) |> summary\n\"3Ã—4 Matrix{Float32}\"\n\njulia> round.(extrema(Flux.glorot_uniform(10, 100)), digits=3)\n(-0.232f0, 0.234f0)\n\njulia> round.(extrema(Flux.glorot_uniform(100, 10)), digits=3)\n(-0.233f0, 0.233f0)\n\njulia> round.(extrema(Flux.glorot_uniform(100, 100)), digits=3)\n(-0.173f0, 0.173f0)\n\njulia> Dense(3 => 2, tanh; init = Flux.glorot_uniform(MersenneTwister(1)))\nDense(3 => 2, tanh)  # 8 parameters\n\njulia> ans.bias\n2-element Vector{Float32}:\n 0.0\n 0.0\n\nReferences\n\n[1] Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010.\n\n\n\n\n\n","category":"function"},{"location":"utilities/#Flux.glorot_normal","page":"Weight Initialisation ðŸ“š","title":"Flux.glorot_normal","text":"glorot_normal([rng = default_rng_value(), size...; gain = 1) -> Array\nglorot_normal([rng]; kw...) -> Function\n\nReturn an Array{Float32} of the given size containing random numbers drawn from a normal distribution with standard deviation gain * sqrt(2 / (fan_in + fan_out)), using nfan.\n\nThis method is described in [1] and also known as Xavier initialization.\n\nExamples\n\njulia> using Statistics\n\njulia> round(std(Flux.glorot_normal(10, 1000)), digits=3)\n0.044f0\n\njulia> round(std(Flux.glorot_normal(1000, 10)), digits=3)\n0.044f0\n\njulia> round(std(Flux.glorot_normal(1000, 1000)), digits=3)\n0.032f0\n\njulia> Dense(10 => 1000, tanh; init = Flux.glorot_normal(gain=100))\nDense(10 => 1000, tanh)  # 11_000 parameters\n\njulia> round(std(ans.weight), sigdigits=3)\n4.45f0\n\nReferences\n\n[1] Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010.\n\n\n\n\n\n","category":"function"},{"location":"utilities/#Flux.kaiming_uniform","page":"Weight Initialisation ðŸ“š","title":"Flux.kaiming_uniform","text":"kaiming_uniform([rng = default_rng_value()], size...; gain = âˆš2) -> Array\nkaiming_uniform([rng]; kw...) -> Function\n\nReturn an Array{Float32} of the given size containing random numbers drawn from a uniform distribution on the interval [-x, x], where x = gain * sqrt(3/fan_in) using nfan.\n\nThis method is described in [1] and also known as He initialization.\n\nExamples\n\njulia> round.(extrema(Flux.kaiming_uniform(100, 10)), digits=3)\n(-0.774f0, 0.774f0)\n\njulia> round.(extrema(Flux.kaiming_uniform(10, 100)), digits=3)\n(-0.245f0, 0.244f0)\n\njulia> round.(extrema(Flux.kaiming_uniform(100, 100)), digits=3)\n(-0.245f0, 0.245f0)\n\nReferences\n\n[1] He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.\" Proceedings of the IEEE international conference on computer vision. 2015.\n\n\n\n\n\n","category":"function"},{"location":"utilities/#Flux.kaiming_normal","page":"Weight Initialisation ðŸ“š","title":"Flux.kaiming_normal","text":"kaiming_normal([rng = default_rng_value()], size...; gain = âˆš2) -> Array\nkaiming_normal([rng]; kw...) -> Function\n\nReturn an Array{Float32} of the given size containing random numbers taken from a normal distribution standard deviation gain / sqrt(fan_in), using nfan.\n\nThis method is described in [1] and also known as He initialization.\n\nExamples\n\njulia> using Statistics\n\njulia> round(std(Flux.kaiming_normal(10, 1000)), digits=3)\n0.045f0\n\njulia> round(std(Flux.kaiming_normal(1000, 10)), digits=3)\n0.447f0\n\njulia> round(std(Flux.kaiming_normal(1000, 1000)), digits=3)\n0.045f0\n\nReferences\n\n[1] He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.\" Proceedings of the IEEE international conference on computer vision. 2015.\n\n\n\n\n\n","category":"function"},{"location":"utilities/#Flux.truncated_normal","page":"Weight Initialisation ðŸ“š","title":"Flux.truncated_normal","text":"truncated_normal([rng = default_rng_value()], size...; mean = 0, std = 1, lo = -2, hi = 2) -> Array\ntruncated_normal([rng]; kw...) -> Function\n\nReturn an Array{Float32} of the given size where each element is drawn from a truncated normal distribution. The numbers are distributed like filter(x -> lo<=x<=hi, mean .+ std .* randn(100)).\n\nThe values are generated by sampling a Uniform(0, 1) (rand()) and then applying the inverse CDF of the truncated normal distribution. This method works best when lo â‰¤ mean â‰¤ hi.\n\nExamples\n\njulia> using Statistics\n\njulia> Flux.truncated_normal(3, 4) |> summary\n\"3Ã—4 Matrix{Float32}\"\n\njulia> round.(extrema(Flux.truncated_normal(10^6)); digits=3)\n(-2.0f0, 2.0f0)\n\njulia> round(std(Flux.truncated_normal(10^6; lo = -100, hi = 100)))\n1.0f0\n\n\n\n\n\n","category":"function"},{"location":"utilities/#Flux.orthogonal","page":"Weight Initialisation ðŸ“š","title":"Flux.orthogonal","text":"orthogonal([rng = default_rng_value()], size...; gain = 1) -> Array\northogonal([rng]; kw...) -> Function\n\nReturn an Array{Float32} of the given size which is a (semi) orthogonal matrix, as described in [1].\n\nCannot construct a vector, i.e. length(size) == 1 is forbidden. For length(size) > 2, a prod(size[1:(end - 1)]) by size[end] orthogonal matrix is computed before reshaping it to the original dimensions.\n\nExamples\n\njulia> W = Flux.orthogonal(5, 7);\n\njulia> summary(W)\n\"5Ã—7 Matrix{Float32}\"\n\njulia> W * W' â‰ˆ I(5)\ntrue\n\njulia> W2 = Flux.orthogonal(7, 5);\n\njulia> W2 * W2' â‰ˆ I(7)\nfalse\n\njulia> W2' * W2 â‰ˆ I(5)\ntrue\n\njulia> W3 = Flux.orthogonal(3, 3, 2, 4);\n\njulia> transpose(reshape(W3, :, 4)) * reshape(W3, :, 4) â‰ˆ I(4)\ntrue\n\nReferences\n\n[1] Saxe, McClelland, Ganguli. \"Exact solutions to the nonlinear dynamics of learning in deep linear neural networks\", ICLR 2014, https://arxiv.org/abs/1312.6120\n\n\n\n\n\n","category":"function"},{"location":"utilities/#Flux.sparse_init","page":"Weight Initialisation ðŸ“š","title":"Flux.sparse_init","text":"sparse_init([rng = default_rng_value()], rows, cols; sparsity, std = 0.01) -> Array\nsparse_init([rng]; kw...) -> Function\n\nReturn a Matrix{Float32} of size rows, cols where each column contains a fixed fraction of zero elements given by sparsity. Non-zero elements are normally distributed with a mean of zero and standard deviation std.\n\nThis method is described in [1].\n\nExamples\n\njulia> count(iszero, Flux.sparse_init(10, 10, sparsity=1/5))\n20\n\njulia> sum(0 .== Flux.sparse_init(10, 11, sparsity=0.9), dims=1)\n1Ã—11 Matrix{Int64}:\n 9  9  9  9  9  9  9  9  9  9  9\n\njulia> Dense(3 => 10, tanh; init=Flux.sparse_init(sparsity=0.5))\nDense(3 => 10, tanh)  # 40 parameters\n\njulia> count(iszero, ans.weight, dims=1)\n1Ã—3 Matrix{Int64}:\n 5  5  5\n\nReferences\n\n[1] Martens, J, \"Deep learning via Hessian-free optimization\" Proceedings of the 27th International Conference on International Conference on Machine Learning. 2010.\n\n\n\n\n\n","category":"function"},{"location":"utilities/#Flux.identity_init","page":"Weight Initialisation ðŸ“š","title":"Flux.identity_init","text":"identity_init(size...; gain=1, shift=0) -> Array\nidentity_init(; kw...) -> Function\n\nReturn an Array{Float32} of the given size which yields an identity mapping when used as parameters in most Flux layers. Use gain to scale the identity by a constant.\n\nOften useful in the context of transfer learning, i.e when one wants to add more capacity to a model but start from the same mapping.\n\nHas the following behaviour\n\n1D: A Vector of zeros (useful for an identity bias)\n2D: An identity matrix (useful for an identity matrix multiplication)\nMore than 2D: A dense block array of center tap spatial filters (useful for an identity convolution)\n\nSome caveats: \n\nNot all layers will be identity mapping when used with this init. Exceptions include recurrent layers and normalization layers.\nLayers must have input_size == output_size for identity mapping to be possible. When this is not the case, extra dimensions of the array are padded with zeros.\nFor convolutional layers, in addition to the above, the kernel sizes must also be odd and padding must be applied so that output feature maps have the same size as input feature maps, e.g by using SamePad.\n\nUse keyword shift (integer or tuple) to apply circular shift to the output, equivalent to Base.circshift(identity_init(size...), shift).\n\nFor consistency with other initialisers, it accepts rng::AbstractRNG as an optional first argument. But this is ignored, since the result is not random.\n\nExamples\n\njulia> Flux.identity_init(3,5)\n3Ã—5 Matrix{Float32}:\n 1.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0\n 0.0  0.0  1.0  0.0  0.0\n\njulia> Dense(5 => 3, relu, init=Flux.identity_init)([1,-2,3,-4,5])\n3-element Vector{Float32}:\n 1.0\n 0.0\n 3.0\n\njulia> Flux.identity_init(3,3,2; gain=100)\n3Ã—3Ã—2 Array{Float32, 3}:\n[:, :, 1] =\n   0.0  0.0  0.0\n 100.0  0.0  0.0\n   0.0  0.0  0.0\n\n[:, :, 2] =\n 0.0    0.0  0.0\n 0.0  100.0  0.0\n 0.0    0.0  0.0\n\njulia> x4 = cat([1 2 3; 4 5 6; 7 8 9]; dims=4);\n\njulia> Conv((2,2), 1 => 1, init=Flux.identity_init(gain=10), pad=SamePad())(x4)\n3Ã—3Ã—1Ã—1 Array{Float32, 4}:\n[:, :, 1, 1] =\n 10.0  20.0  30.0\n 40.0  50.0  60.0\n 70.0  80.0  90.0\n\n\n\n\n\n","category":"function"},{"location":"utilities/#Flux.ones32","page":"Weight Initialisation ðŸ“š","title":"Flux.ones32","text":"ones32(size...) = ones(Float32, size...)\n\nReturn an Array{Float32} of the given size filled with 1s.\n\n\n\n\n\n","category":"function"},{"location":"utilities/#Flux.zeros32","page":"Weight Initialisation ðŸ“š","title":"Flux.zeros32","text":"zeros32(size...) = zeros(Float32, size...)\n\nReturn an Array{Float32} of the given size filled with 0s.\n\n\n\n\n\n","category":"function"},{"location":"utilities/#Flux.rand32","page":"Weight Initialisation ðŸ“š","title":"Flux.rand32","text":"rand32([rng], size...)\n\nReturn an Array{Float32} of the given size, filled like rand. When the size is not provided, rand32(rng::AbstractRNG) returns a function.\n\n\n\n\n\n","category":"function"},{"location":"utilities/#Flux.randn32","page":"Weight Initialisation ðŸ“š","title":"Flux.randn32","text":"randn32([rng], size...)\n\nReturn an Array{Float32} of the given size, filled like randn. When the size is not provided, randn32(rng::AbstractRNG) returns a function.\n\n\n\n\n\n","category":"function"},{"location":"utilities/","page":"Weight Initialisation ðŸ“š","title":"Weight Initialisation ðŸ“š","text":"These functions call:","category":"page"},{"location":"utilities/","page":"Weight Initialisation ðŸ“š","title":"Weight Initialisation ðŸ“š","text":"Flux.rng_from_array\nFlux.default_rng_value\nFlux.nfan","category":"page"},{"location":"utilities/#Flux.rng_from_array","page":"Weight Initialisation ðŸ“š","title":"Flux.rng_from_array","text":"rng_from_array([x])\n\nCreate an instance of the RNG most appropriate for x. The current defaults are:\n\nx isa CuArray: CUDA.default_rng(), else:\nx isa AbstractArray, or no x provided:\nJulia version is < 1.7: Random.GLOBAL_RNG\nJulia version is >= 1.7: Random.default_rng()\n\n\n\n\n\n","category":"function"},{"location":"utilities/#Flux.default_rng_value","page":"Weight Initialisation ðŸ“š","title":"Flux.default_rng_value","text":"default_rng_value()\n\nCreate an instance of the default RNG depending on Julia's version.\n\nJulia version is < 1.7: Random.GLOBAL_RNG\nJulia version is >= 1.7: Random.default_rng()\n\n\n\n\n\n","category":"function"},{"location":"utilities/#Flux.nfan","page":"Weight Initialisation ðŸ“š","title":"Flux.nfan","text":"nfan(n_out, n_in=1) -> Tuple\nnfan(dims...)\nnfan(dims::Tuple)\n\nFor a layer characterized by dimensions dims, return a tuple (fan_in, fan_out), where fan_in is the number of input neurons connected to an output one, and fan_out is the number of output neurons connected to an input one.\n\nThis function is mainly used by weight initializers, e.g., kaiming_normal.\n\nExamples\n\njulia> layer = Dense(10, 20);\n\njulia> Flux.nfan(size(layer.weight))\n(10, 20)\n\njulia> layer = Conv((3, 3), 2=>10);\n\njulia> Flux.nfan(size(layer.weight))\n(18, 90)\n\n\n\n\n\n","category":"function"},{"location":"utilities/#Changing-the-type-of-all-parameters","page":"Weight Initialisation ðŸ“š","title":"Changing the type of all parameters","text":"","category":"section"},{"location":"utilities/","page":"Weight Initialisation ðŸ“š","title":"Weight Initialisation ðŸ“š","text":"The default eltype for models is Float32 since models are often trained/run on GPUs. The eltype of model m can be changed to Float64 by f64(m):","category":"page"},{"location":"utilities/","page":"Weight Initialisation ðŸ“š","title":"Weight Initialisation ðŸ“š","text":"Flux.f64\nFlux.f32","category":"page"},{"location":"utilities/#Flux.f64","page":"Weight Initialisation ðŸ“š","title":"Flux.f64","text":"f64(m)\n\nConverts the eltype of model's parameters to Float64. Recurses into structs marked with @functor.\n\n\n\n\n\n","category":"function"},{"location":"utilities/#Flux.f32","page":"Weight Initialisation ðŸ“š","title":"Flux.f32","text":"f32(m)\n\nConverts the eltype of model's parameters to Float32 (which is Flux's default). Recurses into structs marked with @functor.\n\n\n\n\n\n","category":"function"},{"location":"outputsize/#Shape-Inference","page":"Shape Inference ðŸ“š","title":"Shape Inference","text":"","category":"section"},{"location":"outputsize/","page":"Shape Inference ðŸ“š","title":"Shape Inference ðŸ“š","text":"Flux has some tools to help generate models in an automated fashion, by inferring the size of arrays that layers will recieve, without doing any computation.  This is especially useful for convolutional models, where the same Conv layer accepts any size of image, but the next layer may not. ","category":"page"},{"location":"outputsize/","page":"Shape Inference ðŸ“š","title":"Shape Inference ðŸ“š","text":"The higher-level tool is a macro @autosize which acts on the code defining the layers, and replaces each appearance of _ with the relevant size. This simple example returns a model with Dense(845 => 10) as the last layer:","category":"page"},{"location":"outputsize/","page":"Shape Inference ðŸ“š","title":"Shape Inference ðŸ“š","text":"@autosize (28, 28, 1, 32) Chain(Conv((3, 3), _ => 5, relu, stride=2), Flux.flatten, Dense(_ => 10))","category":"page"},{"location":"outputsize/","page":"Shape Inference ðŸ“š","title":"Shape Inference ðŸ“š","text":"The input size may be provided at runtime, like @autosize (sz..., 1, 32) Chain(Conv(..., but all the layer constructors containing _ must be explicitly written out â€“ the macro sees the code as written.","category":"page"},{"location":"outputsize/","page":"Shape Inference ðŸ“š","title":"Shape Inference ðŸ“š","text":"This macro relies on a lower-level function outputsize, which you can also use directly:","category":"page"},{"location":"outputsize/","page":"Shape Inference ðŸ“š","title":"Shape Inference ðŸ“š","text":"c = Conv((3, 3), 1 => 5, relu, stride=2)\nFlux.outputsize(c, (28, 28, 1, 32))  # returns (13, 13, 5, 32)","category":"page"},{"location":"outputsize/","page":"Shape Inference ðŸ“š","title":"Shape Inference ðŸ“š","text":"The function outputsize works by passing a \"dummy\" array into the model, which propagates through very cheaply. It should work for all layers, including custom layers, out of the box.","category":"page"},{"location":"outputsize/","page":"Shape Inference ðŸ“š","title":"Shape Inference ðŸ“š","text":"An example of how to automate model building is this:","category":"page"},{"location":"outputsize/","page":"Shape Inference ðŸ“š","title":"Shape Inference ðŸ“š","text":"\"\"\"\n    make_model(width, height, [inchannels, nclasses; layer_config])\n\nCreate a CNN for a given set of configuration parameters. Arguments:\n- `width`, `height`: the input image size in pixels\n- `inchannels`: the number of channels in the input image, default `1`\n- `nclasses`: the number of output classes, default `10`\n- Keyword `layer_config`: a vector of the number of channels per layer, default `[16, 16, 32, 64]`\n\"\"\"\nfunction make_model(width, height, inchannels = 1, nclasses = 10;\n                    layer_config = [16, 16, 32, 64])\n  # construct a vector of layers:\n  conv_layers = []\n  push!(conv_layers, Conv((5, 5), inchannels => layer_config[1], relu, pad=SamePad()))\n  for (inch, outch) in zip(layer_config, layer_config[2:end])\n    push!(conv_layers, Conv((3, 3), inch => outch, sigmoid, stride=2))\n  end\n\n  # compute the output dimensions after these conv layers:\n  conv_outsize = Flux.outputsize(conv_layers, (width, height, inchannels); padbatch=true)\n\n  # use this to define appropriate Dense layer:\n  last_layer = Dense(prod(conv_outsize) => nclasses)\n  return Chain(conv_layers..., Flux.flatten, last_layer)\nend\n\nm = make_model(28, 28, 3, layer_config = [9, 17, 33, 65])\n\nFlux.outputsize(m, (28, 28, 3, 42)) == (10, 42) == size(m(randn(Float32, 28, 28, 3, 42)))","category":"page"},{"location":"outputsize/","page":"Shape Inference ðŸ“š","title":"Shape Inference ðŸ“š","text":"Alternatively, using the macro, the definition of make_model could end with:","category":"page"},{"location":"outputsize/","page":"Shape Inference ðŸ“š","title":"Shape Inference ðŸ“š","text":"  # compute the output dimensions & construct appropriate Dense layer:\n  return @autosize (width, height, inchannels, 1) Chain(conv_layers..., Flux.flatten, Dense(_ => nclasses))\nend","category":"page"},{"location":"outputsize/#Listing","page":"Shape Inference ðŸ“š","title":"Listing","text":"","category":"section"},{"location":"outputsize/","page":"Shape Inference ðŸ“š","title":"Shape Inference ðŸ“š","text":"Flux.@autosize\nFlux.outputsize","category":"page"},{"location":"outputsize/#Flux.@autosize","page":"Shape Inference ðŸ“š","title":"Flux.@autosize","text":"@autosize (size...,) Chain(Layer(_ => 2), Layer(_), ...)\n\nReturns the specified model, with each _ replaced by an inferred number, for input of the given size.\n\nThe unknown sizes are usually the second-last dimension of that layer's input, which Flux regards as the channel dimension. (A few layers, Dense & LayerNorm, instead always use the first dimension.) The underscore may appear as an argument of a layer, or inside a =>. It may be used in further calculations, such as Dense(_ => _Ã·4).\n\nExamples\n\njulia> @autosize (3, 1) Chain(Dense(_ => 2, sigmoid), BatchNorm(_, affine=false))\nChain(\n  Dense(3 => 2, Ïƒ),                     # 8 parameters\n  BatchNorm(2, affine=false),\n) \n\njulia> img = [28, 28];\n\njulia> @autosize (img..., 1, 32) Chain(              # size is only needed at runtime\n          Chain(c = Conv((3,3), _ => 5; stride=2, pad=SamePad()),\n                p = MeanPool((3,3)),\n                b = BatchNorm(_),\n                f = Flux.flatten),\n          Dense(_ => _Ã·4, relu, init=Flux.rand32),   # can calculate output size _Ã·4\n          SkipConnection(Dense(_ => _, relu), +),\n          Dense(_ => 10),\n       )\nChain(\n  Chain(\n    c = Conv((3, 3), 1 => 5, pad=1, stride=2),  # 50 parameters\n    p = MeanPool((3, 3)),\n    b = BatchNorm(5),                   # 10 parameters, plus 10\n    f = Flux.flatten,\n  ),\n  Dense(80 => 20, relu),                # 1_620 parameters\n  SkipConnection(\n    Dense(20 => 20, relu),              # 420 parameters\n    +,\n  ),\n  Dense(20 => 10),                      # 210 parameters\n)         # Total: 10 trainable arrays, 2_310 parameters,\n          # plus 2 non-trainable, 10 parameters, summarysize 10.469 KiB.\n\njulia> outputsize(ans, (28, 28, 1, 32))\n(10, 32)\n\nLimitations:\n\nWhile @autosize (5, 32) Flux.Bilinear(_ => 7) is OK, something like Bilinear((_, _) => 7) will fail.\nWhile Scale(_) and LayerNorm(_) are fine (and use the first dimension), Scale(_,_) and LayerNorm(_,_) will fail if size(x,1) != size(x,2).\nRNNs won't work: @autosize (7, 11) LSTM(_ => 5) fails, because outputsize(RNN(3=>7), (3,)) also fails, a known issue.\n\n\n\n\n\n","category":"macro"},{"location":"outputsize/#Flux.outputsize","page":"Shape Inference ðŸ“š","title":"Flux.outputsize","text":"outputsize(m, x_size, y_size, ...; padbatch=false)\n\nFor model or layer m accepting multiple arrays as input, this returns size(m((x, y, ...))) given size_x = size(x), etc.\n\nExamples\n\njulia> x, y = rand(Float32, 5, 64), rand(Float32, 7, 64);\n\njulia> par = Parallel(vcat, Dense(5 => 9), Dense(7 => 11));\n\njulia> Flux.outputsize(par, (5, 64), (7, 64))\n(20, 64)\n\njulia> m = Chain(par, Dense(20 => 13), softmax);\n\njulia> Flux.outputsize(m, (5,), (7,); padbatch=true)\n(13, 1)\n\njulia> par(x, y) == par((x, y)) == Chain(par, identity)((x, y))\ntrue\n\nNotice that Chain only accepts multiple arrays as a tuple, while Parallel also accepts them as multiple arguments; outputsize always supplies the tuple.\n\n\n\n\n\n","category":"function"},{"location":"performance/#Performance-Tips","page":"Performance Tips","title":"Performance Tips","text":"","category":"section"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"All the usual Julia performance tips apply. As always profiling your code is generally a useful way of finding bottlenecks. Below follow some Flux specific tips/reminders.","category":"page"},{"location":"performance/#Don't-use-more-precision-than-you-need","page":"Performance Tips","title":"Don't use more precision than you need","text":"","category":"section"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"Flux works great with all kinds of number types. But often you do not need to be working with say Float64 (let alone BigFloat). Switching to Float32 can give you a significant speed up, not because the operations are faster, but because the memory usage is halved. Which means allocations occur much faster. And you use less memory.","category":"page"},{"location":"performance/#Preserve-inputs'-types","page":"Performance Tips","title":"Preserve inputs' types","text":"","category":"section"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"Not only should your activation and loss functions be type-stable, they should also preserve the type of their inputs.","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"A very artificial example using an activation function like","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"my_tanh(x) = Float64(tanh(x))","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"will result in performance on Float32 input orders of magnitude slower than the normal tanh would, because it results in having to use slow mixed type multiplication in the dense layers. Similar situations can occur in the loss function during backpropagation.","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"Which means if you change your data say from Float64 to Float32 (which should give a speedup: see above), you will see a large slow-down.","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"This can occur sneakily, because you can cause type-promotion by interacting with a numeric literals. E.g. the following will have run into the same problem as above:","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"leaky_tanh(x) = 0.01*x + tanh(x)","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"While one could change the activation function (e.g. to use 0.01f0*x), the idiomatic (and safe way)  to avoid type casts whenever inputs changes is to use oftype:","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"leaky_tanh(x) = oftype(x/1, 0.01)*x + tanh(x)","category":"page"},{"location":"performance/#Evaluate-batches-as-Matrices-of-features","page":"Performance Tips","title":"Evaluate batches as Matrices of features","text":"","category":"section"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"While it can sometimes be tempting to process your observations (feature vectors) one at a time e.g.","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"function loss_total(xs::AbstractVector{<:Vector}, ys::AbstractVector{<:Vector})\n    sum(zip(xs, ys)) do (x, y_target)\n        y_pred = model(x)  # evaluate the model\n        return loss(y_pred, y_target)\n    end\nend","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"It is much faster to concatenate them into a matrix, as this will hit BLAS matrix-matrix multiplication, which is much faster than the equivalent sequence of matrix-vector multiplications. The improvement is enough that it is worthwhile allocating new memory to store them contiguously.","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"x_batch = reduce(hcat, xs)\ny_batch = reduce(hcat, ys)\n...\nfunction loss_total(x_batch::Matrix, y_batch::Matrix)\n    y_preds = model(x_batch)\n    sum(loss.(y_preds, y_batch))\nend","category":"page"},{"location":"performance/","page":"Performance Tips","title":"Performance Tips","text":"When doing this kind of concatenation use reduce(hcat, xs) rather than hcat(xs...). This will avoid the splatting penalty, and will hit the optimised reduce method.","category":"page"},{"location":"data/mlutils/#Working-with-Data,-using-MLUtils.jl","page":"MLUtils.jl ðŸ“š (DataLoader, ...)","title":"Working with Data, using MLUtils.jl","text":"","category":"section"},{"location":"data/mlutils/","page":"MLUtils.jl ðŸ“š (DataLoader, ...)","title":"MLUtils.jl ðŸ“š (DataLoader, ...)","text":"Flux re-exports the DataLoader type and utility functions for working with data from MLUtils.","category":"page"},{"location":"data/mlutils/#DataLoader","page":"MLUtils.jl ðŸ“š (DataLoader, ...)","title":"DataLoader","text":"","category":"section"},{"location":"data/mlutils/","page":"MLUtils.jl ðŸ“š (DataLoader, ...)","title":"MLUtils.jl ðŸ“š (DataLoader, ...)","text":"The DataLoader can be used to create mini-batches of data, in the format train! expects.","category":"page"},{"location":"data/mlutils/","page":"MLUtils.jl ðŸ“š (DataLoader, ...)","title":"MLUtils.jl ðŸ“š (DataLoader, ...)","text":"Flux's website has a dedicated tutorial on DataLoader for more information. ","category":"page"},{"location":"data/mlutils/","page":"MLUtils.jl ðŸ“š (DataLoader, ...)","title":"MLUtils.jl ðŸ“š (DataLoader, ...)","text":"MLUtils.DataLoader","category":"page"},{"location":"data/mlutils/#MLUtils.DataLoader","page":"MLUtils.jl ðŸ“š (DataLoader, ...)","title":"MLUtils.DataLoader","text":"DataLoader(data; [batchsize, buffer, collate, parallel, partial, rng, shuffle])\n\nAn object that iterates over mini-batches of data, each mini-batch containing batchsize observations (except possibly the last one).\n\nTakes as input a single data array, a tuple (or a named tuple) of arrays, or in general any data object that implements the numobs and getobs methods.\n\nThe last dimension in each array is the observation dimension, i.e. the one divided into mini-batches.\n\nThe original data is preserved in the data field of the DataLoader.\n\nArguments\n\ndata: The data to be iterated over. The data type has to be supported by numobs and getobs.\nbatchsize: If less than 0, iterates over individual observations. Otherwise, each iteration (except possibly the last) yields a mini-batch containing batchsize observations. Default 1.\nbuffer: If buffer=true and supported by the type of data, a buffer will be allocated and reused for memory efficiency. You can also pass a preallocated object to buffer. Default false.\ncollate: Batching behavior. If nothing (default), a batch is getobs(data, indices). If false, each batch is  [getobs(data, i) for i in indices]. When true, applies batch to the vector of observations in a batch,   recursively collating arrays in the last dimensions. See batch for more information and examples.\nparallel: Whether to use load data in parallel using worker threads. Greatly   speeds up data loading by factor of available threads. Requires starting   Julia with multiple threads. Check Threads.nthreads() to see the number of   available threads. Passing parallel = true breaks ordering guarantees.   Default false.\npartial: This argument is used only when batchsize > 0. If partial=false and the number of observations is not divisible by the batchsize, then the last mini-batch is dropped. Default true.\nrng: A random number generator. Default Random.GLOBAL_RNG.\nshuffle: Whether to shuffle the observations before iterating. Unlike   wrapping the data container with shuffleobs(data), shuffle=true ensures   that the observations are shuffled anew every time you start iterating over   eachobs. Default false.\n\nExamples\n\njulia> Xtrain = rand(10, 100);\n\njulia> array_loader = DataLoader(Xtrain, batchsize=2);\n\njulia> for x in array_loader\n         @assert size(x) == (10, 2)\n         # do something with x, 50 times\n       end\n\njulia> array_loader.data === Xtrain\ntrue\n\njulia> tuple_loader = DataLoader((Xtrain,), batchsize=2);  # similar, but yielding 1-element tuples\n\njulia> for x in tuple_loader\n         @assert x isa Tuple{Matrix}\n         @assert size(x[1]) == (10, 2)\n       end\n\njulia> Ytrain = rand('a':'z', 100);  # now make a DataLoader yielding 2-element named tuples\n\njulia> train_loader = DataLoader((data=Xtrain, label=Ytrain), batchsize=5, shuffle=true);\n\njulia> for epoch in 1:100\n         for (x, y) in train_loader  # access via tuple destructuring\n           @assert size(x) == (10, 5)\n           @assert size(y) == (5,)\n           # loss += f(x, y) # etc, runs 100 * 20 times\n         end\n       end\n\njulia> first(train_loader).label isa Vector{Char}  # access via property name\ntrue\n\njulia> first(train_loader).label == Ytrain[1:5]  # because of shuffle=true\nfalse\n\njulia> foreach(printlnâˆ˜summary, DataLoader(rand(Int8, 10, 64), batchsize=30))  # partial=false would omit last\n10Ã—30 Matrix{Int8}\n10Ã—30 Matrix{Int8}\n10Ã—4 Matrix{Int8}\n\n\n\n\n\n","category":"type"},{"location":"data/mlutils/#Utility-Functions","page":"MLUtils.jl ðŸ“š (DataLoader, ...)","title":"Utility Functions","text":"","category":"section"},{"location":"data/mlutils/","page":"MLUtils.jl ðŸ“š (DataLoader, ...)","title":"MLUtils.jl ðŸ“š (DataLoader, ...)","text":"The utility functions are meant to be used while working with data; these functions help create inputs for your models or batch your dataset.","category":"page"},{"location":"data/mlutils/","page":"MLUtils.jl ðŸ“š (DataLoader, ...)","title":"MLUtils.jl ðŸ“š (DataLoader, ...)","text":"MLUtils.unsqueeze\nMLUtils.flatten\nMLUtils.stack\nMLUtils.unstack\nMLUtils.numobs\nMLUtils.getobs\nMLUtils.getobs!\nMLUtils.chunk\nMLUtils.group_counts\nMLUtils.group_indices\nMLUtils.batch\nMLUtils.unbatch\nMLUtils.batchseq\nMLUtils.rpad(v::AbstractVector, n::Integer, p)","category":"page"},{"location":"data/mlutils/#MLUtils.unsqueeze","page":"MLUtils.jl ðŸ“š (DataLoader, ...)","title":"MLUtils.unsqueeze","text":"unsqueeze(x; dims)\n\nReturn x reshaped into an array one dimensionality higher than x, where dims indicates in which dimension x is extended.\n\nSee also flatten, stack.\n\nExamples\n\njulia> unsqueeze([1 2; 3 4], dims=2)\n2Ã—1Ã—2 Array{Int64, 3}:\n[:, :, 1] =\n 1\n 3\n\n[:, :, 2] =\n 2\n 4\n\n\njulia> xs = [[1, 2], [3, 4], [5, 6]]\n3-element Vector{Vector{Int64}}:\n [1, 2]\n [3, 4]\n [5, 6]\n\njulia> unsqueeze(xs, dims=1)\n1Ã—3 Matrix{Vector{Int64}}:\n [1, 2]  [3, 4]  [5, 6]\n\n\n\n\n\nunsqueeze(; dims)\n\nReturns a function which, acting on an array, inserts a dimension of size 1 at dims.\n\nExamples\n\njulia> rand(21, 22, 23) |> unsqueeze(dims=2) |> size\n(21, 1, 22, 23)\n\n\n\n\n\n","category":"function"},{"location":"data/mlutils/#MLUtils.flatten","page":"MLUtils.jl ðŸ“š (DataLoader, ...)","title":"MLUtils.flatten","text":"flatten(x::AbstractArray)\n\nReshape arbitrarly-shaped input into a matrix-shaped output, preserving the size of the last dimension.\n\nSee also unsqueeze.\n\nExamples\n\njulia> rand(3,4,5) |> flatten |> size\n(12, 5)\n\n\n\n\n\n","category":"function"},{"location":"data/mlutils/#MLUtils.stack","page":"MLUtils.jl ðŸ“š (DataLoader, ...)","title":"MLUtils.stack","text":"stack(xs; dims)\n\nConcatenate the given array of arrays xs into a single array along the given dimension dims.\n\nSee also stack and batch.\n\nExamples\n\njulia> xs = [[1, 2], [3, 4], [5, 6]]\n3-element Vector{Vector{Int64}}:\n [1, 2]\n [3, 4]\n [5, 6]\n\njulia> stack(xs, dims=1)\n3Ã—2 Matrix{Int64}:\n 1  2\n 3  4\n 5  6\n\njulia> stack(xs, dims=2)\n2Ã—3 Matrix{Int64}:\n 1  3  5\n 2  4  6\n\njulia> stack(xs, dims=3)\n2Ã—1Ã—3 Array{Int64, 3}:\n[:, :, 1] =\n 1\n 2\n\n[:, :, 2] =\n 3\n 4\n\n[:, :, 3] =\n 5\n 6\n\n\n\n\n\n","category":"function"},{"location":"data/mlutils/#MLUtils.unstack","page":"MLUtils.jl ðŸ“š (DataLoader, ...)","title":"MLUtils.unstack","text":"unstack(xs; dims)\n\nUnroll the given xs into an array of arrays along the given dimension dims.\n\nSee also stack and unbatch.\n\nExamples\n\njulia> unstack([1 3 5 7; 2 4 6 8], dims=2)\n4-element Vector{Vector{Int64}}:\n [1, 2]\n [3, 4]\n [5, 6]\n [7, 8]\n\n\n\n\n\n","category":"function"},{"location":"data/mlutils/#MLUtils.numobs","page":"MLUtils.jl ðŸ“š (DataLoader, ...)","title":"MLUtils.numobs","text":"numobs(data)\n\nReturn the total number of observations contained in data.\n\nIf data does not have numobs defined,  then in the case of Tables.table(data) == true returns the number of rows, otherwise returns length(data).\n\nAuthors of custom data containers should implement Base.length for their type instead of numobs. numobs should only be implemented for types where there is a difference between numobs and Base.length (such as multi-dimensional arrays).\n\ngetobs supports by default nested combinations of array, tuple, named tuples, and dictionaries. \n\nSee also getobs.\n\nExamples\n\n\n# named tuples \nx = (a = [1, 2, 3], b = rand(6, 3))\nnumobs(x) == 3\n\n# dictionaries\nx = Dict(:a => [1, 2, 3], :b => rand(6, 3))\nnumobs(x) == 3\n\nAll internal containers must have the same number of observations:\n\njulia> x = (a = [1, 2, 3, 4], b = rand(6, 3));\n\njulia> numobs(x)\nERROR: DimensionMismatch: All data containers must have the same number of observations.\nStacktrace:\n [1] _check_numobs_error()\n   @ MLUtils ~/.julia/dev/MLUtils/src/observation.jl:163\n [2] _check_numobs\n   @ ~/.julia/dev/MLUtils/src/observation.jl:130 [inlined]\n [3] numobs(data::NamedTuple{(:a, :b), Tuple{Vector{Int64}, Matrix{Float64}}})\n   @ MLUtils ~/.julia/dev/MLUtils/src/observation.jl:177\n [4] top-level scope\n   @ REPL[35]:1\n\n\n\n\n\n","category":"function"},{"location":"data/mlutils/#MLUtils.getobs","page":"MLUtils.jl ðŸ“š (DataLoader, ...)","title":"MLUtils.getobs","text":"getobs(data, [idx])\n\nReturn the observations corresponding to the observation index idx. Note that idx can be any type as long as data has defined getobs for that type. If idx is not provided, then materialize all observations in data.\n\nIf data does not have getobs defined, then in the case of Tables.table(data) == true returns the row(s) in position idx, otherwise returns data[idx].\n\nAuthors of custom data containers should implement Base.getindex for their type instead of getobs. getobs should only be implemented for types where there is a difference between getobs and Base.getindex (such as multi-dimensional arrays).\n\nThe returned observation(s) should be in the form intended to be passed as-is to some learning algorithm. There is no strict interface requirement on how this \"actual data\" must look like. Every author behind some custom data container can make this decision themselves. The output should be consistent when idx is a scalar vs vector.\n\ngetobs supports by default nested combinations of array, tuple, named tuples, and dictionaries. \n\nSee also getobs! and numobs.\n\nExamples\n\n# named tuples \nx = (a = [1, 2, 3], b = rand(6, 3))\n\ngetobs(x, 2) == (a = 2, b = x.b[:, 2])\ngetobs(x, [1, 3]) == (a = [1, 3], b = x.b[:, [1, 3]])\n\n\n# dictionaries\nx = Dict(:a => [1, 2, 3], :b => rand(6, 3))\n\ngetobs(x, 2) == Dict(:a => 2, :b => x[:b][:, 2])\ngetobs(x, [1, 3]) == Dict(:a => [1, 3], :b => x[:b][:, [1, 3]])\n\n\n\n\n\n","category":"function"},{"location":"data/mlutils/#MLUtils.getobs!","page":"MLUtils.jl ðŸ“š (DataLoader, ...)","title":"MLUtils.getobs!","text":"getobs!(buffer, data, idx)\n\nInplace version of getobs(data, idx). If this method is defined for the type of data, then buffer should be used to store the result, instead of allocating a dedicated object.\n\nImplementing this function is optional. In the case no such method is provided for the type of data, then buffer will be ignored and the result of getobs returned. This could be because the type of data may not lend itself to the concept of copy!. Thus, supporting a custom getobs! is optional and not required.\n\nSee also getobs and numobs. \n\n\n\n\n\n","category":"function"},{"location":"data/mlutils/#MLUtils.chunk","page":"MLUtils.jl ðŸ“š (DataLoader, ...)","title":"MLUtils.chunk","text":"chunk(x, n; [dims])\nchunk(x; [size, dims])\n\nSplit x into n parts or alternatively, into equal chunks of size size. The parts contain  the same number of elements except possibly for the last one that can be smaller.\n\nIf x is an array, dims can be used to specify along which dimension to  split (defaults to the last dimension).\n\nExamples\n\njulia> chunk(1:10, 3)\n3-element Vector{UnitRange{Int64}}:\n 1:4\n 5:8\n 9:10\n\njulia> chunk(1:10; size = 2)\n5-element Vector{UnitRange{Int64}}:\n 1:2\n 3:4\n 5:6\n 7:8\n 9:10\n\njulia> x = reshape(collect(1:20), (5, 4))\n5Ã—4 Matrix{Int64}:\n 1   6  11  16\n 2   7  12  17\n 3   8  13  18\n 4   9  14  19\n 5  10  15  20\n\njulia> xs = chunk(x, 2, dims=1)\n2-element Vector{SubArray{Int64, 2, Matrix{Int64}, Tuple{UnitRange{Int64}, Base.Slice{Base.OneTo{Int64}}}, false}}:\n [1 6 11 16; 2 7 12 17; 3 8 13 18]\n [4 9 14 19; 5 10 15 20]\n\njulia> xs[1]\n3Ã—4 view(::Matrix{Int64}, 1:3, :) with eltype Int64:\n 1  6  11  16\n 2  7  12  17\n 3  8  13  18\n\njulia> xes = chunk(x; size = 2, dims = 2)\n2-element Vector{SubArray{Int64, 2, Matrix{Int64}, Tuple{Base.Slice{Base.OneTo{Int64}}, UnitRange{Int64}}, true}}:\n [1 6; 2 7; â€¦ ; 4 9; 5 10]\n [11 16; 12 17; â€¦ ; 14 19; 15 20]\n\njulia> xes[2]\n5Ã—2 view(::Matrix{Int64}, :, 3:4) with eltype Int64:\n 11  16\n 12  17\n 13  18\n 14  19\n 15  20\n\n\n\n\n\n","category":"function"},{"location":"data/mlutils/#MLUtils.group_counts","page":"MLUtils.jl ðŸ“š (DataLoader, ...)","title":"MLUtils.group_counts","text":"group_counts(x)\n\nCount the number of times that each element of x appears.\n\nSee also group_indices\n\nExamples\n\njulia> group_counts(['a', 'b', 'b'])\nDict{Char, Int64} with 2 entries:\n  'a' => 1\n  'b' => 2\n\n\n\n\n\n","category":"function"},{"location":"data/mlutils/#MLUtils.group_indices","page":"MLUtils.jl ðŸ“š (DataLoader, ...)","title":"MLUtils.group_indices","text":"group_indices(x) -> Dict\n\nComputes the indices of elements in the vector x for each distinct value contained.  This information is useful for resampling strategies, such as stratified sampling.\n\nSee also group_counts.\n\nExamples\n\njulia> x = [:yes, :no, :maybe, :yes];\n\njulia> group_indices(x)\nDict{Symbol, Vector{Int64}} with 3 entries:\n  :yes   => [1, 4]\n  :maybe => [3]\n  :no    => [2]\n\n\n\n\n\n","category":"function"},{"location":"data/mlutils/#MLUtils.batch","page":"MLUtils.jl ðŸ“š (DataLoader, ...)","title":"MLUtils.batch","text":"batch(xs)\n\nBatch the arrays in xs into a single array with  an extra dimension.\n\nIf the elements of xs are tuples, named tuples, or dicts,  the output will be of the same type. \n\nSee also unbatch.\n\nExamples\n\njulia> batch([[1,2,3], \n              [4,5,6]])\n3Ã—2 Matrix{Int64}:\n 1  4\n 2  5\n 3  6\n\njulia> batch([(a=[1,2], b=[3,4])\n               (a=[5,6], b=[7,8])]) \n(a = [1 5; 2 6], b = [3 7; 4 8])\n\n\n\n\n\n","category":"function"},{"location":"data/mlutils/#MLUtils.unbatch","page":"MLUtils.jl ðŸ“š (DataLoader, ...)","title":"MLUtils.unbatch","text":"unbatch(x)\n\nReverse of the batch operation, unstacking the last dimension of the array x.\n\nSee also unstack.\n\nExamples\n\njulia> unbatch([1 3 5 7;\n                     2 4 6 8])\n4-element Vector{Vector{Int64}}:\n [1, 2]\n [3, 4]\n [5, 6]\n [7, 8]\n\n\n\n\n\n","category":"function"},{"location":"data/mlutils/#MLUtils.batchseq","page":"MLUtils.jl ðŸ“š (DataLoader, ...)","title":"MLUtils.batchseq","text":"batchseq(seqs, val = 0)\n\nTake a list of N sequences, and turn them into a single sequence where each item is a batch of N. Short sequences will be padded by val.\n\nExamples\n\njulia> batchseq([[1, 2, 3], [4, 5]], 0)\n3-element Vector{Vector{Int64}}:\n [1, 4]\n [2, 5]\n [3, 0]\n\n\n\n\n\n","category":"function"},{"location":"models/advanced/#man-advanced","page":"Custom Layers","title":"Defining Customised Layers","text":"","category":"section"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"Here we will try and describe usage of some more advanced features that Flux provides to give more control over model building.","category":"page"},{"location":"models/advanced/#Custom-Model-Example","page":"Custom Layers","title":"Custom Model Example","text":"","category":"section"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"Here is a basic example of a custom model. It simply adds the input to the result from the neural network.","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"struct CustomModel\n  chain::Chain\nend\n\nfunction (m::CustomModel)(x)\n  # Arbitrary code can go here, but note that everything will be differentiated.\n  # Zygote does not allow some operations, like mutating arrays.\n\n  return m.chain(x) + x\nend\n\n# Call @functor to allow for training. Described below in more detail.\nFlux.@functor CustomModel","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"You can then use the model like:","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"chain = Chain(Dense(10, 10))\nmodel = CustomModel(chain)\nmodel(rand(10))","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"For an intro to Flux and automatic differentiation, see this tutorial.","category":"page"},{"location":"models/advanced/#Customising-Parameter-Collection-for-a-Model","page":"Custom Layers","title":"Customising Parameter Collection for a Model","text":"","category":"section"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"Taking reference from our example Affine layer from the basics.","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"By default all the fields in the Affine type are collected as its parameters, however, in some cases it may be desired to hold other metadata in our \"layers\" that may not be needed for training, and are hence supposed to be ignored while the parameters are collected. With Flux, it is possible to mark the fields of our layers that are trainable in two ways.","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"The first way of achieving this is through overloading the trainable function.","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"julia> @functor Affine\n\njulia> a = Affine(rand(3,3), rand(3))\nAffine{Array{Float64,2},Array{Float64,1}}([0.66722 0.774872 0.249809; 0.843321 0.403843 0.429232; 0.683525 0.662455 0.065297], [0.42394, 0.0170927, 0.544955])\n\njulia> Flux.params(a) # default behavior\nParams([[0.66722 0.774872 0.249809; 0.843321 0.403843 0.429232; 0.683525 0.662455 0.065297], [0.42394, 0.0170927, 0.544955]])\n\njulia> Flux.trainable(a::Affine) = (a.W,)\n\njulia> Flux.params(a)\nParams([[0.66722 0.774872 0.249809; 0.843321 0.403843 0.429232; 0.683525 0.662455 0.065297]])","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"Only the fields returned by trainable will be collected as trainable parameters of the layer when calling Flux.params.","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"Another way of achieving this is through the @functor macro directly. Here, we can mark the fields we are interested in by grouping them in the second argument:","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"Flux.@functor Affine (W,)","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"However, doing this requires the struct to have a corresponding constructor that accepts those parameters.","category":"page"},{"location":"models/advanced/#Freezing-Layer-Parameters","page":"Custom Layers","title":"Freezing Layer Parameters","text":"","category":"section"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"When it is desired to not include all the model parameters (for e.g. transfer learning), we can simply not pass in those layers into our call to params.","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"Consider a simple multi-layer perceptron model where we want to avoid optimising the first two Dense layers. We can obtain this using the slicing features Chain provides:","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"m = Chain(\n      Dense(784 => 64, relu),\n      Dense(64 => 64, relu),\n      Dense(32 => 10)\n    );\n\nps = Flux.params(m[3:end])","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"The Zygote.Params object ps now holds a reference to only the parameters of the layers passed to it.","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"During training, the gradients will only be computed for (and applied to) the last Dense layer, therefore only that would have its parameters changed.","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"Flux.params also takes multiple inputs to make it easy to collect parameters from heterogenous models with a single call. A simple demonstration would be if we wanted to omit optimising the second Dense layer in the previous example. It would look something like this:","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"Flux.params(m[1], m[3:end])","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"Sometimes, a more fine-tuned control is needed. We can freeze a specific parameter of a specific layer which already entered a Params object ps, by simply deleting it from ps:","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"ps = Flux.params(m)\ndelete!(ps, m[2].bias) ","category":"page"},{"location":"models/advanced/#Custom-multiple-input-or-output-layer","page":"Custom Layers","title":"Custom multiple input or output layer","text":"","category":"section"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"Sometimes a model needs to receive several separate inputs at once or produce several separate outputs at once. In other words, there multiple paths within this high-level layer, each processing a different input or producing a different output. A simple example of this in machine learning literature is the inception module.","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"Naively, we could have a struct that stores the weights of along each path and implement the joining/splitting in the forward pass function. But that would mean a new struct any time the operations along each path changes. Instead, this guide will show you how to construct a high-level layer (like Chain) that is made of multiple sub-layers for each path.","category":"page"},{"location":"models/advanced/#Multiple-inputs:-a-custom-Join-layer","page":"Custom Layers","title":"Multiple inputs: a custom Join layer","text":"","category":"section"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"Our custom Join layer will accept multiple inputs at once, pass each input through a separate path, then combine the results together. Note that this layer can already be constructed using Parallel, but we will first walk through how do this manually.","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"We start by defining a new struct, Join, that stores the different paths and a combine operation as its fields.","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"using Flux\nusing CUDA\n\n# custom join layer\nstruct Join{T, F}\n  combine::F\n  paths::T\nend\n\n# allow Join(op, m1, m2, ...) as a constructor\nJoin(combine, paths...) = Join(combine, paths)","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"Notice that we parameterized the type of the paths field. This is necessary for fast Julia code; in general, T might be a Tuple or Vector, but we don't need to pay attention to what it specifically is. The same goes for the combine field.","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"The next step is to use Functors.@functor to make our struct behave like a Flux layer. This is important so that calling params on a Join returns the underlying weight arrays on each path.","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"Flux.@functor Join","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"Finally, we define the forward pass. For Join, this means applying each path in paths to each input array, then using combine to merge the results.","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"(m::Join)(xs::Tuple) = m.combine(map((f, x) -> f(x), m.paths, xs)...)\n(m::Join)(xs...) = m(xs)","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"Lastly, we can test our new layer. Thanks to the proper abstractions in Julia, our layer works on GPU arrays out of the box!","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"model = Chain(\n              Join(vcat,\n                   Chain(Dense(1 => 5, relu), Dense(5 => 1)), # branch 1\n                   Dense(1 => 2),                             # branch 2\n                   Dense(1 => 1)                              # branch 3\n                  ),\n              Dense(4 => 1)\n             ) |> gpu\n\nxs = map(gpu, (rand(1), rand(1), rand(1)))\n\nmodel(xs)\n# returns a single float vector with one value","category":"page"},{"location":"models/advanced/#Using-Parallel","page":"Custom Layers","title":"Using Parallel","text":"","category":"section"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"Flux already provides Parallel that can offer the same functionality. In this case, Join is going to just be syntactic sugar for Parallel.","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"Join(combine, paths) = Parallel(combine, paths)\nJoin(combine, paths...) = Join(combine, paths)\n\n# use vararg/tuple version of Parallel forward pass\nmodel = Chain(\n              Join(vcat,\n                   Chain(Dense(1 => 5, relu), Dense(5 => 1)),\n                   Dense(1 => 2),\n                   Dense(1 => 1)\n                  ),\n              Dense(4 => 1)\n             ) |> gpu\n\nxs = map(gpu, (rand(1), rand(1), rand(1)))\n\nmodel(xs)\n# returns a single float vector with one value","category":"page"},{"location":"models/advanced/#Multiple-outputs:-a-custom-Split-layer","page":"Custom Layers","title":"Multiple outputs: a custom Split layer","text":"","category":"section"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"Our custom Split layer will accept a single input, then pass the input through a separate path to produce multiple outputs.","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"We start by following the same steps as the Join layer: define a struct, use Functors.@functor, and define the forward pass.","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"using Flux\nusing CUDA\n\n# custom split layer\nstruct Split{T}\n  paths::T\nend\n\nSplit(paths...) = Split(paths)\n\nFlux.@functor Split\n\n(m::Split)(x::AbstractArray) = map(f -> f(x), m.paths)","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"Now we can test to see that our Split does indeed produce multiple outputs.","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"model = Chain(\n              Dense(10 => 5),\n              Split(Dense(5 => 1, tanh), Dense(5 => 3, tanh), Dense(5 => 2))\n             ) |> gpu\n\nmodel(gpu(rand(10)))\n# returns a tuple with three float vectors","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"A custom loss function for the multiple outputs may look like this:","category":"page"},{"location":"models/advanced/","page":"Custom Layers","title":"Custom Layers","text":"using Statistics\n\n# assuming model returns the output of a Split\n# x is a single input\n# ys is a tuple of outputs\nfunction loss(x, ys, model)\n  # rms over all the mse\n  yÌ‚s = model(x)\n  return sqrt(mean(Flux.mse(y, yÌ‚) for (y, yÌ‚) in zip(ys, yÌ‚s)))\nend","category":"page"},{"location":"ecosystem/#The-Julia-Ecosystem-around-Flux","page":"Flux's Ecosystem","title":"The Julia Ecosystem around Flux","text":"","category":"section"},{"location":"ecosystem/","page":"Flux's Ecosystem","title":"Flux's Ecosystem","text":"One of the main strengths of Julia lies in an ecosystem of packages  globally providing a rich and consistent user experience.","category":"page"},{"location":"ecosystem/","page":"Flux's Ecosystem","title":"Flux's Ecosystem","text":"This is a non-exhaustive list of Julia packages, nicely complementing Flux in typical machine learning and deep learning workflows. To add your project please send a PR. See also academic work citing Flux or Zygote.","category":"page"},{"location":"ecosystem/#Flux-models","page":"Flux's Ecosystem","title":"Flux models","text":"","category":"section"},{"location":"ecosystem/","page":"Flux's Ecosystem","title":"Flux's Ecosystem","text":"Packages that are actual Flux models but are not available directly through the Flux package.","category":"page"},{"location":"ecosystem/#Computer-vision","page":"Flux's Ecosystem","title":"Computer vision","text":"","category":"section"},{"location":"ecosystem/","page":"Flux's Ecosystem","title":"Flux's Ecosystem","text":"ObjectDetector.jl provides ready-to-go image detection via YOLO.\nMetalhead.jl includes many state-of-the-art computer vision models which can easily be used for transfer learning.\nUNet.jl is a generic UNet implementation.","category":"page"},{"location":"ecosystem/#Natural-language-processing","page":"Flux's Ecosystem","title":"Natural language processing","text":"","category":"section"},{"location":"ecosystem/","page":"Flux's Ecosystem","title":"Flux's Ecosystem","text":"Transformers.jl provides components for Transformer models for NLP, as well as providing several trained models out of the box.\nTextAnalysis.jl provides several NLP algorithms that use Flux models under the hood.","category":"page"},{"location":"ecosystem/#Reinforcement-learning","page":"Flux's Ecosystem","title":"Reinforcement learning","text":"","category":"section"},{"location":"ecosystem/","page":"Flux's Ecosystem","title":"Flux's Ecosystem","text":"AlphaZero.jl provides a generic, simple and fast implementation of Deepmind's AlphaZero algorithm.\nReinforcementLearning.jl offers a collection of tools for doing reinforcement learning research in Julia.","category":"page"},{"location":"ecosystem/#Graph-learning","page":"Flux's Ecosystem","title":"Graph learning","text":"","category":"section"},{"location":"ecosystem/","page":"Flux's Ecosystem","title":"Flux's Ecosystem","text":"GraphNeuralNetworks.jl is a fresh, performant and flexible graph neural network library based on Flux.jl.\nGeometricFlux.jl is the first graph neural network library for julia. \nNeuralOperators.jl enables training infinite dimensional PDEs by learning a continuous function instead of using the finite element method.\nSeaPearl.jl is a Constraint Programming solver that uses Reinforcement Learning based on graphs as input.","category":"page"},{"location":"ecosystem/#Time-series","page":"Flux's Ecosystem","title":"Time series","text":"","category":"section"},{"location":"ecosystem/","page":"Flux's Ecosystem","title":"Flux's Ecosystem","text":"FluxArchitectures.jl is a collection of advanced network architectures for time series forecasting.","category":"page"},{"location":"ecosystem/#Tools-closely-associated-with-Flux","page":"Flux's Ecosystem","title":"Tools closely associated with Flux","text":"","category":"section"},{"location":"ecosystem/","page":"Flux's Ecosystem","title":"Flux's Ecosystem","text":"Utility tools you're unlikely to have met if you never used Flux!","category":"page"},{"location":"ecosystem/#High-level-training-flows","page":"Flux's Ecosystem","title":"High-level training flows","text":"","category":"section"},{"location":"ecosystem/","page":"Flux's Ecosystem","title":"Flux's Ecosystem","text":"FastAI.jl is a Julia port of Python's fast.ai library.\nFluxTraining.jl is a package for using and writing powerful, extensible training loops for deep learning models. It supports callbacks for many common use cases like hyperparameter scheduling, metrics tracking and logging, checkpointing, early stopping, and more. It powers training in FastAI.jl","category":"page"},{"location":"ecosystem/#Datasets","page":"Flux's Ecosystem","title":"Datasets","text":"","category":"section"},{"location":"ecosystem/","page":"Flux's Ecosystem","title":"Flux's Ecosystem","text":"Commonly used machine learning datasets are provided by the following packages in the julia ecosystem:","category":"page"},{"location":"ecosystem/","page":"Flux's Ecosystem","title":"Flux's Ecosystem","text":"MLDatasets.jl focuses on downloading, unpacking, and accessing benchmark datasets.\nGraphMLDatasets.jl: a library for machine learning datasets on graph.","category":"page"},{"location":"ecosystem/#Plumbing","page":"Flux's Ecosystem","title":"Plumbing","text":"","category":"section"},{"location":"ecosystem/","page":"Flux's Ecosystem","title":"Flux's Ecosystem","text":"Tools to put data into the right order for creating a model.","category":"page"},{"location":"ecosystem/","page":"Flux's Ecosystem","title":"Flux's Ecosystem","text":"Augmentor.jl is a real-time library augmentation library for increasing the number of training images.\nDataAugmentation.jl aims to make it easy to build stochastic, label-preserving augmentation pipelines for vision use cases involving images, keypoints and segmentation masks.\nMLUtils.jl (replaces MLDataUtils.jl and MLLabelUtils.jl) is a library for processing Machine Learning datasets.","category":"page"},{"location":"ecosystem/#Parameters","page":"Flux's Ecosystem","title":"Parameters","text":"","category":"section"},{"location":"ecosystem/","page":"Flux's Ecosystem","title":"Flux's Ecosystem","text":"Parameters.jl types with default field values, keyword constructors and (un-)pack macros.\nParameterSchedulers.jl standard scheduling policies for machine learning.","category":"page"},{"location":"ecosystem/#Differentiable-programming","page":"Flux's Ecosystem","title":"Differentiable programming","text":"","category":"section"},{"location":"ecosystem/","page":"Flux's Ecosystem","title":"Flux's Ecosystem","text":"Packages based on differentiable programming but not necessarily related to Machine Learning. ","category":"page"},{"location":"ecosystem/","page":"Flux's Ecosystem","title":"Flux's Ecosystem","text":"The SciML ecosystem uses Flux and Zygote to mix neural nets with differential equations, to get the best of black box and mechanistic modelling.\nDiffEqFlux.jl provides tools for creating Neural Differential Equations.\nFlux3D.jl shows off machine learning on 3D data.\nRayTracer.jl combines ML with computer vision via a differentiable renderer.\nDuckietown.jl Differentiable Duckietown simulator.\nThe Yao.jl project uses Flux and Zygote for Quantum Differentiable Programming.\nAtomicGraphNets.jl enables learning graph based models on atomic systems used in chemistry.\nDiffImages.jl differentiable computer vision modeling in Julia with the Images.jl ecosystem.","category":"page"},{"location":"ecosystem/#Probabilistic-programming","page":"Flux's Ecosystem","title":"Probabilistic programming","text":"","category":"section"},{"location":"ecosystem/","page":"Flux's Ecosystem","title":"Flux's Ecosystem","text":"Turing.jl extends Flux's differentiable programming capabilities to probabilistic programming.\nOmega.jl is a research project aimed at causal, higher-order probabilistic programming.\nStheno.jl provides flexible Gaussian processes.","category":"page"},{"location":"ecosystem/#Statistics","page":"Flux's Ecosystem","title":"Statistics","text":"","category":"section"},{"location":"ecosystem/","page":"Flux's Ecosystem","title":"Flux's Ecosystem","text":"OnlineStats.jl provides single-pass algorithms for statistics.","category":"page"},{"location":"ecosystem/#Useful-miscellaneous-packages","page":"Flux's Ecosystem","title":"Useful miscellaneous packages","text":"","category":"section"},{"location":"ecosystem/","page":"Flux's Ecosystem","title":"Flux's Ecosystem","text":"Some useful and random packages!","category":"page"},{"location":"ecosystem/","page":"Flux's Ecosystem","title":"Flux's Ecosystem","text":"AdversarialPrediction.jl provides a way to easily optimize generic performance metrics in supervised learning settings using the Adversarial Prediction framework.\nMill.jl helps to prototype flexible multi-instance learning models.\nMLMetrics.jl is a utility for scoring models in data science and machine learning.\nTorch.jl exposes torch in Julia.\nValueHistories.jl is a utility for efficient tracking of optimization histories, training curves or other information of arbitrary types and at arbitrarily spaced sampling times.\nInvertibleNetworks.jl Building blocks for invertible neural networks in the Julia programming language.\nProgressMeter.jl progress meters for long-running computations.\nTensorBoardLogger.jl easy peasy logging to tensorboard in Julia\nArgParse.jl is a package for parsing command-line arguments to Julia programs.\nBSON.jl is a package for working with the Binary JSON serialisation format.\nDataFrames.jl in-memory tabular data in Julia.\nDrWatson.jl is a scientific project assistant software.","category":"page"},{"location":"ecosystem/","page":"Flux's Ecosystem","title":"Flux's Ecosystem","text":"This tight integration among Julia packages is shown in some of the examples in the model-zoo repository.","category":"page"},{"location":"models/functors/#Recursive-transformations-from-Functors.jl","page":"Functors.jl ðŸ“š (fmap, ...)","title":"Recursive transformations from Functors.jl","text":"","category":"section"},{"location":"models/functors/","page":"Functors.jl ðŸ“š (fmap, ...)","title":"Functors.jl ðŸ“š (fmap, ...)","text":"Flux models are deeply nested structures, and Functors.jl provides tools needed to explore such objects, apply functions to the parameters they contain, and re-build them.","category":"page"},{"location":"models/functors/","page":"Functors.jl ðŸ“š (fmap, ...)","title":"Functors.jl ðŸ“š (fmap, ...)","text":"New layers should be annotated using the Functors.@functor macro. This will enable params to see the parameters inside, and gpu to move them to the GPU.","category":"page"},{"location":"models/functors/","page":"Functors.jl ðŸ“š (fmap, ...)","title":"Functors.jl ðŸ“š (fmap, ...)","text":"Functors.jl has its own notes on basic usage for more details. Additionally, the Advanced Model Building and Customisation page covers the use cases of Functors in greater details.","category":"page"},{"location":"models/functors/","page":"Functors.jl ðŸ“š (fmap, ...)","title":"Functors.jl ðŸ“š (fmap, ...)","text":"Functors.@functor\nFunctors.fmap\nFunctors.isleaf\nFunctors.children\nFunctors.fcollect\nFunctors.functor\nFunctors.fmapstructure","category":"page"},{"location":"models/functors/#Functors.fmap","page":"Functors.jl ðŸ“š (fmap, ...)","title":"Functors.fmap","text":"fmap(f, x; exclude = Functors.isleaf, walk = Functors._default_walk)\n\nA structure and type preserving map.\n\nBy default it transforms every leaf node (identified by exclude, default isleaf) by applying f, and otherwise traverses x recursively using functor.\n\nExamples\n\njulia> fmap(string, (x=1, y=(2, 3)))\n(x = \"1\", y = (\"2\", \"3\"))\n\njulia> nt = (a = [1,2], b = [23, (45,), (x=6//7, y=())], c = [8,9]);\n\njulia> fmap(println, nt)\n[1, 2]\n23\n45\n6//7\n()\n[8, 9]\n(a = nothing, b = Any[nothing, (nothing,), (x = nothing, y = nothing)], c = nothing)\n\njulia> fmap(println, nt; exclude = x -> x isa Array)\n[1, 2]\nAny[23, (45,), (x = 6//7, y = ())]\n[8, 9]\n(a = nothing, b = nothing, c = nothing)\n\njulia> twice = [1, 2];\n\njulia> fmap(println, (i = twice, ii = 34, iii = [5, 6], iv = (twice, 34), v = 34.0))\n[1, 2]\n34\n[5, 6]\n34.0\n(i = nothing, ii = nothing, iii = nothing, iv = (nothing, nothing), v = nothing)\n\nIf the same node (same according to ===) appears more than once, it will only be handled once, and only be transformed once with f. Thus the result will also have this relationship.\n\nBy default, Tuples, NamedTuples, and some other container-like types in Base have children to recurse into. Arrays of numbers do not. To enable recursion into new types, you must provide a method of functor, which can be done using the macro @functor:\n\njulia> struct Foo; x; y; end\n\njulia> @functor Foo\n\njulia> struct Bar; x; end\n\njulia> @functor Bar\n\njulia> m = Foo(Bar([1,2,3]), (4, 5, Bar(Foo(6, 7))));\n\njulia> fmap(x -> 10x, m)\nFoo(Bar([10, 20, 30]), (40, 50, Bar(Foo(60, 70))))\n\njulia> fmap(string, m)\nFoo(Bar(\"[1, 2, 3]\"), (\"4\", \"5\", Bar(Foo(\"6\", \"7\"))))\n\njulia> fmap(string, m, exclude = v -> v isa Bar)\nFoo(\"Bar([1, 2, 3])\", (4, 5, \"Bar(Foo(6, 7))\"))\n\nTo recurse into custom types without reconstructing them afterwards, use fmapstructure.\n\nFor advanced customization of the traversal behaviour, pass a custom walk function of the form (f', xs) -> .... This function walks (maps) over xs calling the continuation f' to continue traversal.\n\njulia> fmap(x -> 10x, m, walk=(f, x) -> x isa Bar ? x : Functors._default_walk(f, x))\nFoo(Bar([1, 2, 3]), (40, 50, Bar(Foo(6, 7))))\n\nThe behaviour when the same node appears twice can be altered by giving a value to the prune keyword, which is then used in place of all but the first:\n\njulia> twice = [1, 2];\n\njulia> fmap(float, (x = twice, y = [1,2], z = twice); prune = missing)\n(x = [1.0, 2.0], y = [1.0, 2.0], z = missing)\n\n\n\n\n\n","category":"function"},{"location":"models/functors/#Functors.isleaf","page":"Functors.jl ðŸ“š (fmap, ...)","title":"Functors.isleaf","text":"Functors.isleaf(x)\n\nReturn true if x has no children according to functor.\n\nExamples\n\njulia> Functors.isleaf(1)\ntrue\n\njulia> Functors.isleaf([2, 3, 4])\ntrue\n\njulia> Functors.isleaf([\"five\", [6, 7]])\nfalse\n\njulia> Functors.isleaf([])\nfalse\n\njulia> Functors.isleaf((8, 9))\nfalse\n\njulia> Functors.isleaf(())\ntrue\n\n\n\n\n\n","category":"function"},{"location":"models/functors/#Functors.children","page":"Functors.jl ðŸ“š (fmap, ...)","title":"Functors.children","text":"Functors.children(x)\n\nReturn the children of x as defined by functor. Equivalent to functor(x)[1].\n\n\n\n\n\n","category":"function"},{"location":"models/functors/#Functors.fcollect","page":"Functors.jl ðŸ“š (fmap, ...)","title":"Functors.fcollect","text":"fcollect(x; exclude = v -> false)\n\nTraverse x by recursing each child of x as defined by functor and collecting the results into a flat array, ordered by a breadth-first traversal of x, respecting the iteration order of children calls.\n\nDoesn't recurse inside branches rooted at nodes v for which exclude(v) == true. In such cases, the root v is also excluded from the result. By default, exclude always yields false.\n\nSee also children.\n\nExamples\n\njulia> struct Foo; x; y; end\n\njulia> @functor Foo\n\njulia> struct Bar; x; end\n\njulia> @functor Bar\n\njulia> struct NoChildren; x; y; end\n\njulia> m = Foo(Bar([1,2,3]), NoChildren(:a, :b))\nFoo(Bar([1, 2, 3]), NoChildren(:a, :b))\n\njulia> fcollect(m)\n4-element Vector{Any}:\n Foo(Bar([1, 2, 3]), NoChildren(:a, :b))\n Bar([1, 2, 3])\n [1, 2, 3]\n NoChildren(:a, :b)\n\njulia> fcollect(m, exclude = v -> v isa Bar)\n2-element Vector{Any}:\n Foo(Bar([1, 2, 3]), NoChildren(:a, :b))\n NoChildren(:a, :b)\n\njulia> fcollect(m, exclude = v -> Functors.isleaf(v))\n2-element Vector{Any}:\n Foo(Bar([1, 2, 3]), NoChildren(:a, :b))\n Bar([1, 2, 3])\n\n\n\n\n\n","category":"function"},{"location":"models/functors/#Functors.functor","page":"Functors.jl ðŸ“š (fmap, ...)","title":"Functors.functor","text":"Functors.functor(x) = functor(typeof(x), x)\n\nReturns a tuple containing, first, a NamedTuple of the children of x (typically its fields), and second, a reconstruction funciton. This controls the behaviour of fmap.\n\nMethods should be added to functor(::Type{T}, x) for custom types, usually using the macro @functor.\n\n\n\n\n\n","category":"function"},{"location":"models/functors/#Functors.fmapstructure","page":"Functors.jl ðŸ“š (fmap, ...)","title":"Functors.fmapstructure","text":"fmapstructure(f, x; exclude = isleaf)\n\nLike fmap, but doesn't preserve the type of custom structs. Instead, it returns a NamedTuple (or a Tuple, or an array), or a nested set of these.\n\nUseful for when the output must not contain custom structs.\n\nExamples\n\njulia> struct Foo; x; y; end\n\njulia> @functor Foo\n\njulia> m = Foo([1,2,3], [4, (5, 6), Foo(7, 8)]);\n\njulia> fmapstructure(x -> 2x, m)\n(x = [2, 4, 6], y = Any[8, (10, 12), (x = 14, y = 16)])\n\njulia> fmapstructure(println, m)\n[1, 2, 3]\n4\n5\n6\n7\n8\n(x = nothing, y = Any[nothing, (nothing, nothing), (x = nothing, y = nothing)])\n\n\n\n\n\n","category":"function"},{"location":"models/overview/#man-overview","page":"Fitting a Line","title":"Flux Overview: Fitting a Straight Line","text":"","category":"section"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"Flux is a pure Julia ML stack that allows you to build predictive models. Here are the steps for a typical Flux program:","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"Provide training and test data\nBuild a model with configurable parameters to make predictions\nIteratively train the model by tweaking the parameters to improve predictions\nVerify your model","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"Under the hood, Flux uses a technique called automatic differentiation to take gradients that help improve predictions. Flux is also fully written in Julia so you can easily replace any layer of Flux with your own code to improve your understanding or satisfy special requirements.","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"Here's how you'd use Flux to build and train the most basic of models, step by step.","category":"page"},{"location":"models/overview/#A-Trivial-Prediction","page":"Fitting a Line","title":"A Trivial Prediction","text":"","category":"section"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"This example will predict the output of the function 4x + 2. Making such predictions is called \"linear regression\", and is really too simple to need a neural network. But it's a nice toy example.","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"First, import Flux and define the function we want to simulate:","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"julia> using Flux\n\njulia> actual(x) = 4x + 2\nactual (generic function with 1 method)","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"This example will build a model to approximate the actual function.","category":"page"},{"location":"models/overview/#.-Provide-Training-and-Test-Data","page":"Fitting a Line","title":"1. Provide Training and Test Data","text":"","category":"section"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"Use the actual function to build sets of data for training and verification:","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"julia> x_train, x_test = hcat(0:5...), hcat(6:10...)\n([0 1 â€¦ 4 5], [6 7 â€¦ 9 10])\n\njulia> y_train, y_test = actual.(x_train), actual.(x_test)\n([2 6 â€¦ 18 22], [26 30 â€¦ 38 42])","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"Normally, your training and test data come from real world observations, but here we simulate them.","category":"page"},{"location":"models/overview/#.-Build-a-Model-to-Make-Predictions","page":"Fitting a Line","title":"2. Build a Model to Make Predictions","text":"","category":"section"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"Now, build a model to make predictions with 1 input and 1 output:","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"julia> model = Dense(1 => 1)\nDense(1 => 1)       # 2 parameters\n\njulia> model.weight\n1Ã—1 Matrix{Float32}:\n 0.95041317\n\njulia> model.bias\n1-element Vector{Float32}:\n 0.0","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"Under the hood, a dense layer is a struct with fields weight and bias. weight represents a weights' matrix and bias represents a bias vector. There's another way to think about a model. In Flux, models are conceptually predictive functions: ","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"julia> predict = Dense(1 => 1)\nDense(1 => 1)       # 2 parameters","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"Dense(1 => 1) also implements the function Ïƒ(Wx+b) where W and b are the weights and biases. Ïƒ is an activation function (more on activations later). Our model has one weight and one bias, but typical models will have many more. Think of weights and biases as knobs and levers Flux can use to tune predictions. Activation functions are transformations that tailor models to your needs. ","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"This model will already make predictions, though not accurate ones yet:","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"julia> predict(x_train)\n1Ã—6 Matrix{Float32}:\n 0.0  0.906654  1.81331  2.71996  3.62662  4.53327","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"In order to make better predictions, you'll need to provide a loss function to tell Flux how to objectively evaluate the quality of a prediction. Loss functions compute the cumulative distance between actual values and predictions. ","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"julia> using Statistics\n\njulia> loss(model, x, y) = mean(abs2.(model(x) .- y));\n\njulia> loss(predict, x_train, y_train)\n122.64734f0","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"More accurate predictions will yield a lower loss. You can write your own loss functions or rely on those already provided by Flux. This loss function is called mean squared error (and built-in as mse). Flux works by iteratively reducing the loss through training.","category":"page"},{"location":"models/overview/#.-Improve-the-Prediction","page":"Fitting a Line","title":"3. Improve the Prediction","text":"","category":"section"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"Under the hood, the Flux Flux.train! function uses a loss function and training data to improve the parameters of your model based on a pluggable optimiser:","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"julia> using Flux: train!\n\njulia> opt = Descent()\nDescent(0.1)\n\njulia> data = [(x_train, y_train)]\n1-element Vector{Tuple{Matrix{Int64}, Matrix{Int64}}}:\n ([0 1 â€¦ 4 5], [2 6 â€¦ 18 22])","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"Now, we have the optimiser and data we'll pass to train!. All that remains are the parameters of the model. Remember, each model is a Julia struct with a function and configurable parameters. Remember, the dense layer has weights and biases that depend on the dimensions of the inputs and outputs: ","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"julia> predict.weight\n1Ã—1 Matrix{Float32}:\n 0.9066542\n\njulia> predict.bias\n1-element Vector{Float32}:\n 0.0","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"The dimensions of these model parameters depend on the number of inputs and outputs.","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"Flux will adjust predictions by iteratively changing these parameters according to the optimizer.","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"This optimiser implements the classic gradient descent strategy. Now improve the parameters of the model with a call to Flux.train! like this:","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"julia> train!(loss, predict, data, opt)","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"And check the loss:","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"julia> loss(predict, x_train, y_train)\n116.38745f0","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"It went down. Why? ","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"julia> predict.weight, predict.bias\n(Float32[7.5777884], Float32[1.9466728])","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"The parameters have changed. This single step is the essence of machine learning.","category":"page"},{"location":"models/overview/#.-Iteratively-Train-the-Model","page":"Fitting a Line","title":"3+. Iteratively Train the Model","text":"","category":"section"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"In the previous section, we made a single call to train! which iterates over the data we passed in just once. An epoch refers to one pass over the dataset. Typically, we will run the training for multiple epochs to drive the loss down even further. Let's run it a few more times:","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"julia> for epoch in 1:200\n         train!(loss, predict, data, opt)\n       end\n\njulia> loss(predict, x_train, y_train)\n0.00339581f0\n\njulia> predict.weight, predict.bias\n(Float32[4.0178537], Float32[2.0050256])","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"After 200 training steps, the loss went down, and the parameters are getting close to those in the function the model is built to predict.","category":"page"},{"location":"models/overview/#.-Verify-the-Results","page":"Fitting a Line","title":"4. Verify the Results","text":"","category":"section"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"Now, let's verify the predictions:","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"julia> predict(x_test)\n1Ã—5 Matrix{Float32}:\n 26.1121  30.13  34.1479  38.1657  42.1836\n\njulia> y_test\n1Ã—5 Matrix{Int64}:\n 26  30  34  38  42","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"The predictions are good. Here's how we got there. ","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"First, we gathered real-world data into the variables x_train, y_train, x_test, and y_test. The x_* data defines inputs, and the y_* data defines outputs. The *_train data is for training the model, and the *_test data is for verifying the model. Our data was based on the function 4x + 2.","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"Then, we built a single input, single output predictive model, predict = Dense(1 => 1). The initial predictions weren't accurate, because we had not trained the model yet.","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"After building the model, we trained it with train!(loss, predict, data, opt). The loss function is first, followed by the model itself, the training data, and the Descent optimizer provided by Flux. We ran the training step once, and observed that the parameters changed and the loss went down. Then, we ran the train! many times to finish the training process.","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"After we trained the model, we verified it with the test data to verify the results. ","category":"page"},{"location":"models/overview/","page":"Fitting a Line","title":"Fitting a Line","text":"This overall flow represents how Flux works. Let's drill down a bit to understand what's going on inside the individual layers of Flux.","category":"page"},{"location":"models/nnlib/#Neural-Network-primitives-from-NNlib.jl","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"Neural Network primitives from NNlib.jl","text":"","category":"section"},{"location":"models/nnlib/","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.jl ðŸ“š (softmax, conv, ...)","text":"Flux re-exports all of the functions exported by the NNlib package. This includes activation functions, described on the next page. Many of the functions on this page exist primarily as the internal implementation of Flux layer, but can also be used independently.","category":"page"},{"location":"models/nnlib/#Softmax","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"Softmax","text":"","category":"section"},{"location":"models/nnlib/","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.jl ðŸ“š (softmax, conv, ...)","text":"Flux's logitcrossentropy uses NNlib.softmax internally.","category":"page"},{"location":"models/nnlib/","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.jl ðŸ“š (softmax, conv, ...)","text":"softmax\nlogsoftmax","category":"page"},{"location":"models/nnlib/#NNlib.softmax","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.softmax","text":"softmax(x; dims = 1)\n\nSoftmax turns input array x into probability distributions that sum to 1 along the dimensions specified by dims. It is semantically equivalent to the following:\n\nsoftmax(x; dims = 1) = exp.(x) ./ sum(exp.(x), dims = dims)\n\nwith additional manipulations enhancing numerical stability.\n\nFor a matrix input x it will by default (dims = 1) treat it as a batch of vectors, with each column independent. Keyword dims = 2 will instead treat rows independently, and so on.\n\nSee also logsoftmax.\n\nExamples\n\njulia> softmax([1, 2, 3])\n3-element Vector{Float64}:\n 0.09003057317038046\n 0.24472847105479764\n 0.6652409557748218\n\njulia> softmax([1 2 3; 2 2 2])  # dims=1\n2Ã—3 Matrix{Float64}:\n 0.268941  0.5  0.731059\n 0.731059  0.5  0.268941\n\njulia> softmax([1 2 3; 2 2 2]; dims=2)\n2Ã—3 Matrix{Float64}:\n 0.0900306  0.244728  0.665241\n 0.333333   0.333333  0.333333\n\nNote that, when used with Flux.jl, softmax must not be passed to layers like Dense which accept an activation function. The activation is broadcasted over the result, thus applies to individual numbers. But softmax always needs to see the whole column.\n\njulia> using Flux\n\njulia> x = randn(Float32, 4, 4, 3, 13);\n\njulia> model = Chain(Conv((4, 4), 3 => 8, tanh), Flux.flatten, Dense(8 => 7), softmax);\n\njulia> model(x) |> size\n(7, 13)\n\njulia> Dense(4 => 7, softmax)(x)\nERROR: `softmax(x)` called with a number, but it expects an array. \n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#NNlib.logsoftmax","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.logsoftmax","text":"logsoftmax(x; dims = 1)\n\nComputes the log of softmax in a more numerically stable way than directly taking log.(softmax(xs)). Commonly used in computing cross entropy loss.\n\nIt is semantically equivalent to the following:\n\nlogsoftmax(x; dims = 1) = x .- log.(sum(exp.(x), dims = dims))\n\nSee also softmax.\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#Pooling","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"Pooling","text":"","category":"section"},{"location":"models/nnlib/","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.jl ðŸ“š (softmax, conv, ...)","text":"Flux's AdaptiveMaxPool, AdaptiveMeanPool, GlobalMaxPool, GlobalMeanPool, MaxPool, and MeanPool use NNlib.PoolDims, NNlib.maxpool, and NNlib.meanpool as their backend.","category":"page"},{"location":"models/nnlib/","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.jl ðŸ“š (softmax, conv, ...)","text":"PoolDims\nmaxpool\nmeanpool","category":"page"},{"location":"models/nnlib/#NNlib.PoolDims","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.PoolDims","text":"PoolDims(x_size::NTuple{M}, k::Union{NTuple{L, Int}, Int};\n        stride=k, padding=0, dilation=1)  where {M, L}\n\nDimensions for a \"pooling\" operation that can have an arbitrary input size, kernel size, stride, dilation, and channel count.  Used to dispatch onto efficient implementations at compile-time.\n\n\n\n\n\n","category":"type"},{"location":"models/nnlib/#NNlib.maxpool","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.maxpool","text":"maxpool(x, k::NTuple; pad=0, stride=k)\n\nPerform max pool operation with window size k on input tensor x.\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#NNlib.meanpool","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.meanpool","text":"meanpool(x, k::NTuple; pad=0, stride=k)\n\nPerform mean pool operation with window size k on input tensor x.\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#Padding","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"Padding","text":"","category":"section"},{"location":"models/nnlib/","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.jl ðŸ“š (softmax, conv, ...)","text":"pad_reflect\npad_constant\npad_repeat\npad_zeros","category":"page"},{"location":"models/nnlib/#NNlib.pad_reflect","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.pad_reflect","text":"pad_reflect(x, pad::Tuple; [dims])\npad_reflect(x, pad::Int; [dims])\n\nPad the array x reflecting its values across the border.\n\npad can a tuple of integers (l1, r1, ..., ln, rn) of some length 2n that specifies the left and right padding size for each of the dimensions in dims. If dims is not given,  it defaults to the first n dimensions.\n\nFor integer pad input instead, it is applied on both sides on every dimension in dims. In this case, dims  defaults to the first ndims(x)-2 dimensions  (i.e. excludes the channel and batch dimension).\n\nSee also pad_repeat and pad_constant.\n\njulia> r = reshape(1:9, 3, 3)\n3Ã—3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\n 1  4  7\n 2  5  8\n 3  6  9\n\njulia> pad_reflect(r, (1,2,1,2))\n6Ã—6 Matrix{Int64}:\n 5  2  5  8  5  2\n 4  1  4  7  4  1\n 5  2  5  8  5  2\n 6  3  6  9  6  3\n 5  2  5  8  5  2\n 4  1  4  7  4  1\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#NNlib.pad_constant","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.pad_constant","text":"pad_constant(x, pad::Tuple, val = 0; [dims = :])\npad_constant(x, pad::Int, val = 0; [dims = :])\n\nPad the array x with the constant value val.\n\npad can be a tuple of integers. If it is of some length 2 * length(dims) that specifies the left and right padding size for each of the dimensions in dims as (l1, r1, ..., ln, rn).  If supplied with a tuple of length length(dims) instead, it applies symmetric padding. If dims is not given, it defaults to all dimensions.\n\nFor integer pad input, it is applied on both sides on every dimension in dims.\n\nSee also pad_zeros, pad_reflect and pad_repeat.\n\njulia> r = reshape(1:4, 2, 2)\n2Ã—2 reshape(::UnitRange{Int64}, 2, 2) with eltype Int64:\n 1  3\n 2  4\n\njulia> pad_constant(r, (1, 2, 3, 4), 8)\n5Ã—9 Matrix{Int64}:\n 8  8  8  8  8  8  8  8  8\n 8  8  8  1  3  8  8  8  8\n 8  8  8  2  4  8  8  8  8\n 8  8  8  8  8  8  8  8  8\n 8  8  8  8  8  8  8  8  8\n\njulia> pad_constant(r, 1, 8)\n4Ã—4 Matrix{Int64}:\n 8  8  8  8\n 8  1  3  8\n 8  2  4  8\n 8  8  8  8\n\njulia> r = reshape(1:27, 3, 3, 3)\n3Ã—3Ã—3 reshape(::UnitRange{Int64}, 3, 3, 3) with eltype Int64:\n[:, :, 1] =\n 1  4  7\n 2  5  8\n 3  6  9\n\n[:, :, 2] =\n 10  13  16\n 11  14  17\n 12  15  18\n\n[:, :, 3] =\n 19  22  25\n 20  23  26\n 21  24  27\n\njulia> pad_constant(r, (2,1), dims = 1) # assymetric padding\n6Ã—3Ã—3 Array{Int64, 3}:\n[:, :, 1] =\n 0  0  0\n 0  0  0\n 1  4  7\n 2  5  8\n 3  6  9\n 0  0  0\n\n[:, :, 2] =\n  0   0   0\n  0   0   0\n 10  13  16\n 11  14  17\n 12  15  18\n  0   0   0\n\n[:, :, 3] =\n  0   0   0\n  0   0   0\n 19  22  25\n 20  23  26\n 21  24  27\n  0   0   0\n\njulia> pad_constant(r, (2,1, 3), dims = (1,2)) # padding must always be either the same length as dims, or double it\nERROR: ArgumentError: Could not parse padding (2, 1, 3) and dims (1, 2)\nStacktrace:\n[...]\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#NNlib.pad_repeat","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.pad_repeat","text":"pad_repeat(x, pad::Tuple; [dims])\npad_repeat(x, pad::Int; [dims])\n\nPad the array x repeating the values on the border.\n\npad can a tuple of integers (l1, r1, ..., ln, rn) of some length 2n that specifies the left and right padding size for each of the dimensions in dims. If dims is not given,  it defaults to the first n dimensions.\n\nFor integer pad input instead, it is applied on both sides on every dimension in dims. In this case, dims  defaults to the first ndims(x)-2 dimensions  (i.e. excludes the channel and batch dimension). \n\nSee also pad_reflect and pad_constant.\n\njulia> r = reshape(1:9, 3, 3)\n3Ã—3 reshape(::UnitRange{Int64}, 3, 3) with eltype Int64:\n 1  4  7\n 2  5  8\n 3  6  9\n\njulia> pad_repeat(r, (1,2,3,4))\n6Ã—10 Matrix{Int64}:\n 1  1  1  1  4  7  7  7  7  7\n 1  1  1  1  4  7  7  7  7  7\n 2  2  2  2  5  8  8  8  8  8\n 3  3  3  3  6  9  9  9  9  9\n 3  3  3  3  6  9  9  9  9  9\n 3  3  3  3  6  9  9  9  9  9\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#NNlib.pad_zeros","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.pad_zeros","text":"pad_zeros(x, pad::Tuple; [dims])\npad_zeros(x, pad::Int; [dims])\n\nPad the array x with zeros. Equivalent to pad_constant with the constant equal to 0. \n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#Convolution","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"Convolution","text":"","category":"section"},{"location":"models/nnlib/","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.jl ðŸ“š (softmax, conv, ...)","text":"Flux's Conv and CrossCor layers use NNlib.DenseConvDims and NNlib.conv internally. ","category":"page"},{"location":"models/nnlib/","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.jl ðŸ“š (softmax, conv, ...)","text":"conv\nConvDims\ndepthwiseconv\nDepthwiseConvDims\nDenseConvDims","category":"page"},{"location":"models/nnlib/#NNlib.conv","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.conv","text":"conv(x, w; stride = 1, pad = 0, dilation = 1, flipped = false, groups = 1)\n\nApply convolution filter w to input x. x and w are 3d/4d/5d tensors in 1d/2d/3d convolutions respectively.\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#NNlib.ConvDims","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.ConvDims","text":"ConvDims\n\nType system-level information about convolution dimensions. Critical for things like im2col!() to generate efficient code, and helpful to reduce the number of kwargs getting passed around.\n\n\n\n\n\n","category":"type"},{"location":"models/nnlib/#NNlib.depthwiseconv","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.depthwiseconv","text":"depthwiseconv(x, w; stride=1, pad=0, dilation=1, flipped=false)\n\nDepthwise convolution operation with filter w on input x. x and w are 3d/4d/5d tensors in 1d/2d/3d convolutions respectively.\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#NNlib.DepthwiseConvDims","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.DepthwiseConvDims","text":"DepthwiseConvDims\n\nConcrete subclass of ConvDims for a depthwise convolution.  Differs primarily due to characterization by Cin, Cmult, rather than Cin, Cout.  Useful to be separate from DenseConvDims primarily for channel calculation differences.\n\n\n\n\n\n","category":"type"},{"location":"models/nnlib/#NNlib.DenseConvDims","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.DenseConvDims","text":"DenseConvDims\n\nConcrete subclass of ConvDims for a normal, dense, conv2d/conv3d.\n\n\n\n\n\n","category":"type"},{"location":"models/nnlib/#Upsampling","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"Upsampling","text":"","category":"section"},{"location":"models/nnlib/","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.jl ðŸ“š (softmax, conv, ...)","text":"Flux's Upsample layer uses NNlib.upsample_nearest, NNlib.upsample_bilinear, and NNlib.upsample_trilinear as its backend. Additionally, Flux's PixelShuffle layer uses NNlib.pixel_shuffle as its backend.","category":"page"},{"location":"models/nnlib/","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.jl ðŸ“š (softmax, conv, ...)","text":"upsample_nearest\nâˆ‡upsample_nearest\nupsample_linear\nâˆ‡upsample_linear\nupsample_bilinear\nâˆ‡upsample_bilinear\nupsample_trilinear\nâˆ‡upsample_trilinear\npixel_shuffle","category":"page"},{"location":"models/nnlib/#NNlib.upsample_nearest","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.upsample_nearest","text":"upsample_nearest(x, scale::NTuple{S,Int})\nupsample_nearest(x; size::NTuple{S,Int})\n\nUpsamples the array x by integer multiples along the first S dimensions. Subsequent dimensions of x are not altered.\n\nEither the scale factors or the final output size can be specified.\n\nSee also upsample_bilinear, for two dimensions of an N=4 array.\n\nExample\n\njulia> upsample_nearest([1 2 3; 4 5 6], (2, 3))\n4Ã—9 Matrix{Int64}:\n 1  1  1  2  2  2  3  3  3\n 1  1  1  2  2  2  3  3  3\n 4  4  4  5  5  5  6  6  6\n 4  4  4  5  5  5  6  6  6\n\njulia> ans == upsample_nearest([1 2 3; 4 5 6]; size=(4, 9))  # equivalent\ntrue\n\njulia> upsample_nearest([1 2 3; 4 5 6], (2,))\n4Ã—3 Matrix{Int64}:\n 1  2  3\n 1  2  3\n 4  5  6\n 4  5  6\n\njulia> ans == upsample_nearest([1 2 3; 4 5 6], size=(4,))\ntrue\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#NNlib.upsample_linear","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.upsample_linear","text":"upsample_linear(x::AbstractArray{T,3}, scale::Real)\nupsample_linear(x::AbstractArray{T,3}; size::Integer)\n\nUpsamples the first dimension of the array x by the upsample provided scale, using linear interpolation. As an alternative to using scale, the resulting array size can be directly specified with a keyword argument.\n\nThe size of the output is equal to (scale*S1, S2, S3), where S1, S2, S3 = size(x).\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#NNlib.âˆ‡upsample_linear","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.âˆ‡upsample_linear","text":"âˆ‡upsample_linear(Î”::AbstractArray{T,3}; size::Integer) where T\n\nArguments\n\nÎ”: Incoming gradient array, backpropagated from downstream layers\nsize: Size of the image upsampled in the first place\n\nOutputs\n\ndx: Downsampled version of Î”\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#NNlib.upsample_bilinear","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.upsample_bilinear","text":"upsample_bilinear(x::AbstractArray{T,4}, scale::NTuple{2,Real})\nupsample_bilinear(x::AbstractArray{T,4}; size::NTuple{2,Integer})\n\nUpsamples the first 2 dimensions of the array x by the upsample factors stored in scale, using bilinear interpolation. As an alternative to using scale, the resulting image size can be directly specified with a keyword argument.\n\nThe size of the output is equal to (scale[1]*S1, scale[2]*S2, S3, S4), where S1, S2, S3, S4 = size(x).\n\nExamples\n\njulia> x = reshape(Float32[1 2 3; 4 5 6], (2,3,1,1))\n2Ã—3Ã—1Ã—1 Array{Float32, 4}:\n[:, :, 1, 1] =\n 1.0  2.0  3.0\n 4.0  5.0  6.0\n\njulia> upsample_bilinear(x, (2, 3))\n4Ã—9Ã—1Ã—1 Array{Float32, 4}:\n[:, :, 1, 1] =\n 1.0  1.25  1.5  1.75  2.0  2.25  2.5  2.75  3.0\n 2.0  2.25  2.5  2.75  3.0  3.25  3.5  3.75  4.0\n 3.0  3.25  3.5  3.75  4.0  4.25  4.5  4.75  5.0\n 4.0  4.25  4.5  4.75  5.0  5.25  5.5  5.75  6.0\n\njulia> ans == upsample_bilinear(x; size=(4, 9))  # specify ouput size instead\ntrue\n\njulia> upsample_bilinear(x, (2.5, 3.5))  # non-integer scaling factors are allowed\n5Ã—10Ã—1Ã—1 Array{Float32, 4}:\n[:, :, 1, 1] =\n 1.0   1.22222  1.44444  1.66667  1.88889  â€¦  2.33333  2.55556  2.77778  3.0\n 1.75  1.97222  2.19444  2.41667  2.63889     3.08333  3.30556  3.52778  3.75\n 2.5   2.72222  2.94444  3.16667  3.38889     3.83333  4.05556  4.27778  4.5\n 3.25  3.47222  3.69444  3.91667  4.13889     4.58333  4.80556  5.02778  5.25\n 4.0   4.22222  4.44444  4.66667  4.88889     5.33333  5.55556  5.77778  6.0\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#NNlib.âˆ‡upsample_bilinear","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.âˆ‡upsample_bilinear","text":"âˆ‡upsample_bilinear(Î”::AbstractArray{T,4}; size::NTuple{2,Integer}) where T\n\nArguments\n\nÎ”: Incoming gradient array, backpropagated from downstream layers\nsize: Lateral (W,H) size of the image upsampled in the first place\n\nOutputs\n\ndx: Downsampled version of Î”\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#NNlib.upsample_trilinear","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.upsample_trilinear","text":"upsample_trilinear(x::AbstractArray{T,5}, scale::NTuple{3,Real})\nupsample_trilinear(x::AbstractArray{T,5}; size::NTuple{3,Integer})\n\nUpsamples the first 3 dimensions of the array x by the upsample factors stored in scale, using trilinear interpolation. As an alternative to using scale, the resulting image size can be directly specified with a keyword argument.\n\nThe size of the output is equal to (scale[1]*S1, scale[2]*S2, scale[3]*S3, S4, S5), where S1, S2, S3, S4, S5 = size(x).\n\nExamples\n\nupsample_trilinear(x, (2, 3, 4))\nupsample_trilinear(x; size=(4, 9, 11))  # specify ouput size instead\nupsample_trilinear(x, (2.5, 3.5, pi))  # non-integer scaling factors are allowed\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#NNlib.âˆ‡upsample_trilinear","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.âˆ‡upsample_trilinear","text":"âˆ‡upsample_trilinear(Î”::AbstractArray{T,5}; size::NTuple{3,Integer}) where T\n\nArguments\n\nÎ”: Incoming gradient array, backpropagated from downstream layers\nsize: Lateral size & depth (W,H,D) of the image upsampled in the first place\n\nOutputs\n\ndx: Downsampled version of Î”\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#NNlib.pixel_shuffle","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.pixel_shuffle","text":"pixel_shuffle(x, r::Integer)\n\nPixel shuffling operation, upscaling by a factor r.\n\nFor 4-arrays representing N images, the operation converts input size(x) == (W, H, r^2*C, N) to output of size (r*W, r*H, C, N). For D-dimensional data, it expects ndims(x) == D+2 with channel and batch dimensions, and divides the number of channels by r^D.\n\nUsed in super-resolution networks to upsample towards high resolution features. Reference: Shi et. al., \"Real-Time Single Image and Video Super-Resolution ...\", CVPR 2016, https://arxiv.org/abs/1609.05158\n\nExamples\n\njulia> x = [10i + j + channel/10 for i in 1:2, j in 1:3, channel in 1:4, batch in 1:1]\n2Ã—3Ã—4Ã—1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 11.1  12.1  13.1\n 21.1  22.1  23.1\n\n[:, :, 2, 1] =\n 11.2  12.2  13.2\n 21.2  22.2  23.2\n\n[:, :, 3, 1] =\n 11.3  12.3  13.3\n 21.3  22.3  23.3\n\n[:, :, 4, 1] =\n 11.4  12.4  13.4\n 21.4  22.4  23.4\n\njulia> pixel_shuffle(x, 2)  # 4 channels used up as 2x upscaling of image dimensions\n4Ã—6Ã—1Ã—1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 11.1  11.3  12.1  12.3  13.1  13.3\n 11.2  11.4  12.2  12.4  13.2  13.4\n 21.1  21.3  22.1  22.3  23.1  23.3\n 21.2  21.4  22.2  22.4  23.2  23.4\n\njulia> y = [i + channel/10 for i in 1:3, channel in 1:6, batch in 1:1]\n3Ã—6Ã—1 Array{Float64, 3}:\n[:, :, 1] =\n 1.1  1.2  1.3  1.4  1.5  1.6\n 2.1  2.2  2.3  2.4  2.5  2.6\n 3.1  3.2  3.3  3.4  3.5  3.6\n\njulia> pixel_shuffle(y, 2)  # 1D image, with 6 channels reduced to 3\n6Ã—3Ã—1 Array{Float64, 3}:\n[:, :, 1] =\n 1.1  1.3  1.5\n 1.2  1.4  1.6\n 2.1  2.3  2.5\n 2.2  2.4  2.6\n 3.1  3.3  3.5\n 3.2  3.4  3.6\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#Batched-Operations","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"Batched Operations","text":"","category":"section"},{"location":"models/nnlib/","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.jl ðŸ“š (softmax, conv, ...)","text":"Flux's Bilinear layer uses NNlib.batched_mul internally.","category":"page"},{"location":"models/nnlib/","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.jl ðŸ“š (softmax, conv, ...)","text":"batched_mul\nbatched_mul!\nbatched_adjoint\nbatched_transpose\nbatched_vec","category":"page"},{"location":"models/nnlib/#NNlib.batched_mul","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.batched_mul","text":"batched_mul(A, B) -> C\nA âŠ  B  # \\boxtimes\n\nBatched matrix multiplication. Result has C[:,:,k] == A[:,:,k] * B[:,:,k] for all k. If size(B,3) == 1 then instead C[:,:,k] == A[:,:,k] * B[:,:,1], and similarly for A.\n\nTo transpose each matrix, apply batched_transpose to the array, or batched_adjoint for conjugate-transpose:\n\njulia> A, B = randn(2,5,17), randn(5,9,17);\n\njulia> A âŠ  B |> size\n(2, 9, 17)\n\njulia> batched_adjoint(A) |> size\n(5, 2, 17)\n\njulia> batched_mul(A, batched_adjoint(randn(9,5,17))) |> size\n(2, 9, 17)\n\njulia> A âŠ  randn(5,9,1) |> size\n(2, 9, 17)\n\njulia> batched_transpose(A) == PermutedDimsArray(A, (2,1,3))\ntrue\n\nThe equivalent PermutedDimsArray may be used in place of batched_transpose. Other permutations are also handled by BLAS, provided that the batch index k is not the first dimension of the underlying array. Thus PermutedDimsArray(::Array, (1,3,2)) and PermutedDimsArray(::Array, (3,1,2)) are fine.\n\nHowever, A = PermutedDimsArray(::Array, (3,2,1)) is not acceptable to BLAS, since the batch dimension is the contiguous one: stride(A,3) == 1. This will be copied, as doing so is faster than batched_mul_generic!.\n\nBoth this copy and batched_mul_generic! produce @debug messages, and setting for instance ENV[\"JULIA_DEBUG\"] = NNlib will display them.\n\n\n\n\n\nbatched_mul(A::Array{T,3}, B::Matrix)\nbatched_mul(A::Matrix, B::Array{T,3})\nA âŠ  B\n\nThis is always matrix-matrix multiplication, but either A or B may lack a batch index.\n\nWhen B is a matrix, result has C[:,:,k] == A[:,:,k] * B[:,:] for all k.\nWhen A is a matrix, then C[:,:,k] == A[:,:] * B[:,:,k]. This can also be done by reshaping and calling *, for instance A âŠ¡ B using TensorCore.jl, but is implemented here using batched_gemm instead of gemm.\n\njulia> randn(16,8,32) âŠ  randn(8,4) |> size\n(16, 4, 32)\n\njulia> randn(16,8,32) âŠ  randn(8,4,1) |> size  # equivalent\n(16, 4, 32)\n\njulia> randn(16,8) âŠ  randn(8,4,32) |> size\n(16, 4, 32)\n\nSee also batched_vec to regard B as a batch of vectors, A[:,:,k] * B[:,k].\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#NNlib.batched_mul!","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.batched_mul!","text":"batched_mul!(C, A, B) -> C\nbatched_mul!(C, A, B, Î±=1, Î²=0)\n\nIn-place batched matrix multiplication, equivalent to mul!(C[:,:,k], A[:,:,k], B[:,:,k], Î±, Î²) for all k. If size(B,3) == 1 then every batch uses B[:,:,1] instead.\n\nThis will call batched_gemm! whenever possible. For real arrays this means that, for X âˆˆ [A,B,C], either strides(X,1)==1 or strides(X,2)==1, the latter may be caused by batched_transpose or by for instance PermutedDimsArray(::Array, (3,1,2)). Unlike batched_mul this will never make a copy.\n\nFor complex arrays, the wrapper made by batched_adjoint must be outermost to be seen. In this case the strided accepted by BLAS are more restricted, if stride(C,1)==1 then only stride(AorB::BatchedAdjoint,2) == 1 is accepted.\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#NNlib.batched_adjoint","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.batched_adjoint","text":"batched_transpose(A::AbstractArray{T,3})\nbatched_adjoint(A)\n\nEquivalent to applying transpose or adjoint to each matrix A[:,:,k].\n\nThese exist to control how batched_mul behaves, as it operates on such matrix slices of an array with ndims(A)==3.\n\nPermutedDimsArray(A, (2,1,3)) is equivalent to batched_transpose(A), and is also understood by batched_mul (and more widely supported elsewhere).\n\nBatchedTranspose{T, S} <: AbstractBatchedMatrix{T, 3}\nBatchedAdjoint{T, S}\n\nLazy wrappers analogous to Transpose and Adjoint, returned by batched_transpose etc.\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#NNlib.batched_transpose","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.batched_transpose","text":"batched_transpose(A::AbstractArray{T,3})\nbatched_adjoint(A)\n\nEquivalent to applying transpose or adjoint to each matrix A[:,:,k].\n\nThese exist to control how batched_mul behaves, as it operates on such matrix slices of an array with ndims(A)==3.\n\nPermutedDimsArray(A, (2,1,3)) is equivalent to batched_transpose(A), and is also understood by batched_mul (and more widely supported elsewhere).\n\nBatchedTranspose{T, S} <: AbstractBatchedMatrix{T, 3}\nBatchedAdjoint{T, S}\n\nLazy wrappers analogous to Transpose and Adjoint, returned by batched_transpose etc.\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#NNlib.batched_vec","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.batched_vec","text":"batched_vec(A::Array{T,3}, B::Matrix)\nbatched_vec(A::Array{T,3}, b::Vector)\n\nBatched matrix-vector multiplication: the result has C[:,:,k] == A[:,:,k] * B[:,k] for all k, or else C[:,:,k] == A[:,:,k] * b for b::Vector.\n\nWith the same argument types, batched_mul(A, B) would regard B as a fixed matrix, not a batch of vectors. Both reshape and then call batched_mul(::Array{T,3}, ::Array{T,3}).\n\njulia> A, B, b = randn(16,8,32), randn(8,32), randn(8);\n\njulia> batched_vec(A,B) |> size\n(16, 32)\n\njulia> batched_vec(A,b) |> size\n(16, 32)\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#Gather-and-Scatter","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"Gather and Scatter","text":"","category":"section"},{"location":"models/nnlib/","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.jl ðŸ“š (softmax, conv, ...)","text":"Flux's Embedding layer uses NNlib.gather as its backend.","category":"page"},{"location":"models/nnlib/","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.jl ðŸ“š (softmax, conv, ...)","text":"NNlib.gather\nNNlib.gather!\nNNlib.scatter\nNNlib.scatter!","category":"page"},{"location":"models/nnlib/#NNlib.gather","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.gather","text":"NNlib.gather(src, idx) -> dst\n\nReverse operation of scatter. Gathers data from source src  and writes it in a destination dst according to the index array idx. For each k in CartesianIndices(idx), assign values to dst  according to\n\ndst[:, ... , k] .= src[:, ... , idx[k]...]\n\nNotice that if idx is a vector containing integers and src is a matrix, previous expression simplifies to\n\ndst[:, k] .= src[:, idx[k]]\n\nand k will run over 1:length(idx). \n\nThe elements of idx can be integers or integer tuples and may be repeated.  A single src column can end up being copied into zero, one,  or multiple dst columns.\n\nSee gather! for an in-place version.\n\nExamples\n\njulia> NNlib.gather([1,20,300,4000], [2,4,2])\n3-element Vector{Int64}:\n   20\n 4000\n   20\n\njulia> NNlib.gather([1 2 3; 4 5 6], [1,3,1,3,1])\n2Ã—5 Matrix{Int64}:\n 1  3  1  3  1\n 4  6  4  6  4\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#NNlib.gather!","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.gather!","text":"NNlib.gather!(dst, src, idx)\n\nReverse operation of scatter!. Gathers data from source src  and writes it in destination dst according to the index array idx. For each k in CartesianIndices(idx), assign values to dst according to\n\ndst[:, ... , k] .= src[:, ... , idx[k]...]\n\nNotice that if idx is a vector containing integers, and both dst and src are matrices, previous expression simplifies to\n\ndst[:, k] .= src[:, idx[k]]\n\nand k will run over 1:length(idx). \n\nThe elements of idx can be integers or integer tuples and may be repeated.  A single src column can end up being copied into zero, one,  or multiple dst columns.\n\nSee gather for an allocating version.\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#NNlib.scatter","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.scatter","text":"NNlib.scatter(op, src, idx; [init, dstsize])\n\nScatter operation allocating a destination array dst and  calling scatter!(op, dst, src, idx) on it.\n\nIf keyword init is provided, it is used to initialize the content of dst. Otherwise, the init values is inferred from the reduction operator op for some common operators (e.g. init = 0 for op = +). \nIf dstsize is provided, it will be used to define the size of destination array, otherwise it will be inferred by src and idx.\n\nSee scatter! for full details on how idx works.\n\nExamples\n\njulia> NNlib.scatter(+, [10,100,1000], [3,1,2])\n3-element Vector{Int64}:\n  100\n 1000\n   10\n\njulia> NNlib.scatter(+, [1 2 3 4; 5 6 7 8], [2,1,1,5])\n2Ã—5 Matrix{Int64}:\n  5  1  0  0  4\n 13  5  0  0  8\n\njulia> NNlib.scatter(*, [10,200,3000], [1,4,2]; init = 10, dstsize = 6)\n6-element Vector{Int64}:\n   100\n 30000\n    10\n  2000\n    10\n    10\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#NNlib.scatter!","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.scatter!","text":"NNlib.scatter!(op, dst, src, idx)\n\nScatter operation, which writes data in src into dst at locations idx. A binary reduction operator op is applied during the scatter.  For each index k in idx, accumulates values in dst according to\n\ndst[:, ..., idx[k]...] = (op).(dst[:, ..., idx[k]...], src[:, ..., k...])\n\nSee also scatter, gather.\n\nArguments\n\nop: Operations to be applied on dst and src, e.g. +, -, *, /, max, min and mean.\ndst: The destination for src to aggregate to. This argument will be mutated.\nsrc: The source data for aggregating.\nidx: The mapping for aggregation from source (index) to destination (value).         The idx array can contain either integers or tuples.\n\nExamples\n\njulia> NNlib.scatter!(+, ones(3), [10,100], [1,3])\n3-element Vector{Float64}:\n  11.0\n   1.0\n 101.0\n\njulia> NNlib.scatter!(*, fill(0.5, 2, 4), [1 10; 100 1000], [3,2])\n2Ã—4 Matrix{Float64}:\n 0.5    5.0   0.5  0.5\n 0.5  500.0  50.0  0.5\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#Sampling","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"Sampling","text":"","category":"section"},{"location":"models/nnlib/","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.jl ðŸ“š (softmax, conv, ...)","text":"grid_sample\nâˆ‡grid_sample","category":"page"},{"location":"models/nnlib/#NNlib.grid_sample","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.grid_sample","text":"grid_sample(input::AbstractArray{T, 4}, grid::AbstractArray{T, 4}; padding_mode = :zeros)\n\nGiven input, compute output by sampling input values at pixel locations from grid. Uses bilinear interpolation to calculate output values.\n\nThis implementation assumes the extrema (-1 and 1) are considered as referring to the center points of the inputâ€™s corner pixels (i.e. align corners is true).\n\nArguments\n\ninput: Input array in (W_in, H_in, C, N) shape.\ngrid: Input grid in (2, W_out, H_out, N) shape.   Where for each (W_out, H_out, N) grid contains (x, y)   coordinates that specify sampling locations normalized by the input shape.\nTherefore, x and y should have values in [-1, 1] range.   For example, (x = -1, y = -1) is the left-top pixel of input,   and (x = 1, y = 1) is the right-bottom pixel of input.\nOut-of-bound values are handled according to the padding_mode.\npadding_mode: Out-of-bound padding.   :zeros to use 0 for out-of-bound grid locations.   :border to use border values for out-of-bound grid locations.   Default is :zeros.\n\nReturns\n\n(W_out, H_out, C, N) sampled grid from input.\n\nExamples\n\nIn the example below, grid contains two out-of-bound sampling locations, which are handled differently, depending on the padding_mode.\n\njulia> x = reshape(collect(1.0:4.0), (2, 2, 1, 1))\n2Ã—2Ã—1Ã—1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 1.0  3.0\n 2.0  4.0\n\njulia> grid = Array{Float64}(undef, 2, 3, 2, 1);\n\njulia> grid[:, 1, 1, 1] .= (-3, -1);\n\njulia> grid[:, 2, 1, 1] .= (0, -1);\n\njulia> grid[:, 3, 1, 1] .= (1, -1);\n\njulia> grid[:, 1, 2, 1] .= (-1, 1);\n\njulia> grid[:, 2, 2, 1] .= (0, 1);\n\njulia> grid[:, 3, 2, 1] .= (3, 1);\n\njulia> grid_sample(x, grid; padding_mode=:zeros)\n3Ã—2Ã—1Ã—1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 0.0  3.0\n 1.5  3.5\n 2.0  0.0\n\njulia> grid_sample(x, grid; padding_mode=:border)\n3Ã—2Ã—1Ã—1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 1.0  3.0\n 1.5  3.5\n 2.0  4.0\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#NNlib.âˆ‡grid_sample","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.âˆ‡grid_sample","text":"âˆ‡grid_sample(Î”::AbstractArray{T, 4}, input::AbstractArray{T, 4}, grid::AbstractArray{T, 4}; padding_mode = :zeros) where T\n\nArguments\n\nÎ”: Input gradient in (W_out, H_out, C, N) shape   (same as output of the primal computation).\ninput: Input from primal computation in (W_in, H_in, C, N) shape.\ngrid: Grid from primal computation in (2, W_out, H_out, N) shape.\npadding_mode: Out-of-bound padding.   :zeros to use 0 for out-of-bound grid locations.   :border to use border values for out-of-bound grid locations.   Should be the same as in primal computation.   Default is :zeros.\n\nReturns\n\ndinput (same shape as input) and dgrid (same shape as grid) gradients.\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#Losses","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"Losses","text":"","category":"section"},{"location":"models/nnlib/","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.jl ðŸ“š (softmax, conv, ...)","text":"ctc_loss","category":"page"},{"location":"models/nnlib/#NNlib.ctc_loss","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.ctc_loss","text":"ctc_loss(yÌ‚, y)\n\nComputes the connectionist temporal classification loss between yÌ‚ and y. yÌ‚ must be a classes-by-time matrices, i.e., each row represents a class and each column represents a time step. Additionally, the logsoftmax function will be applied to yÌ‚, so yÌ‚ must be the raw activation values from the neural network and not, for example, the activations after being passed through a softmax activation function. y must be a 1D array of the labels associated with yÌ‚. The blank label is assumed to be the last label category in yÌ‚, so it is equivalent to size(yÌ‚, 1). Used for sequence-to-sequence classification problems such as speech recognition and handwriting recognition where the exact time-alignment of the output (e.g., letters) is not needed to solve the problem. See Graves et al. (2006) or Graves (2012) for mathematical details.\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#Miscellaneous","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"Miscellaneous","text":"","category":"section"},{"location":"models/nnlib/","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.jl ðŸ“š (softmax, conv, ...)","text":"logsumexp\nNNlib.glu","category":"page"},{"location":"models/nnlib/#NNlib.logsumexp","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.logsumexp","text":"logsumexp(x; dims = :)\n\nComputes log.(sum(exp.(x); dims)) in a numerically stable way. Without dims keyword this returns a scalar.\n\nSee also logsoftmax.\n\n\n\n\n\n","category":"function"},{"location":"models/nnlib/#NNlib.glu","page":"NNlib.jl ðŸ“š (softmax, conv, ...)","title":"NNlib.glu","text":"glu(x, dim = 1)\n\nThe gated linear unit from the \"Language Modeling with Gated Convolutional Networks\" paper.\n\nCalculates a .* sigmoid(b), where x is split in half along given dimension dim to form a and b.\n\n\n\n\n\n","category":"function"},{"location":"saving/#Saving-and-Loading-Models","page":"Saving & Loading","title":"Saving and Loading Models","text":"","category":"section"},{"location":"saving/","page":"Saving & Loading","title":"Saving & Loading","text":"You may wish to save models so that they can be loaded and run in a later session. The easiest way to do this is via BSON.jl.","category":"page"},{"location":"saving/","page":"Saving & Loading","title":"Saving & Loading","text":"Save a model:","category":"page"},{"location":"saving/","page":"Saving & Loading","title":"Saving & Loading","text":"julia> using Flux\n\njulia> model = Chain(Dense(10, 5, NNlib.relu), Dense(5, 2), NNlib.softmax)\nChain(\n  Dense(10 => 5, relu),                 # 55 parameters\n  Dense(5 => 2),                        # 12 parameters\n  NNlib.softmax,\n)                   # Total: 4 arrays, 67 parameters, 524 bytes.\n\njulia> using BSON: @save\n\njulia> @save \"mymodel.bson\" model","category":"page"},{"location":"saving/","page":"Saving & Loading","title":"Saving & Loading","text":"Load it again:","category":"page"},{"location":"saving/","page":"Saving & Loading","title":"Saving & Loading","text":"julia> using Flux # Flux must be loaded before calling @load\n\njulia> using BSON: @load\n\njulia> @load \"mymodel.bson\" model\n\njulia> model\nChain(\n  Dense(10 => 5, relu),                 # 55 parameters\n  Dense(5 => 2),                        # 12 parameters\n  NNlib.softmax,\n)                   # Total: 4 arrays, 67 parameters, 524 bytes.","category":"page"},{"location":"saving/","page":"Saving & Loading","title":"Saving & Loading","text":"Models are just normal Julia structs, so it's fine to use any Julia storage format for this purpose. BSON.jl is particularly well supported and most likely to be forwards compatible (that is, models saved now will load in future versions of Flux).","category":"page"},{"location":"saving/","page":"Saving & Loading","title":"Saving & Loading","text":"note: Note\nIf a saved model's parameters are stored on the GPU, the model will not load later on if there is no GPU support available. It's best to move your model to the CPU with cpu(model) before saving it.","category":"page"},{"location":"saving/","page":"Saving & Loading","title":"Saving & Loading","text":"warning: Warning\nPrevious versions of Flux suggested saving only the model weights using @save \"mymodel.bson\" params(model). This is no longer recommended and even strongly discouraged. Saving models this way will only store the trainable parameters which will result in incorrect behavior for layers like BatchNorm.","category":"page"},{"location":"saving/","page":"Saving & Loading","title":"Saving & Loading","text":"julia> using Flux\n\njulia> model = Chain(Dense(10 => 5,relu),Dense(5 => 2),softmax)\nChain(\n  Dense(10 => 5, relu),                 # 55 parameters\n  Dense(5 => 2),                        # 12 parameters\n  NNlib.softmax,\n)                   # Total: 4 arrays, 67 parameters, 524 bytes.\n\njulia> weights = Flux.params(model);","category":"page"},{"location":"saving/","page":"Saving & Loading","title":"Saving & Loading","text":"Loading the model as shown above will return a new model with the stored parameters. But sometimes you already have a model, and you want to load stored parameters into it. This can be done as","category":"page"},{"location":"saving/","page":"Saving & Loading","title":"Saving & Loading","text":"using Flux: loadmodel!\nusing BSON: @load\n\n# some predefined model\nmodel = Chain(Dense(10 => 5, relu), Dense(5 => 2), softmax)\n\n# load one model into another\nmodel = loadmodel!(model, @load(\"mymodel.bson\"))","category":"page"},{"location":"saving/","page":"Saving & Loading","title":"Saving & Loading","text":"This ensures that the model loaded from \"mymodel.bson\" matches the structure of model. Flux.loadmodel! is also convenient for copying parameters between models in memory.","category":"page"},{"location":"saving/","page":"Saving & Loading","title":"Saving & Loading","text":"Flux.loadmodel!","category":"page"},{"location":"saving/#Flux.loadmodel!","page":"Saving & Loading","title":"Flux.loadmodel!","text":"loadmodel!(dst, src)\n\nCopy all the parameters (trainable and non-trainable) from src into dst.\n\nRecursively walks dst and src together using Functors.children, and calling copyto! on parameter arrays or throwing an error when there is a mismatch. Non-array elements (such as activation functions) are not copied and need not match. Zero bias vectors and bias=false are considered equivalent (see extended help for more details).\n\nExamples\n\njulia> dst = Chain(Dense(Flux.ones32(2, 5), Flux.ones32(2), tanh), Dense(2 => 1; bias = [1f0]))\nChain(\n  Dense(5 => 2, tanh),                  # 12 parameters\n  Dense(2 => 1),                        # 3 parameters\n)                   # Total: 4 arrays, 15 parameters, 316 bytes.\n\njulia> dst[1].weight â‰ˆ ones(2, 5)  # by construction\ntrue\n\njulia> src = Chain(Dense(5 => 2, relu), Dense(2 => 1, bias=false));\n\njulia> Flux.loadmodel!(dst, src);\n\njulia> dst[1].weight â‰ˆ ones(2, 5)  # values changed\nfalse\n\njulia> iszero(dst[2].bias)\ntrue\n\nExtended help\n\nThrows an error when:\n\ndst and src do not share the same fields (at any level)\nthe sizes of leaf nodes are mismatched between dst and src\ncopying non-array values to/from an array parameter (except inactive parameters described below)\ndst is a \"tied\" parameter (i.e. refers to another parameter) and loaded into multiple times with mismatched source values\n\nInactive parameters can be encoded by using the boolean value false instead of an array. If dst == false and src is an all-zero array, no error will be raised (and no values copied); however, attempting to copy a non-zero array to an inactive parameter will throw an error. Likewise, copying a src value of false to any dst array is valid, but copying a src value of true will error.\n\n\n\n\n\n","category":"function"},{"location":"saving/#Checkpointing","page":"Saving & Loading","title":"Checkpointing","text":"","category":"section"},{"location":"saving/","page":"Saving & Loading","title":"Saving & Loading","text":"In longer training runs it's a good idea to periodically save your model, so that you can resume if training is interrupted (for example, if there's a power cut). You can do this by saving the model in the callback provided to train!.","category":"page"},{"location":"saving/","page":"Saving & Loading","title":"Saving & Loading","text":"julia> using Flux: throttle\n\njulia> using BSON: @save\n\njulia> m = Chain(Dense(10 => 5, relu), Dense(5 => 2), softmax)\nChain(\n  Dense(10 => 5, relu),                 # 55 parameters\n  Dense(5 => 2),                        # 12 parameters\n  NNlib.softmax,\n)                   # Total: 4 arrays, 67 parameters, 524 bytes.\n\njulia> evalcb = throttle(30) do\n         # Show loss\n         @save \"model-checkpoint.bson\" model\n       end;","category":"page"},{"location":"saving/","page":"Saving & Loading","title":"Saving & Loading","text":"This will update the \"model-checkpoint.bson\" file every thirty seconds.","category":"page"},{"location":"saving/","page":"Saving & Loading","title":"Saving & Loading","text":"You can get more advanced by saving a series of models throughout training, for example","category":"page"},{"location":"saving/","page":"Saving & Loading","title":"Saving & Loading","text":"@save \"model-$(now()).bson\" model","category":"page"},{"location":"saving/","page":"Saving & Loading","title":"Saving & Loading","text":"will produce a series of models like \"model-2018-03-06T02:57:10.41.bson\". You could also store the current test set loss, so that it's easy to (for example) revert to an older copy of the model if it starts to overfit.","category":"page"},{"location":"saving/","page":"Saving & Loading","title":"Saving & Loading","text":"@save \"model-$(now()).bson\" model loss = testloss()","category":"page"},{"location":"saving/","page":"Saving & Loading","title":"Saving & Loading","text":"Note that to resume a model's training, you might need to restore other stateful parts of your training loop. Possible examples are stateful optimizers (which usually utilize an IdDict to store their state), and the randomness used to partition the original data into the training and validation sets.","category":"page"},{"location":"saving/","page":"Saving & Loading","title":"Saving & Loading","text":"You can store the optimiser state alongside the model, to resume training exactly where you left off. BSON is smart enough to cache values and insert links when saving, but only if it knows everything to be saved up front. Thus models and optimizers must be saved together to have the latter work after restoring.","category":"page"},{"location":"saving/","page":"Saving & Loading","title":"Saving & Loading","text":"opt = Adam()\n@save \"model-$(now()).bson\" model opt","category":"page"},{"location":"models/layers/#Basic-Layers","page":"Built-in Layers ðŸ“š","title":"Basic Layers","text":"","category":"section"},{"location":"models/layers/","page":"Built-in Layers ðŸ“š","title":"Built-in Layers ðŸ“š","text":"These core layers form the foundation of almost all neural networks.","category":"page"},{"location":"models/layers/","page":"Built-in Layers ðŸ“š","title":"Built-in Layers ðŸ“š","text":"Chain\nDense","category":"page"},{"location":"models/layers/#Flux.Chain","page":"Built-in Layers ðŸ“š","title":"Flux.Chain","text":"Chain(layers...)\nChain(name = layer, ...)\n\nCollects multiple layers / functions to be called in sequence on a given input. Supports indexing and slicing, m[2] or m[1:end-1], and if names are given, m[:name] == m[1] etc.\n\nExamples\n\njulia> m = Chain(x -> x^2, x -> x+1);\n\njulia> m(5) == 26\ntrue\n\njulia> m = Chain(Dense(10 => 5, tanh), Dense(5 => 2));\n\njulia> x = rand(10, 32);\n\njulia> m(x) == m[2](m[1](x))\ntrue\n\njulia> m2 = Chain(enc = Chain(Flux.flatten, Dense(10 => 5, tanh)), \n                  dec = Dense(5 => 2));\n\njulia> m2(x) == (m2[:dec] âˆ˜ m2[:enc])(x)\ntrue\n\nFor large models, there is a special type-unstable path which can reduce compilation times. This can be used by supplying a vector of layers Chain([layer1, layer2, ...]). This feature is somewhat experimental, beware!\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Flux.Dense","page":"Built-in Layers ðŸ“š","title":"Flux.Dense","text":"Dense(in => out, Ïƒ=identity; bias=true, init=glorot_uniform)\nDense(W::AbstractMatrix, [bias, Ïƒ])\n\nCreate a traditional fully connected layer, whose forward pass is given by:\n\ny = Ïƒ.(W * x .+ bias)\n\nThe input x should be a vector of length in, or batch of vectors represented as an in Ã— N matrix, or any array with size(x,1) == in. The out y will be a vector  of length out, or a batch with size(y) == (out, size(x)[2:end]...)\n\nKeyword bias=false will switch off trainable bias for the layer. The initialisation of the weight matrix is W = init(out, in), calling the function given to keyword init, with default glorot_uniform. The weight matrix and/or the bias vector (of length out) may also be provided explicitly.\n\nExamples\n\njulia> d = Dense(5 => 2)\nDense(5 => 2)       # 12 parameters\n\njulia> d(rand(Float32, 5, 64)) |> size\n(2, 64)\n\njulia> d(rand(Float32, 5, 1, 1, 64)) |> size  # treated as three batch dimensions\n(2, 1, 1, 64)\n\njulia> d1 = Dense(ones(2, 5), false, tanh)  # using provided weight matrix\nDense(5 => 2, tanh; bias=false)  # 10 parameters\n\njulia> d1(ones(5))\n2-element Vector{Float64}:\n 0.9999092042625951\n 0.9999092042625951\n\njulia> Flux.params(d1)  # no trainable bias\nParams([[1.0 1.0 â€¦ 1.0 1.0; 1.0 1.0 â€¦ 1.0 1.0]])\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Convolution-and-Pooling-Layers","page":"Built-in Layers ðŸ“š","title":"Convolution and Pooling Layers","text":"","category":"section"},{"location":"models/layers/","page":"Built-in Layers ðŸ“š","title":"Built-in Layers ðŸ“š","text":"These layers are used to build convolutional neural networks (CNNs).","category":"page"},{"location":"models/layers/","page":"Built-in Layers ðŸ“š","title":"Built-in Layers ðŸ“š","text":"Conv\nConv(weight::AbstractArray)\nAdaptiveMaxPool\nMaxPool\nGlobalMaxPool\nAdaptiveMeanPool\nMeanPool\nGlobalMeanPool\nDepthwiseConv\nConvTranspose\nConvTranspose(weight::AbstractArray)\nCrossCor\nCrossCor(weight::AbstractArray)\nSamePad\nFlux.flatten","category":"page"},{"location":"models/layers/#Flux.Conv","page":"Built-in Layers ðŸ“š","title":"Flux.Conv","text":"Conv(filter, in => out, Ïƒ = identity;\n     stride = 1, pad = 0, dilation = 1, groups = 1, [bias, init])\n\nStandard convolutional layer. filter is a tuple of integers specifying the size of the convolutional kernel; in and out specify the number of input and output channels.\n\nImage data should be stored in WHCN order (width, height, channels, batch). In other words, a 100Ã—100 RGB image would be a 100Ã—100Ã—3Ã—1 array, and a batch of 50 would be a 100Ã—100Ã—3Ã—50 array. This has N = 2 spatial dimensions, and needs a kernel size like (5,5), a 2-tuple of integers.\n\nTo take convolutions along N feature dimensions, this layer expects as input an array with ndims(x) == N+2, where size(x, N+1) == in is the number of input channels, and size(x, ndims(x)) is (as always) the number of observations in a batch. Then:\n\nfilter should be a tuple of N integers.\nKeywords stride and dilation should each be either single integer, or a tuple with N integers.\nKeyword pad specifies the number of elements added to the borders of the data array. It can be\na single integer for equal padding all around,\na tuple of N integers, to apply the same padding at begin/end of each spatial dimension,\na tuple of 2*N integers, for asymmetric padding, or\nthe singleton SamePad(), to calculate padding such that size(output,d) == size(x,d) / stride (possibly rounded) for each spatial dimension.\nKeyword groups is expected to be an Int. It specifies the number of groups to divide a convolution into.\n\nKeywords to control initialization of the layer:\n\ninit - Function used to generate initial weights. Defaults to glorot_uniform.\nbias - The initial bias vector is all zero by default. Trainable bias can be disabled entirely by setting this to false, or another vector can be provided such as bias = randn(Float32, out).\n\nSee also ConvTranspose, DepthwiseConv, CrossCor.\n\nExamples\n\njulia> xs = rand(Float32, 100, 100, 3, 50); # a batch of images\n\njulia> layer = Conv((5,5), 3 => 7, relu; bias = false)\nConv((5, 5), 3 => 7, relu, bias=false)  # 525 parameters\n\njulia> layer(xs) |> size\n(96, 96, 7, 50)\n\njulia> Conv((5,5), 3 => 7; stride = 2)(xs) |> size\n(48, 48, 7, 50)\n\njulia> Conv((5,5), 3 => 7; stride = 2, pad = SamePad())(xs) |> size\n(50, 50, 7, 50)\n\njulia> Conv((1,1), 3 => 7; pad = (20,10,0,0))(xs) |> size\n(130, 100, 7, 50)\n\njulia> Conv((5,5), 3 => 7; stride = 2, dilation = 4)(xs) |> size\n(42, 42, 7, 50)\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Flux.Conv-Tuple{AbstractArray}","page":"Built-in Layers ðŸ“š","title":"Flux.Conv","text":"Conv(weight::AbstractArray, [bias, activation; stride, pad, dilation])\n\nConstructs a convolutional layer with the given weight and bias. Accepts the same keywords and has the same defaults as Conv(k::NTuple{N,Integer}, ch::Pair{<:Integer,<:Integer}, Ïƒ; ...).\n\njulia> weight = rand(3, 4, 5);\n\njulia> bias = zeros(5);\n\njulia> layer = Conv(weight, bias, sigmoid)  # expects 1 spatial dimension\nConv((3,), 4 => 5, Ïƒ)  # 65 parameters\n\njulia> layer(randn(100, 4, 64)) |> size\n(98, 5, 64)\n\njulia> Flux.params(layer) |> length\n2\n\n\n\n\n\n","category":"method"},{"location":"models/layers/#Flux.AdaptiveMaxPool","page":"Built-in Layers ðŸ“š","title":"Flux.AdaptiveMaxPool","text":"AdaptiveMaxPool(out::NTuple)\n\nAdaptive max pooling layer. Calculates the necessary window size such that its output has size(y)[1:N] == out.\n\nExpects as input an array with ndims(x) == N+2, i.e. channel and batch dimensions, after the N feature dimensions, where N = length(out).\n\nSee also MaxPool, AdaptiveMeanPool.\n\nExamples\n\njulia> xs = rand(Float32, 100, 100, 3, 50);  # batch of 50 RGB images\n\njulia> AdaptiveMaxPool((25, 25))(xs) |> size\n(25, 25, 3, 50)\n\njulia> MaxPool((4,4))(xs) â‰ˆ AdaptiveMaxPool((25, 25))(xs)\ntrue\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Flux.MaxPool","page":"Built-in Layers ðŸ“š","title":"Flux.MaxPool","text":"MaxPool(window::NTuple; pad=0, stride=window)\n\nMax pooling layer, which replaces all pixels in a block of size window with one.\n\nExpects as input an array with ndims(x) == N+2, i.e. channel and batch dimensions, after the N feature dimensions, where N = length(window).\n\nBy default the window size is also the stride in each dimension. The keyword pad accepts the same options as for the Conv layer, including SamePad().\n\nSee also Conv, MeanPool, AdaptiveMaxPool, GlobalMaxPool.\n\nExamples\n\njulia> xs = rand(Float32, 100, 100, 3, 50);  # batch of 50 RGB images\n\njulia> m = Chain(Conv((5, 5), 3 => 7, pad=SamePad()), MaxPool((5, 5), pad=SamePad()))\nChain(\n  Conv((5, 5), 3 => 7, pad=2),          # 532 parameters\n  MaxPool((5, 5), pad=2),\n)\n\njulia> m[1](xs) |> size\n(100, 100, 7, 50)\n\njulia> m(xs) |> size\n(20, 20, 7, 50)\n\njulia> layer = MaxPool((5,), pad=2, stride=(3,))  # one-dimensional window\nMaxPool((5,), pad=2, stride=3)\n\njulia> layer(rand(Float32, 100, 7, 50)) |> size\n(34, 7, 50)\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Flux.GlobalMaxPool","page":"Built-in Layers ðŸ“š","title":"Flux.GlobalMaxPool","text":"GlobalMaxPool()\n\nGlobal max pooling layer.\n\nTransforms (w,h,c,b)-shaped input into (1,1,c,b)-shaped output, by performing max pooling on the complete (w,h)-shaped feature maps.\n\nSee also MaxPool, GlobalMeanPool.\n\njulia> xs = rand(Float32, 100, 100, 3, 50);\n\njulia> m = Chain(Conv((3,3), 3 => 7), GlobalMaxPool());\n\njulia> m(xs) |> size\n(1, 1, 7, 50)\n\njulia> GlobalMaxPool()(rand(3,5,7)) |> size  # preserves 2 dimensions\n(1, 5, 7)\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Flux.AdaptiveMeanPool","page":"Built-in Layers ðŸ“š","title":"Flux.AdaptiveMeanPool","text":"AdaptiveMeanPool(out::NTuple)\n\nAdaptive mean pooling layer. Calculates the necessary window size such that its output has size(y)[1:N] == out.\n\nExpects as input an array with ndims(x) == N+2, i.e. channel and batch dimensions, after the N feature dimensions, where N = length(out).\n\nSee also MaxPool, AdaptiveMaxPool.\n\nExamples\n\njulia> xs = rand(Float32, 100, 100, 3, 50);  # batch of 50 RGB images\n\njulia> AdaptiveMeanPool((25, 25))(xs) |> size\n(25, 25, 3, 50)\n\njulia> MeanPool((4,4))(xs) â‰ˆ AdaptiveMeanPool((25, 25))(xs)\ntrue\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Flux.MeanPool","page":"Built-in Layers ðŸ“š","title":"Flux.MeanPool","text":"MeanPool(window::NTuple; pad=0, stride=window)\n\nMean pooling layer, averaging all pixels in a block of size window.\n\nExpects as input an array with ndims(x) == N+2, i.e. channel and batch dimensions, after the N feature dimensions, where N = length(window).\n\nBy default the window size is also the stride in each dimension. The keyword pad accepts the same options as for the Conv layer, including SamePad().\n\nSee also Conv, MaxPool, AdaptiveMeanPool.\n\nExamples\n\njulia> xs = rand(Float32, 100, 100, 3, 50);\n\njulia> m = Chain(Conv((5,5), 3 => 7), MeanPool((5,5), pad=SamePad()))\nChain(\n  Conv((5, 5), 3 => 7),                 # 532 parameters\n  MeanPool((5, 5), pad=2),\n)\n\njulia> m[1](xs) |> size\n(96, 96, 7, 50)\n\njulia> m(xs) |> size\n(20, 20, 7, 50)\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Flux.GlobalMeanPool","page":"Built-in Layers ðŸ“š","title":"Flux.GlobalMeanPool","text":"GlobalMeanPool()\n\nGlobal mean pooling layer.\n\nTransforms (w,h,c,b)-shaped input into (1,1,c,b)-shaped output, by performing mean pooling on the complete (w,h)-shaped feature maps.\n\njulia> xs = rand(Float32, 100, 100, 3, 50);\n\njulia> m = Chain(Conv((3,3), 3 => 7), GlobalMeanPool());\n\njulia> m(xs) |> size\n(1, 1, 7, 50)\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Flux.DepthwiseConv","page":"Built-in Layers ðŸ“š","title":"Flux.DepthwiseConv","text":"DepthwiseConv(filter, in => out, Ïƒ=identity; stride=1, pad=0, dilation=1, [bias, init])\nDepthwiseConv(weight::AbstractArray, [bias, activation; stride, pad, dilation])\n\nReturn a depthwise convolutional layer, that is a Conv layer with number of groups equal to the number of input channels.\n\nSee Conv for a description of the arguments.\n\nExamples\n\njulia> xs = rand(Float32, 100, 100, 3, 50);  # a batch of 50 RGB images\n\njulia> layer = DepthwiseConv((5,5), 3 => 6, relu; bias=false)\nConv((5, 5), 3 => 6, relu, groups=3, bias=false)  # 150 parameters \n\njulia> layer(xs) |> size\n(96, 96, 6, 50)\n\njulia> DepthwiseConv((5, 5), 3 => 9, stride=2, pad=2)(xs) |> size\n(50, 50, 9, 50)\n\n\n\n\n\n","category":"function"},{"location":"models/layers/#Flux.ConvTranspose","page":"Built-in Layers ðŸ“š","title":"Flux.ConvTranspose","text":"ConvTranspose(filter, in => out, Ïƒ=identity; stride=1, pad=0, dilation=1, [bias, init])\n\nStandard convolutional transpose layer. filter is a tuple of integers specifying the size of the convolutional kernel, while in and out specify the number of input and output channels.\n\nNote that pad=SamePad() here tries to ensure size(output,d) == size(x,d) * stride.\n\nParameters are controlled by additional keywords, with defaults init=glorot_uniform and bias=true.\n\nSee also Conv for more detailed description of keywords.\n\nExamples\n\njulia> xs = rand(Float32, 100, 100, 3, 50);  # a batch of 50 RGB images\n\njulia> layer = ConvTranspose((5,5), 3 => 7, relu)\nConvTranspose((5, 5), 3 => 7, relu)  # 532 parameters\n\njulia> layer(xs) |> size\n(104, 104, 7, 50)\n\njulia> ConvTranspose((5,5), 3 => 7, stride=2)(xs) |> size\n(203, 203, 7, 50)\n\njulia> ConvTranspose((5,5), 3 => 7, stride=3, pad=SamePad())(xs) |> size\n(300, 300, 7, 50)\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Flux.ConvTranspose-Tuple{AbstractArray}","page":"Built-in Layers ðŸ“š","title":"Flux.ConvTranspose","text":"ConvTranspose(weight::AbstractArray, [bias, activation; stride, pad, dilation, groups])\n\nConstructs a ConvTranspose layer with the given weight and bias. Accepts the same keywords and has the same defaults as ConvTranspose(k::NTuple{N,Integer}, ch::Pair{<:Integer,<:Integer}, Ïƒ; ...).\n\nExamples\n\njulia> weight = rand(3, 4, 5);\n\njulia> bias = zeros(4);\n\njulia> layer = ConvTranspose(weight, bias, sigmoid)\nConvTranspose((3,), 5 => 4, Ïƒ)  # 64 parameters\n\njulia> layer(randn(100, 5, 64)) |> size  # transposed convolution will increase the dimension size (upsampling)\n(102, 4, 64)\n\njulia> Flux.params(layer) |> length\n2\n\n\n\n\n\n","category":"method"},{"location":"models/layers/#Flux.CrossCor","page":"Built-in Layers ðŸ“š","title":"Flux.CrossCor","text":"CrossCor(filter, in => out, Ïƒ=identity; stride=1, pad=0, dilation=1, [bias, init])\n\nStandard cross correlation layer. filter is a tuple of integers specifying the size of the convolutional kernel; in and out specify the number of input and output channels.\n\nParameters are controlled by additional keywords, with defaults init=glorot_uniform and bias=true.\n\nSee also Conv for more detailed description of keywords.\n\nExamples\n\njulia> xs = rand(Float32, 100, 100, 3, 50);  # a batch of 50 RGB images\n\njulia> layer = CrossCor((5,5), 3 => 6, relu; bias=false)\nCrossCor((5, 5), 3 => 6, relu, bias=false)  # 450 parameters\n\njulia> layer(xs) |> size\n(96, 96, 6, 50)\n\njulia> CrossCor((5,5), 3 => 7, stride=3, pad=(2,0))(xs) |> size\n(34, 32, 7, 50)\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Flux.CrossCor-Tuple{AbstractArray}","page":"Built-in Layers ðŸ“š","title":"Flux.CrossCor","text":"CrossCor(weight::AbstractArray, [bias, activation; stride, pad, dilation])\n\nConstructs a CrossCor layer with the given weight and bias. Accepts the same keywords and has the same defaults as CrossCor(k::NTuple{N,Integer}, ch::Pair{<:Integer,<:Integer}, Ïƒ; ...).\n\nExamples\n\njulia> weight = rand(3, 4, 5);\n\njulia> bias = zeros(5);\n\njulia> layer = CrossCor(weight, bias, relu)\nCrossCor((3,), 4 => 5, relu)  # 65 parameters\n\njulia> layer(randn(100, 4, 64)) |> size\n(98, 5, 64)\n\n\n\n\n\n","category":"method"},{"location":"models/layers/#Flux.SamePad","page":"Built-in Layers ðŸ“š","title":"Flux.SamePad","text":"SamePad()\n\nPassed as an option to convolutional layers (and friends), this causes the padding to be chosen such that the input and output sizes agree (on the first N dimensions, the kernel or window) when stride==1. When strideâ‰ 1, the output size equals ceil(input_size/stride).\n\nSee also Conv, MaxPool.\n\nExamples\n\njulia> xs = rand(Float32, 100, 100, 3, 50);  # a batch of images\n\njulia> layer = Conv((2,2), 3 => 7, pad=SamePad())\nConv((2, 2), 3 => 7, pad=(1, 0, 1, 0))  # 91 parameters\n\njulia> layer(xs) |> size  # notice how the dimensions stay the same with this padding\n(100, 100, 7, 50)\n\njulia> layer2 = Conv((2,2), 3 => 7)\nConv((2, 2), 3 => 7)  # 91 parameters\n\njulia> layer2(xs) |> size  # the output dimension changes as the padding was not \"same\"\n(99, 99, 7, 50)\n\njulia> layer3 = Conv((5, 5), 3 => 7, stride=2, pad=SamePad())\nConv((5, 5), 3 => 7, pad=2, stride=2)  # 532 parameters\n\njulia> layer3(xs) |> size  # output size = `ceil(input_size/stride)` = 50\n(50, 50, 7, 50)\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Flux.flatten","page":"Built-in Layers ðŸ“š","title":"Flux.flatten","text":"flatten(x::AbstractArray)\n\nReshape arbitrarly-shaped input into a matrix-shaped output, preserving the size of the last dimension.\n\nSee also unsqueeze.\n\nExamples\n\njulia> rand(3,4,5) |> Flux.flatten |> size\n(12, 5)\n\njulia> xs = rand(Float32, 10,10,3,7);\n\njulia> m = Chain(Conv((3,3), 3 => 4, pad=1), Flux.flatten, Dense(400 => 33));\n\njulia> xs |> m[1] |> size\n(10, 10, 4, 7)\n\njulia> xs |> m |> size\n(33, 7)\n\n\n\n\n\n","category":"function"},{"location":"models/layers/#Upsampling-Layers","page":"Built-in Layers ðŸ“š","title":"Upsampling Layers","text":"","category":"section"},{"location":"models/layers/","page":"Built-in Layers ðŸ“š","title":"Built-in Layers ðŸ“š","text":"Upsample\nPixelShuffle","category":"page"},{"location":"models/layers/#Flux.Upsample","page":"Built-in Layers ðŸ“š","title":"Flux.Upsample","text":"Upsample(mode = :nearest; [scale, size]) \nUpsample(scale, mode = :nearest)\n\nAn upsampling layer. One of two keywords must be given:\n\nIf scale is a number, this applies to all but the last two dimensions (channel and batch) of the input.  It may also be a tuple, to control dimensions individually. Alternatively, keyword  size accepts a tuple, to directly specify the leading dimensions of the output.\n\nCurrently supported upsampling modes  and corresponding NNlib's methods are:\n\n:nearest -> NNlib.upsample_nearest \n:bilinear -> NNlib.upsample_bilinear\n:trilinear -> NNlib.upsample_trilinear\n\nExamples\n\njulia> m = Upsample(scale = (2, 3))\nUpsample(:nearest, scale = (2, 3))\n\njulia> m(ones(2, 2, 1, 1)) |> size\n(4, 6, 1, 1)\n\njulia> m = Upsample(:bilinear, size = (4, 5))\nUpsample(:bilinear, size = (4, 5))\n\njulia> m(ones(2, 2, 1, 1)) |> size\n(4, 5, 1, 1)\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Flux.PixelShuffle","page":"Built-in Layers ðŸ“š","title":"Flux.PixelShuffle","text":"PixelShuffle(r::Int)\n\nPixel shuffling layer with upscale factor r. Usually used for generating higher resolution images while upscaling them.\n\nSee NNlib.pixel_shuffle.\n\nExamples\n\njulia> p = PixelShuffle(2);\n\njulia> xs = [2row + col + channel/10 for row in 1:2, col in 1:2, channel in 1:4, n in 1:1]\n2Ã—2Ã—4Ã—1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 3.1  4.1\n 5.1  6.1\n\n[:, :, 2, 1] =\n 3.2  4.2\n 5.2  6.2\n\n[:, :, 3, 1] =\n 3.3  4.3\n 5.3  6.3\n\n[:, :, 4, 1] =\n 3.4  4.4\n 5.4  6.4\n\njulia> p(xs)\n4Ã—4Ã—1Ã—1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 3.1  3.3  4.1  4.3\n 3.2  3.4  4.2  4.4\n 5.1  5.3  6.1  6.3\n 5.2  5.4  6.2  6.4\n\njulia> xs = [3row + col + channel/10 for row in 1:2, col in 1:3, channel in 1:4, n in 1:1]\n2Ã—3Ã—4Ã—1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 4.1  5.1  6.1\n 7.1  8.1  9.1\n\n[:, :, 2, 1] =\n 4.2  5.2  6.2\n 7.2  8.2  9.2\n\n[:, :, 3, 1] =\n 4.3  5.3  6.3\n 7.3  8.3  9.3\n\n[:, :, 4, 1] =\n 4.4  5.4  6.4\n 7.4  8.4  9.4\n\njulia> p(xs)\n4Ã—6Ã—1Ã—1 Array{Float64, 4}:\n[:, :, 1, 1] =\n 4.1  4.3  5.1  5.3  6.1  6.3\n 4.2  4.4  5.2  5.4  6.2  6.4\n 7.1  7.3  8.1  8.3  9.1  9.3\n 7.2  7.4  8.2  8.4  9.2  9.4\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Recurrent-Layers","page":"Built-in Layers ðŸ“š","title":"Recurrent Layers","text":"","category":"section"},{"location":"models/layers/","page":"Built-in Layers ðŸ“š","title":"Built-in Layers ðŸ“š","text":"Much like the core layers above, but can be used to process sequence data (as well as other kinds of structured data).","category":"page"},{"location":"models/layers/","page":"Built-in Layers ðŸ“š","title":"Built-in Layers ðŸ“š","text":"RNN\nLSTM\nGRU\nGRUv3\nFlux.Recur\nFlux.reset!","category":"page"},{"location":"models/layers/#Flux.RNN","page":"Built-in Layers ðŸ“š","title":"Flux.RNN","text":"RNN(in => out, Ïƒ = tanh)\n\nThe most basic recurrent layer; essentially acts as a Dense layer, but with the output fed back into the input each time step.\n\nThe arguments in and out describe the size of the feature vectors passed as input and as output. That is, it accepts a vector of length in or a batch of vectors represented as a in x B matrix and outputs a vector of length out or a batch of vectors of size out x B.\n\nThis constructor is syntactic sugar for Recur(RNNCell(a...)), and so RNNs are stateful. Note that the state shape can change depending on the inputs, and so it is good to reset! the model between inference calls if the batch size changes. See the examples below.\n\nExamples\n\njulia> r = RNN(3 => 5)\nRecur(\n  RNNCell(3 => 5, tanh),                # 50 parameters\n)         # Total: 4 trainable arrays, 50 parameters,\n          # plus 1 non-trainable, 5 parameters, summarysize 432 bytes.\n\njulia> r(rand(Float32, 3)) |> size\n(5,)\n\njulia> Flux.reset!(r);\n\njulia> r(rand(Float32, 3, 10)) |> size # batch size of 10\n(5, 10)\n\nwarning: Batch size changes\nFailing to call reset! when the input batch size changes can lead to unexpected behavior. See the following example:julia> r = RNN(3 => 5)\nRecur(\n  RNNCell(3 => 5, tanh),                # 50 parameters\n)         # Total: 4 trainable arrays, 50 parameters,\n          # plus 1 non-trainable, 5 parameters, summarysize 432 bytes.\n\njulia> r.state |> size\n(5, 1)\n\njulia> r(rand(Float32, 3)) |> size\n(5,)\n\njulia> r.state |> size\n(5, 1)\n\njulia> r(rand(Float32, 3, 10)) |> size # batch size of 10\n(5, 10)\n\njulia> r.state |> size # state shape has changed\n(5, 10)\n\njulia> r(rand(Float32, 3)) |> size # erroneously outputs a length 5*10 = 50 vector.\n(50,)\n\nNote:\n\nRNNCells can be constructed directly by specifying the non-linear function, the Wi and Wh internal matrices, a bias vector b, and a learnable initial state state0. The  Wi and Wh matrices do not need to be the same type, but if Wh is dxd, then Wi should be of shape dxN.\n\n```julia   julia> using LinearAlgebra\n\njulia> r = Flux.Recur(Flux.RNNCell(tanh, rand(5, 4), Tridiagonal(rand(5, 5)), rand(5), rand(5, 1)))\n\njulia> r(rand(4, 10)) |> size # batch size of 10   (5, 10)   ```\n\n\n\n\n\n","category":"function"},{"location":"models/layers/#Flux.LSTM","page":"Built-in Layers ðŸ“š","title":"Flux.LSTM","text":"LSTM(in => out)\n\nLong Short Term Memory recurrent layer. Behaves like an RNN but generally exhibits a longer memory span over sequences.\n\nThe arguments in and out describe the size of the feature vectors passed as input and as output. That is, it accepts a vector of length in or a batch of vectors represented as a in x B matrix and outputs a vector of length out or a batch of vectors of size out x B.\n\nThis constructor is syntactic sugar for Recur(LSTMCell(a...)), and so LSTMs are stateful. Note that the state shape can change depending on the inputs, and so it is good to reset! the model between inference calls if the batch size changes. See the examples below.\n\nSee this article for a good overview of the internals.\n\nExamples\n\njulia> l = LSTM(3 => 5)\nRecur(\n  LSTMCell(3 => 5),                     # 190 parameters\n)         # Total: 5 trainable arrays, 190 parameters,\n          # plus 2 non-trainable, 10 parameters, summarysize 1.062 KiB.\n\njulia> l(rand(Float32, 3)) |> size\n(5,)\n\njulia> Flux.reset!(l);\n\njulia> l(rand(Float32, 3, 10)) |> size # batch size of 10\n(5, 10)\n\nwarning: Batch size changes\nFailing to call reset! when the input batch size changes can lead to unexpected behavior. See the example in RNN.\n\nNote:\n\nLSTMCells can be constructed directly by specifying the non-linear function, the Wi and Wh internal matrices, a bias vector b, and a learnable initial state state0. The  Wi and Wh matrices do not need to be the same type. See the example in RNN.\n\n\n\n\n\n","category":"function"},{"location":"models/layers/#Flux.GRU","page":"Built-in Layers ðŸ“š","title":"Flux.GRU","text":"GRU(in => out)\n\nGated Recurrent Unit layer. Behaves like an RNN but generally exhibits a longer memory span over sequences. This implements the variant proposed in v1 of the referenced paper.\n\nThe integer arguments in and out describe the size of the feature vectors passed as input and as output. That is, it accepts a vector of length in or a batch of vectors represented as a in x B matrix and outputs a vector of length out or a batch of vectors of size out x B.\n\nThis constructor is syntactic sugar for Recur(GRUCell(a...)), and so GRUs are stateful. Note that the state shape can change depending on the inputs, and so it is good to reset! the model between inference calls if the batch size changes. See the examples below.\n\nSee this article for a good overview of the internals.\n\nExamples\n\njulia> g = GRU(3 => 5)\nRecur(\n  GRUCell(3 => 5),                      # 140 parameters\n)         # Total: 4 trainable arrays, 140 parameters,\n          # plus 1 non-trainable, 5 parameters, summarysize 792 bytes.\n\njulia> g(rand(Float32, 3)) |> size\n(5,)\n\njulia> Flux.reset!(g);\n\njulia> g(rand(Float32, 3, 10)) |> size # batch size of 10\n(5, 10)\n\nwarning: Batch size changes\nFailing to call reset! when the input batch size changes can lead to unexpected behavior. See the example in RNN.\n\nNote:\n\nGRUCells can be constructed directly by specifying the non-linear function, the Wi and Wh internal matrices, a bias vector b, and a learnable initial state state0. The  Wi and Wh matrices do not need to be the same type. See the example in RNN.\n\n\n\n\n\n","category":"function"},{"location":"models/layers/#Flux.GRUv3","page":"Built-in Layers ðŸ“š","title":"Flux.GRUv3","text":"GRUv3(in => out)\n\nGated Recurrent Unit layer. Behaves like an RNN but generally exhibits a longer memory span over sequences. This implements the variant proposed in v3 of the referenced paper.\n\nThe arguments in and out describe the size of the feature vectors passed as input and as output. That is, it accepts a vector of length in or a batch of vectors represented as a in x B matrix and outputs a vector of length out or a batch of vectors of size out x B.\n\nThis constructor is syntactic sugar for Recur(GRUv3Cell(a...)), and so GRUv3s are stateful. Note that the state shape can change depending on the inputs, and so it is good to reset! the model between inference calls if the batch size changes. See the examples below.\n\nSee this article for a good overview of the internals.\n\nExamples\n\njulia> g = GRUv3(3 => 5)\nRecur(\n  GRUv3Cell(3 => 5),                    # 140 parameters\n)         # Total: 5 trainable arrays, 140 parameters,\n          # plus 1 non-trainable, 5 parameters, summarysize 848 bytes.\n\njulia> g(rand(Float32, 3)) |> size\n(5,)\n\njulia> Flux.reset!(g);\n\njulia> g(rand(Float32, 3, 10)) |> size # batch size of 10\n(5, 10)\n\nwarning: Batch size changes\nFailing to call reset! when the input batch size changes can lead to unexpected behavior. See the example in RNN.\n\nNote:\n\nGRUv3Cells can be constructed directly by specifying the non-linear function, the Wi, Wh, and Wh_h internal matrices, a bias vector b, and a learnable initial state state0. The  Wi, Wh, and Wh_h matrices do not need to be the same type. See the example in RNN.\n\n\n\n\n\n","category":"function"},{"location":"models/layers/#Flux.Recur","page":"Built-in Layers ðŸ“š","title":"Flux.Recur","text":"Recur(cell)\n\nRecur takes a recurrent cell and makes it stateful, managing the hidden state in the background. cell should be a model of the form:\n\nh, y = cell(h, x...)\n\nFor example, here's a recurrent network that keeps a running total of its inputs:\n\nExamples\n\njulia> accum(h, x) = (h + x, x)\naccum (generic function with 1 method)\n\njulia> rnn = Flux.Recur(accum, 0)\nRecur(accum)\n\njulia> rnn(2) \n2\n\njulia> rnn(3)\n3\n\njulia> rnn.state\n5\n\nFolding over a 3d Array of dimensions (features, batch, time) is also supported:\n\njulia> accum(h, x) = (h .+ x, x)\naccum (generic function with 1 method)\n\njulia> rnn = Flux.Recur(accum, zeros(Int, 1, 1))\nRecur(accum)\n\njulia> rnn([2])\n1-element Vector{Int64}:\n 2\n\njulia> rnn([3])\n1-element Vector{Int64}:\n 3\n\njulia> rnn.state\n1Ã—1 Matrix{Int64}:\n 5\n\njulia> out = rnn(reshape(1:10, 1, 1, :));  # apply to a sequence of (features, batch, time)\n\njulia> out |> size\n(1, 1, 10)\n\njulia> vec(out)\n10-element Vector{Int64}:\n  1\n  2\n  3\n  4\n  5\n  6\n  7\n  8\n  9\n 10\n\njulia> rnn.state\n1Ã—1 Matrix{Int64}:\n 60\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Flux.reset!","page":"Built-in Layers ðŸ“š","title":"Flux.reset!","text":"reset!(rnn)\n\nReset the hidden state of a recurrent layer back to its original value.\n\nAssuming you have a Recur layer rnn, this is roughly equivalent to:\n\nrnn.state = hidden(rnn.cell)\n\nExamples\n\njulia> r = Flux.RNNCell(relu, ones(1,1), zeros(1,1), ones(1,1), zeros(1,1));  # users should use the RNN wrapper struct instead\n\njulia> y = Flux.Recur(r, ones(1,1));\n\njulia> y.state\n1Ã—1 Matrix{Float64}:\n 1.0\n\njulia> y(ones(1,1))  # relu(1*1 + 1)\n1Ã—1 Matrix{Float64}:\n 2.0\n\njulia> y.state\n1Ã—1 Matrix{Float64}:\n 2.0\n\njulia> Flux.reset!(y)\n1Ã—1 Matrix{Float64}:\n 0.0\n\njulia> y.state\n1Ã—1 Matrix{Float64}:\n 0.0\n\n\n\n\n\n","category":"function"},{"location":"models/layers/#Other-General-Purpose-Layers","page":"Built-in Layers ðŸ“š","title":"Other General Purpose Layers","text":"","category":"section"},{"location":"models/layers/","page":"Built-in Layers ðŸ“š","title":"Built-in Layers ðŸ“š","text":"These are marginally more obscure than the Basic Layers. But in contrast to the layers described in the other sections are not readily grouped around a particular purpose (e.g. CNNs or RNNs).","category":"page"},{"location":"models/layers/","page":"Built-in Layers ðŸ“š","title":"Built-in Layers ðŸ“š","text":"Maxout\nSkipConnection\nParallel\nFlux.Bilinear\nFlux.Scale\nFlux.Embedding","category":"page"},{"location":"models/layers/#Flux.Maxout","page":"Built-in Layers ðŸ“š","title":"Flux.Maxout","text":"Maxout(layers...)\nMaxout(f, n_alts)\n\nThis contains a number of internal layers, each of which receives the same input. Its output is the elementwise maximum of the the internal layers' outputs.\n\nInstead of defining layers individually, you can provide a zero-argument function which constructs them, and the number to construct.\n\nMaxout over linear dense layers satisfies the univeral approximation theorem. See Goodfellow, Warde-Farley, Mirza, Courville & Bengio \"Maxout Networks\"  https://arxiv.org/abs/1302.4389.\n\nSee also Parallel to reduce with other operators.\n\nExamples\n\njulia> m = Maxout(x -> abs2.(x), x -> x .* 3);\n\njulia> m([-2 -1 0 1 2])\n1Ã—5 Matrix{Int64}:\n 4  1  0  3  6\n\njulia> m3 = Maxout(() -> Dense(5 => 7, tanh), 3)\nMaxout(\n  Dense(5 => 7, tanh),                  # 42 parameters\n  Dense(5 => 7, tanh),                  # 42 parameters\n  Dense(5 => 7, tanh),                  # 42 parameters\n)                   # Total: 6 arrays, 126 parameters, 888 bytes.\n\njulia> Flux.outputsize(m3, (5, 11))\n(7, 11)\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Flux.SkipConnection","page":"Built-in Layers ðŸ“š","title":"Flux.SkipConnection","text":"SkipConnection(layer, connection)\n\nCreate a skip connection which consists of a layer or Chain of consecutive layers and a shortcut connection linking the block's input to the output through a user-supplied 2-argument callable. The first argument to the callable will be propagated through the given layer while the second is the unchanged, \"skipped\" input.\n\nThe simplest \"ResNet\"-type connection is just SkipConnection(layer, +). Here is a more complicated example:\n\njulia> m = Conv((3,3), 4 => 7, pad=(1,1));\n\njulia> x = ones(Float32, 5, 5, 4, 10);\n\njulia> size(m(x)) == (5, 5, 7, 10)\ntrue\n\njulia> sm = SkipConnection(m, (mx, x) -> cat(mx, x, dims=3));\n\njulia> size(sm(x)) == (5, 5, 11, 10)\ntrue\n\nSee also Parallel, Maxout.\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Flux.Parallel","page":"Built-in Layers ðŸ“š","title":"Flux.Parallel","text":"Parallel(connection, layers...)\nParallel(connection; name = layer, ...)\n\nCreate a layer which passes an input array to each path in layers, before reducing the output with connection.\n\nCalled with one input x, this is equivalent to connection([l(x) for l in layers]...). If called with multiple inputs, one is passed to each layer, thus Parallel(+, f, g)(x, y) = f(x) + g(y).\n\nLike Chain, its sub-layers may be given names using the keyword constructor. These can be accessed by indexing: m[1] == m[:name] is the first layer.\n\nSee also SkipConnection which is Parallel with one identity, and Maxout which reduces by broadcasting max.\n\nExamples\n\njulia> model = Chain(Dense(3 => 5),\n                     Parallel(vcat, Dense(5 => 4), Chain(Dense(5 => 7), Dense(7 => 4))),\n                     Dense(8 => 17));\n\njulia> model(rand(3)) |> size\n(17,)\n\njulia> model2 = Parallel(+; Î± = Dense(10, 2, tanh), Î² = Dense(5, 2))\nParallel(\n  +,\n  Î± = Dense(10 => 2, tanh),             # 22 parameters\n  Î² = Dense(5 => 2),                    # 12 parameters\n)                   # Total: 4 arrays, 34 parameters, 392 bytes.\n\njulia> model2(rand(10), rand(5)) |> size\n(2,)\n\njulia> model2[:Î±](rand(10)) |> size\n(2,)\n\njulia> model2[:Î²] == model2[2]\ntrue\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Flux.Bilinear","page":"Built-in Layers ðŸ“š","title":"Flux.Bilinear","text":"Bilinear((in1, in2) => out, Ïƒ=identity; bias=true, init=glorot_uniform)\nBilinear(W::AbstractArray, [bias, Ïƒ])\n\nCreates a layer which is fully connected between two inputs and the output, and otherwise similar to Dense. Its output, given vectors x & y, is another vector z with, for all i âˆˆ 1:out:\n\nz[i] = Ïƒ(x' * W[i,:,:] * y + bias[i])\n\nIf x and y are matrices, then each column of the output z = B(x, y) is of this form, with B the Bilinear layer.\n\nIf the second input y is not given, it is taken to be equal to x, i.e. B(x) == B(x, x)\n\nThe two inputs may also be provided as a tuple, B((x, y)) == B(x, y), which is accepted as the input to a Chain.\n\nIf the two input sizes are the same, in1 == in2, then you may write Bilinear(in => out, Ïƒ).\n\nThe initialisation works as for Dense layer, with W = init(out, in1, in2). By default the bias vector is zeros(Float32, out), option bias=false will switch off trainable bias. Either of these may be provided explicitly.\n\nExamples\n\njulia> x, y = randn(Float32, 5, 32), randn(Float32, 5, 32);\n\njulia> B = Flux.Bilinear((5, 5) => 7)\nBilinear(5 => 7)    # 182 parameters\n\njulia> B(x) |> size  # interactions based on one input\n(7, 32)\n\njulia> B(x,y) == B((x,y))  # two inputs, may be given as a tuple\ntrue\n\njulia> sc = SkipConnection(\n                Chain(Dense(5 => 20, tanh), Dense(20 => 9, tanh)),\n                Flux.Bilinear((9, 5) => 3, bias=false),\n            );  # used as the recombinator, with skip as the second input\n\njulia> sc(x) |> size\n(3, 32)\n\njulia> Flux.Bilinear(rand(4,8,16), false, tanh)  # first dim of weight is the output\nBilinear((8, 16) => 4, tanh; bias=false)  # 512 parameters\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Flux.Scale","page":"Built-in Layers ðŸ“š","title":"Flux.Scale","text":"Scale(size::Integer..., Ïƒ=identity; bias=true, init=ones32)\nScale(scale::AbstractArray, [bias, Ïƒ])\n\nCreate an element-wise layer, whose forward pass is given by:\n\ny = Ïƒ.(scale .* x .+ bias)\n\nThis uses .* instead of matrix multiplication * of Dense.\n\nThe learnable scale & bias are initialised init(size...) and zeros32(size...), with init=ones32 by default. You may specify the function init,  turn off trainable bias with bias=false, or provide the array(s) explicitly.\n\nUsed by LayerNorm with affine=true.\n\nExamples\n\njulia> a = Flux.Scale(2)\nScale(2)            # 4 parameters\n\njulia> Flux.params(a)\nParams([Float32[1.0, 1.0], Float32[0.0, 0.0]])\n\njulia> a([1 2 3])\n2Ã—3 Matrix{Float32}:\n 1.0  2.0  3.0\n 1.0  2.0  3.0\n\njulia> b = Flux.Scale([1 2 3 4], false, abs2)\nScale(1, 4, abs2; bias=false)  # 4 parameters\n\njulia> b([1, 10])\n2Ã—4 Matrix{Int64}:\n   1    4    9    16\n 100  400  900  1600\n\njulia> Flux.params(b)\nParams([[1 2 3 4]])\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Flux.Embedding","page":"Built-in Layers ðŸ“š","title":"Flux.Embedding","text":"Embedding(in => out; init=randn32)\n\nA lookup table that stores embeddings of dimension out  for a vocabulary of size in, as a trainable matrix.\n\nThis layer is often used to store word embeddings and retrieve them using indices.  The input to the layer can be a vocabulary index in 1:in, an array of indices, or the corresponding onehot encoding.\n\nFor indices x, the result is of size (out, size(x)...), allowing several batch dimensions. For one-hot ohx, the result is of size (out, size(ohx)[2:end]...).\n\nExamples\n\njulia> emb = Embedding(26 => 4, init=Flux.identity_init(gain=22))\nEmbedding(26 => 4)  # 104 parameters\n\njulia> emb(2)  # one column of e.weight (here not random!)\n4-element Vector{Float32}:\n  0.0\n 22.0\n  0.0\n  0.0\n\njulia> emb([3, 1, 20, 14, 4, 15, 7])  # vocabulary indices, in 1:26\n4Ã—7 Matrix{Float32}:\n  0.0  22.0  0.0  0.0   0.0  0.0  0.0\n  0.0   0.0  0.0  0.0   0.0  0.0  0.0\n 22.0   0.0  0.0  0.0   0.0  0.0  0.0\n  0.0   0.0  0.0  0.0  22.0  0.0  0.0\n\njulia> ans == emb(Flux.onehotbatch(\"cat&dog\", 'a':'z', 'n'))\ntrue\n\njulia> emb(rand(1:26, (10, 1, 12))) |> size  # three batch dimensions\n(4, 10, 1, 12)\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Normalisation-and-Regularisation","page":"Built-in Layers ðŸ“š","title":"Normalisation & Regularisation","text":"","category":"section"},{"location":"models/layers/","page":"Built-in Layers ðŸ“š","title":"Built-in Layers ðŸ“š","text":"These layers don't affect the structure of the network but may improve training times or reduce overfitting.","category":"page"},{"location":"models/layers/","page":"Built-in Layers ðŸ“š","title":"Built-in Layers ðŸ“š","text":"Flux.normalise\nBatchNorm\nDropout\nFlux.dropout\nAlphaDropout\nLayerNorm\nInstanceNorm\nGroupNorm","category":"page"},{"location":"models/layers/#Flux.normalise","page":"Built-in Layers ðŸ“š","title":"Flux.normalise","text":"normalise(x; dims=ndims(x), Ïµ=1e-5)\n\nNormalise x to mean 0 and standard deviation 1 across the dimension(s) given by dims. Per default, dims is the last dimension.  Ïµ is a small additive factor added to the denominator for numerical stability.\n\nExamples\n\njulia> using Statistics\n\njulia> x = [9, 10, 20, 60];\n\njulia> y = Flux.normalise(x);\n\njulia> isapprox(std(y), 1, atol=0.2) && std(y) != std(x)\ntrue\n\njulia> x = rand(1:100, 10, 2);\n\njulia> y = Flux.normalise(x, dims=1);\n\njulia> isapprox(std(y, dims=1), ones(1, 2), atol=0.2) && std(y, dims=1) != std(x, dims=1)\ntrue\n\n\n\n\n\n","category":"function"},{"location":"models/layers/#Flux.BatchNorm","page":"Built-in Layers ðŸ“š","title":"Flux.BatchNorm","text":"BatchNorm(channels::Integer, Î»=identity;\n          initÎ²=zeros32, initÎ³=ones32,\n          affine = true, track_stats = true,\n          Ïµ=1f-5, momentum= 0.1f0)\n\nBatch Normalization layer. channels should be the size of the channel dimension in your data (see below).\n\nGiven an array with N dimensions, call the N-1th the channel dimension. For a batch of feature vectors this is just the data dimension, for WHCN images it's the usual channel dimension.\n\nBatchNorm computes the mean and variance for each D_1Ã—...Ã—D_{N-2}Ã—1Ã—D_N input slice and normalises the input accordingly.\n\nIf affine=true, it also applies  a shift and a rescale to the input through to learnable per-channel bias Î² and scale Î³ parameters.\n\nAfter normalisation, elementwise activation Î» is applied.\n\nIf track_stats=true, accumulates mean and var statistics in training phase that will be used to renormalize the input in test phase.\n\nUse testmode! during inference.\n\nExamples\n\njulia> using Statistics\n\njulia> xs = rand(3, 3, 3, 2);  # a batch of 2 images, each having 3 channels\n\njulia> m = BatchNorm(3);\n\njulia> Flux.trainmode!(m);\n\njulia> isapprox(std(m(xs)), 1, atol=0.1) && std(xs) != std(m(xs))\ntrue\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Flux.Dropout","page":"Built-in Layers ðŸ“š","title":"Flux.Dropout","text":"Dropout(p; dims=:, rng = default_rng_value())\n\nDropout layer.\n\nWhile training, for each input, this layer either sets that input to 0 (with probability p) or scales it by 1 / (1 - p). To apply dropout along certain dimension(s), specify the  dims keyword. e.g. Dropout(p; dims = 3) will randomly zero out entire channels on WHCN input (also called 2D dropout). This is used as a regularisation, i.e. it reduces overfitting during  training.\n\nIn the forward pass, this layer applies the Flux.dropout function. See that for more details.\n\nSpecify rng to use a custom RNG instead of the default. Custom RNGs are only supported on the CPU.\n\nDoes nothing to the input once Flux.testmode! is true.\n\nExamples\n\njulia> m = Chain(Dense(1 => 1), Dropout(1));\n\njulia> Flux.trainmode!(m);\n\njulia> y = m([1]);\n\njulia> y == [0]\ntrue\n\njulia> m = Chain(Dense(1000 => 1000), Dropout(0.5));\n\njulia> Flux.trainmode!(m);\n\njulia> y = m(ones(1000));\n\njulia> isapprox(count(==(0), y) / length(y), 0.5, atol=0.1)\ntrue\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Flux.dropout","page":"Built-in Layers ðŸ“š","title":"Flux.dropout","text":"dropout([rng = rng_from_array(x)], x, p; dims=:, active=true)\n\nThe dropout function. If active is true, for each input, either sets that input to 0 (with probability p) or scales it by 1 / (1 - p). dims specifies the unbroadcasted dimensions, e.g. dims=1 applies dropout along columns and dims=2 along rows. If active is false, it just returns the input x.\n\nSpecify rng for custom RNGs instead of the default RNG. Note that custom RNGs are only supported on the CPU.\n\nWarning: when using this function, you have to manually manage the activation state. Usually in fact, dropout is used while training but is deactivated in the inference phase. This can be automatically managed using the Dropout layer instead of the dropout function.\n\nThe Dropout layer is what you should use in most scenarios.\n\n\n\n\n\n","category":"function"},{"location":"models/layers/#Flux.AlphaDropout","page":"Built-in Layers ðŸ“š","title":"Flux.AlphaDropout","text":"AlphaDropout(p; rng = default_rng_value())\n\nA dropout layer. Used in Self-Normalizing Neural Networks. The AlphaDropout layer ensures that mean and variance of activations remain the same as before.\n\nDoes nothing to the input once testmode! is true.\n\nExamples\n\njulia> using Statistics\n\njulia> x = randn(1000,1);\n\njulia> m = Chain(Dense(1000 => 1000, selu), AlphaDropout(0.2));\n\njulia> Flux.trainmode!(m);\n\njulia> y = m(x);\n\njulia> isapprox(std(x), std(y), atol=0.2)\ntrue\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Flux.LayerNorm","page":"Built-in Layers ðŸ“š","title":"Flux.LayerNorm","text":"LayerNorm(size..., Î»=identity; affine=true, Ïµ=1fe-5)\n\nA normalisation layer designed to be used with recurrent hidden states. The argument size should be an integer or a tuple of integers. In the forward pass, the layer normalises the mean and standard deviation of the input, then applies the elementwise activation Î». The input is normalised along the first length(size) dimensions for tuple size, and along the first dimension for integer size. The input is expected to have first dimensions' size equal to size.\n\nIf affine=true, it also applies a learnable shift and rescaling using the Scale layer.\n\nSee also BatchNorm, InstanceNorm, GroupNorm, and normalise.\n\nExamples\n\njulia> using Statistics\n\njulia> xs = rand(3, 3, 3, 2);  # a batch of 2 images, each having 3 channels\n\njulia> m = LayerNorm(3);\n\njulia> y = m(xs);\n\njulia> isapprox(std(y, dims=1:3), ones(1, 1, 1, 2), atol=0.1) && std(y, dims=1:3) != std(xs, dims=1:3)\ntrue\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Flux.InstanceNorm","page":"Built-in Layers ðŸ“š","title":"Flux.InstanceNorm","text":"InstanceNorm(channels::Integer, Î»=identity;\n             initÎ²=zeros32, initÎ³=ones32,\n             affine=false, track_stats=false,\n             Ïµ=1f-5, momentum=0.1f0)\n\nInstance Normalization layer. channels should be the size of the channel dimension in your data (see below).\n\nGiven an array with N > 2 dimensions, call the N-1th the channel dimension. For WHCN images it's the usual channel dimension.\n\nInstanceNorm computes the mean and variance for each D_1Ã—...Ã—D_{N-2}Ã—1Ã—1 input slice and normalises the input accordingly.\n\nIf affine=true, it also applies  a shift and a rescale to the input through to learnable per-channel bias Î² and scale Î³ parameters.\n\nIf track_stats=true, accumulates mean and var statistics in training phase that will be used to renormalize the input in test phase.\n\nWarning: the defaults for affine and track_stats used to be true in previous Flux versions (< v0.12).\n\nExamples\n\njulia> using Statistics\n\njulia> xs = rand(3, 3, 3, 2);  # a batch of 2 images, each having 3 channels\n\njulia> m = InstanceNorm(3);\n\njulia> y = m(xs);\n\njulia> isapprox(std(y, dims=1:2), ones(1, 1, 3, 2), atol=0.2) && std(y, dims=1:2) != std(xs, dims=1:2)\ntrue\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Flux.GroupNorm","page":"Built-in Layers ðŸ“š","title":"Flux.GroupNorm","text":"GroupNorm(channels::Integer, G::Integer, Î»=identity;\n          initÎ²=zeros32, initÎ³=ones32,\n          affine=true, track_stats=false,\n          Ïµ=1f-5, momentum=0.1f0)\n\nGroup Normalization layer.\n\nchs is the number of channels, the channel dimension of your input. For an array of N dimensions, the N-1th index is the channel dimension.\n\nG is the number of groups along which the statistics are computed. The number of channels must be an integer multiple of the number of groups.\n\nchannels should be the size of the channel dimension in your data (see below).\n\nGiven an array with N > 2 dimensions, call the N-1th the channel dimension. For WHCN images it's the usual channel dimension.\n\nIf affine=true, it also applies  a shift and a rescale to the input through to learnable per-channel bias Î² and scale Î³ parameters.\n\nIf track_stats=true, accumulates mean and var statistics in training phase that will be used to renormalize the input in test phase.\n\nExamples\n\njulia> using Statistics\n\njulia> xs = rand(3, 3, 4, 2);  # a batch of 2 images, each having 4 channels\n\njulia> m = GroupNorm(4, 2);\n\njulia> y = m(xs);\n\njulia> isapprox(std(y[:, :, 1:2, 1]), 1, atol=0.1) && std(xs[:, :, 1:2, 1]) != std(y[:, :, 1:2, 1])\ntrue\n\njulia> isapprox(std(y[:, :, 3:4, 2]), 1, atol=0.1) && std(xs[:, :, 3:4, 2]) != std(y[:, :, 3:4, 2])\ntrue\n\n\n\n\n\n","category":"type"},{"location":"models/layers/#Testmode","page":"Built-in Layers ðŸ“š","title":"Testmode","text":"","category":"section"},{"location":"models/layers/","page":"Built-in Layers ðŸ“š","title":"Built-in Layers ðŸ“š","text":"Many normalisation layers behave differently under training and inference (testing). By default, Flux will automatically determine when a layer evaluation is part of training or inference. Still, depending on your use case, it may be helpful to manually specify when these layers should be treated as being trained or not. For this, Flux provides Flux.testmode!. When called on a model (e.g. a layer or chain of layers), this function will place the model into the mode specified.","category":"page"},{"location":"models/layers/","page":"Built-in Layers ðŸ“š","title":"Built-in Layers ðŸ“š","text":"Flux.testmode!\ntrainmode!","category":"page"},{"location":"models/layers/#Flux.testmode!","page":"Built-in Layers ðŸ“š","title":"Flux.testmode!","text":"testmode!(m, mode = true)\n\nSet a layer or model's test mode (see below). Using :auto mode will treat any gradient computation as training.\n\nNote: if you manually set a model into test mode, you need to manually place it back into train mode during training phase.\n\nPossible values include:\n\nfalse for training\ntrue for testing\n:auto or nothing for Flux to detect the mode automatically\n\n\n\n\n\n","category":"function"},{"location":"models/layers/#Flux.trainmode!","page":"Built-in Layers ðŸ“š","title":"Flux.trainmode!","text":"trainmode!(m, mode = true)\n\nSet a layer of model's train mode (see below). Symmetric to testmode! (i.e. trainmode!(m, mode) == testmode!(m, !mode)).\n\nNote: if you manually set a model into train mode, you need to manually place it into test mode during testing phase.\n\nPossible values include:\n\ntrue for training\nfalse for testing\n:auto or nothing for Flux to detect the mode automatically\n\n\n\n\n\n","category":"function"},{"location":"models/quickstart/#man-quickstart","page":"Quick Start","title":"A Neural Network in One Minute","text":"","category":"section"},{"location":"models/quickstart/","page":"Quick Start","title":"Quick Start","text":"If you have used neural networks before, then this simple example might be helpful for seeing how the major parts of Flux work together. Try pasting the code into the REPL prompt.","category":"page"},{"location":"models/quickstart/","page":"Quick Start","title":"Quick Start","text":"If you haven't, then you might prefer the Fitting a Straight Line page.","category":"page"},{"location":"models/quickstart/","page":"Quick Start","title":"Quick Start","text":"# With Julia 1.7+, this will prompt if neccessary to install everything, including CUDA:\nusing Flux, Statistics\n\n# Generate some data for the XOR problem: vectors of length 2, as columns of a matrix:\nnoisy = rand(Float32, 2, 1000)                                    # 2Ã—1000 Matrix{Float32}\ntruth = map(col -> xor(col...), eachcol(noisy .> 0.5))            # 1000-element Vector{Bool}\n\n# Define our model, a multi-layer perceptron with one hidden layer of size 3:\nmodel = Chain(Dense(2 => 3, tanh), BatchNorm(3), Dense(3 => 2), softmax)\n\n# The model encapsulates parameters, randomly initialised. Its initial output is:\nout1 = model(noisy)                                               # 2Ã—1000 Matrix{Float32}\n\n# To train the model, we use batches of 64 samples:\nmat = Flux.onehotbatch(truth, [true, false])                      # 2Ã—1000 OneHotMatrix\ndata = Flux.DataLoader((noisy, mat), batchsize=64, shuffle=true);\nfirst(data) .|> summary                                           # (\"2Ã—64 Matrix{Float32}\", \"2Ã—64 Matrix{Bool}\")\n\npars = Flux.params(model)  # contains references to arrays in model\nopt = Flux.Adam(0.01)      # will store optimiser momentum, etc.\n\n# Training loop, using the whole data set 1000 times:\nfor epoch in 1:1_000\n    Flux.train!(pars, data, opt) do x, y\n        # First argument of train! is a loss function, here defined by a `do` block.\n        # This gets x and y, each a 2Ã—64 Matrix, from data, and compares:\n        Flux.crossentropy(model(x), y)\n    end\nend\n\npars  # has changed!\nopt\nout2 = model(noisy)\n\nmean((out2[1,:] .> 0.5) .== truth)  # accuracy 94% so far!","category":"page"},{"location":"models/quickstart/","page":"Quick Start","title":"Quick Start","text":"(Image: )","category":"page"},{"location":"models/quickstart/","page":"Quick Start","title":"Quick Start","text":"using Plots  # to draw the above figure\n\np_true = scatter(noisy[1,:], noisy[2,:], zcolor=truth, title=\"True classification\", legend=false)\np_raw =  scatter(noisy[1,:], noisy[2,:], zcolor=out1[1,:], title=\"Untrained network\", label=\"\", clims=(0,1))\np_done = scatter(noisy[1,:], noisy[2,:], zcolor=out2[1,:], title=\"Trained network\", legend=false)\n\nplot(p_true, p_raw, p_done, layout=(1,3), size=(1000,330))","category":"page"},{"location":"models/quickstart/","page":"Quick Start","title":"Quick Start","text":"This XOR (\"exclusive or\") problem is a variant of the famous one which drove Minsky and Papert to invent deep neural networks in 1969. For small values of \"deep\" â€“ this has one hidden layer, while earlier perceptrons had none. (What they call a hidden layer, Flux calls the output of the first layer, model[1](noisy).)","category":"page"},{"location":"models/quickstart/","page":"Quick Start","title":"Quick Start","text":"Since then things have developed a little. ","category":"page"},{"location":"models/quickstart/#Features-of-Note","page":"Quick Start","title":"Features of Note","text":"","category":"section"},{"location":"models/quickstart/","page":"Quick Start","title":"Quick Start","text":"Some things to notice in this example are:","category":"page"},{"location":"models/quickstart/","page":"Quick Start","title":"Quick Start","text":"The batch dimension of data is always the last one. Thus a 2Ã—1000 Matrix is a thousand observations, each a column of length 2.\nThe model can be called like a function, y = model(x). It encapsulates the parameters (and state).\nBut the model does not contain the loss function, nor the optimisation rule. Instead the Adam() object stores between iterations the momenta it needs.\nThe function train! likes data as an iterator generating Tuples, here produced by DataLoader. This mutates both the model and the optimiser state inside opt.","category":"page"},{"location":"models/quickstart/","page":"Quick Start","title":"Quick Start","text":"There are other ways to train Flux models, for more control than train! provides:","category":"page"},{"location":"models/quickstart/","page":"Quick Start","title":"Quick Start","text":"Within Flux, you can easily write a training loop, calling gradient and update!.\nFor a lower-level way, see the package Optimisers.jl.\nFor higher-level ways, see FluxTraining.jl and FastAI.jl.","category":"page"},{"location":"models/losses/#man-losses","page":"Loss Functions ðŸ“š","title":"Loss Functions","text":"","category":"section"},{"location":"models/losses/","page":"Loss Functions ðŸ“š","title":"Loss Functions ðŸ“š","text":"Flux provides a large number of common loss functions used for training machine learning models. They are grouped together in the Flux.Losses module.","category":"page"},{"location":"models/losses/","page":"Loss Functions ðŸ“š","title":"Loss Functions ðŸ“š","text":"Loss functions for supervised learning typically expect as inputs a target y, and a prediction yÌ‚ from your model. In Flux's convention, the order of the arguments is the following","category":"page"},{"location":"models/losses/","page":"Loss Functions ðŸ“š","title":"Loss Functions ðŸ“š","text":"loss(yÌ‚, y)","category":"page"},{"location":"models/losses/","page":"Loss Functions ðŸ“š","title":"Loss Functions ðŸ“š","text":"Most loss functions in Flux have an optional argument agg, denoting the type of aggregation performed over the batch:","category":"page"},{"location":"models/losses/","page":"Loss Functions ðŸ“š","title":"Loss Functions ðŸ“š","text":"loss(yÌ‚, y)                         # defaults to `mean`\nloss(yÌ‚, y, agg=sum)                # use `sum` for reduction\nloss(yÌ‚, y, agg=x->sum(x, dims=2))  # partial reduction\nloss(yÌ‚, y, agg=x->mean(w .* x))    # weighted mean\nloss(yÌ‚, y, agg=identity)           # no aggregation.","category":"page"},{"location":"models/losses/#Function-listing","page":"Loss Functions ðŸ“š","title":"Function listing","text":"","category":"section"},{"location":"models/losses/","page":"Loss Functions ðŸ“š","title":"Loss Functions ðŸ“š","text":"Flux.Losses.mae\nFlux.Losses.mse\nFlux.Losses.msle\nFlux.Losses.huber_loss\nFlux.Losses.label_smoothing\nFlux.Losses.crossentropy\nFlux.Losses.logitcrossentropy\nFlux.Losses.binarycrossentropy\nFlux.Losses.logitbinarycrossentropy\nFlux.Losses.kldivergence\nFlux.Losses.poisson_loss\nFlux.Losses.hinge_loss\nFlux.Losses.squared_hinge_loss\nFlux.Losses.dice_coeff_loss\nFlux.Losses.tversky_loss\nFlux.Losses.binary_focal_loss\nFlux.Losses.focal_loss\nFlux.Losses.siamese_contrastive_loss","category":"page"},{"location":"models/losses/#Flux.Losses.mae","page":"Loss Functions ðŸ“š","title":"Flux.Losses.mae","text":"mae(yÌ‚, y; agg = mean)\n\nReturn the loss corresponding to mean absolute error:\n\nagg(abs.(yÌ‚ .- y))\n\nExample\n\njulia> y_model = [1.1, 1.9, 3.1];\n\njulia> Flux.mae(y_model, 1:3)\n0.10000000000000009\n\n\n\n\n\n","category":"function"},{"location":"models/losses/#Flux.Losses.mse","page":"Loss Functions ðŸ“š","title":"Flux.Losses.mse","text":"mse(yÌ‚, y; agg = mean)\n\nReturn the loss corresponding to mean square error:\n\nagg((yÌ‚ .- y) .^ 2)\n\nSee also: mae, msle, crossentropy.\n\nExample\n\njulia> y_model = [1.1, 1.9, 3.1];\n\njulia> y_true = 1:3;\n\njulia> Flux.mse(y_model, y_true)\n0.010000000000000018\n\n\n\n\n\n","category":"function"},{"location":"models/losses/#Flux.Losses.msle","page":"Loss Functions ðŸ“š","title":"Flux.Losses.msle","text":"msle(yÌ‚, y; agg = mean, Ïµ = eps(yÌ‚))\n\nThe loss corresponding to mean squared logarithmic errors, calculated as\n\nagg((log.(yÌ‚ .+ Ïµ) .- log.(y .+ Ïµ)) .^ 2)\n\nThe Ïµ term provides numerical stability. Penalizes an under-estimation more than an over-estimatation.\n\nExample\n\njulia> Flux.msle(Float32[1.1, 2.2, 3.3], 1:3)\n0.009084041f0\n\njulia> Flux.msle(Float32[0.9, 1.8, 2.7], 1:3)\n0.011100831f0\n\n\n\n\n\n","category":"function"},{"location":"models/losses/#Flux.Losses.huber_loss","page":"Loss Functions ðŸ“š","title":"Flux.Losses.huber_loss","text":"huber_loss(yÌ‚, y; Î´ = 1, agg = mean)\n\nReturn the mean of the Huber loss given the prediction yÌ‚ and true values y.\n\n             | 0.5 * |yÌ‚ - y|^2,            for |yÌ‚ - y| <= Î´\nHuber loss = |\n             |  Î´ * (|yÌ‚ - y| - 0.5 * Î´), otherwise\n\nExample\n\njulia> yÌ‚ = [1.1, 2.1, 3.1];\n\njulia> Flux.huber_loss(yÌ‚, 1:3)  # default Î´ = 1 > |yÌ‚ - y|\n0.005000000000000009\n\njulia> Flux.huber_loss(yÌ‚, 1:3, Î´=0.05)  # changes behaviour as |yÌ‚ - y| > Î´\n0.003750000000000005\n\n\n\n\n\n","category":"function"},{"location":"models/losses/#Flux.Losses.label_smoothing","page":"Loss Functions ðŸ“š","title":"Flux.Losses.label_smoothing","text":"label_smoothing(y::Union{Number, AbstractArray}, Î±; dims::Int=1)\n\nReturns smoothed labels, meaning the confidence on label values are relaxed.\n\nWhen y is given as one-hot vector or batch of one-hot, its calculated as\n\ny .* (1 - Î±) .+ Î± / size(y, dims)\n\nwhen y is given as a number or batch of numbers for binary classification, its calculated as\n\ny .* (1 - Î±) .+ Î± / 2\n\nin which case the labels are squeezed towards 0.5.\n\nÎ± is a number in interval (0, 1) called the smoothing factor. Higher the value of Î± larger the smoothing of y.\n\ndims denotes the one-hot dimension, unless dims=0 which denotes the application of label smoothing to binary distributions encoded in a single number.\n\nExample\n\njulia> y = Flux.onehotbatch([1, 1, 1, 0, 1, 0], 0:1)\n2Ã—6 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n â‹…  â‹…  â‹…  1  â‹…  1\n 1  1  1  â‹…  1  â‹…\n\njulia> y_smoothed = Flux.label_smoothing(y, 0.2f0)\n2Ã—6 Matrix{Float32}:\n 0.1  0.1  0.1  0.9  0.1  0.9\n 0.9  0.9  0.9  0.1  0.9  0.1\n\njulia> y_sim = softmax(y .* log(2f0))\n2Ã—6 Matrix{Float32}:\n 0.333333  0.333333  0.333333  0.666667  0.333333  0.666667\n 0.666667  0.666667  0.666667  0.333333  0.666667  0.333333\n\njulia> y_dis = vcat(y_sim[2,:]', y_sim[1,:]')\n2Ã—6 Matrix{Float32}:\n 0.666667  0.666667  0.666667  0.333333  0.666667  0.333333\n 0.333333  0.333333  0.333333  0.666667  0.333333  0.666667\n\njulia> Flux.crossentropy(y_sim, y) < Flux.crossentropy(y_sim, y_smoothed)\ntrue\n\njulia> Flux.crossentropy(y_dis, y) > Flux.crossentropy(y_dis, y_smoothed)\ntrue\n\n\n\n\n\n","category":"function"},{"location":"models/losses/#Flux.Losses.crossentropy","page":"Loss Functions ðŸ“š","title":"Flux.Losses.crossentropy","text":"crossentropy(yÌ‚, y; dims = 1, Ïµ = eps(yÌ‚), agg = mean)\n\nReturn the cross entropy between the given probability distributions; calculated as\n\nagg(-sum(y .* log.(yÌ‚ .+ Ïµ); dims))\n\nCross entropy is typically used as a loss in multi-class classification, in which case the labels y are given in a one-hot format. dims specifies the dimension (or the dimensions) containing the class probabilities. The prediction yÌ‚ is supposed to sum to one across dims, as would be the case with the output of a softmax operation.\n\nFor numerical stability, it is recommended to use logitcrossentropy rather than softmax followed by crossentropy .\n\nUse label_smoothing to smooth the true labels as preprocessing before computing the loss.\n\nSee also: logitcrossentropy, binarycrossentropy, logitbinarycrossentropy.\n\nExample\n\njulia> y_label = Flux.onehotbatch([0, 1, 2, 1, 0], 0:2)\n3Ã—5 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n 1  â‹…  â‹…  â‹…  1\n â‹…  1  â‹…  1  â‹…\n â‹…  â‹…  1  â‹…  â‹…\n\njulia> y_model = softmax(reshape(-7:7, 3, 5) .* 1f0)\n3Ã—5 Matrix{Float32}:\n 0.0900306  0.0900306  0.0900306  0.0900306  0.0900306\n 0.244728   0.244728   0.244728   0.244728   0.244728\n 0.665241   0.665241   0.665241   0.665241   0.665241\n\njulia> sum(y_model; dims=1)\n1Ã—5 Matrix{Float32}:\n 1.0  1.0  1.0  1.0  1.0\n\njulia> Flux.crossentropy(y_model, y_label)\n1.6076053f0\n\njulia> 5 * ans â‰ˆ Flux.crossentropy(y_model, y_label; agg=sum)\ntrue\n\njulia> y_smooth = Flux.label_smoothing(y_label, 0.15f0)\n3Ã—5 Matrix{Float32}:\n 0.9   0.05  0.05  0.05  0.9\n 0.05  0.9   0.05  0.9   0.05\n 0.05  0.05  0.9   0.05  0.05\n\njulia> Flux.crossentropy(y_model, y_smooth)\n1.5776052f0\n\n\n\n\n\n","category":"function"},{"location":"models/losses/#Flux.Losses.logitcrossentropy","page":"Loss Functions ðŸ“š","title":"Flux.Losses.logitcrossentropy","text":"logitcrossentropy(yÌ‚, y; dims = 1, agg = mean)\n\nReturn the cross entropy calculated by\n\nagg(-sum(y .* logsoftmax(yÌ‚; dims); dims))\n\nThis is mathematically equivalent to crossentropy(softmax(yÌ‚), y), but is more numerically stable than using functions crossentropy and softmax separately.\n\nSee also: binarycrossentropy, logitbinarycrossentropy, label_smoothing.\n\nExample\n\njulia> y_label = Flux.onehotbatch(collect(\"abcabaa\"), 'a':'c')\n3Ã—7 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n 1  â‹…  â‹…  1  â‹…  1  1\n â‹…  1  â‹…  â‹…  1  â‹…  â‹…\n â‹…  â‹…  1  â‹…  â‹…  â‹…  â‹…\n\njulia> y_model = reshape(vcat(-9:0, 0:9, 7.5f0), 3, 7)\n3Ã—7 Matrix{Float32}:\n -9.0  -6.0  -3.0  0.0  2.0  5.0  8.0\n -8.0  -5.0  -2.0  0.0  3.0  6.0  9.0\n -7.0  -4.0  -1.0  1.0  4.0  7.0  7.5\n\njulia> Flux.logitcrossentropy(y_model, y_label)\n1.5791205f0\n\njulia> Flux.crossentropy(softmax(y_model), y_label)\n1.5791197f0\n\n\n\n\n\n","category":"function"},{"location":"models/losses/#Flux.Losses.binarycrossentropy","page":"Loss Functions ðŸ“š","title":"Flux.Losses.binarycrossentropy","text":"binarycrossentropy(yÌ‚, y; agg = mean, Ïµ = eps(yÌ‚))\n\nReturn the binary cross-entropy loss, computed as\n\nagg(@.(-y * log(yÌ‚ + Ïµ) - (1 - y) * log(1 - yÌ‚ + Ïµ)))\n\nWhere typically, the prediction yÌ‚ is given by the output of a sigmoid activation. The Ïµ term is included to avoid infinity. Using logitbinarycrossentropy is recomended over binarycrossentropy for numerical stability.\n\nUse label_smoothing to smooth the y value as preprocessing before computing the loss.\n\nSee also: crossentropy, logitcrossentropy.\n\nExamples\n\njulia> y_bin = Bool[1,0,1]\n3-element Vector{Bool}:\n 1\n 0\n 1\n\njulia> y_prob = softmax(reshape(vcat(1:3, 3:5), 2, 3) .* 1f0)\n2Ã—3 Matrix{Float32}:\n 0.268941  0.5  0.268941\n 0.731059  0.5  0.731059\n\njulia> Flux.binarycrossentropy(y_prob[2,:], y_bin)\n0.43989f0\n\njulia> all(p -> 0 < p < 1, y_prob[2,:])  # else DomainError\ntrue\n\njulia> y_hot = Flux.onehotbatch(y_bin, 0:1)\n2Ã—3 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n â‹…  1  â‹…\n 1  â‹…  1\n\njulia> Flux.crossentropy(y_prob, y_hot)\n0.43989f0\n\n\n\n\n\n","category":"function"},{"location":"models/losses/#Flux.Losses.logitbinarycrossentropy","page":"Loss Functions ðŸ“š","title":"Flux.Losses.logitbinarycrossentropy","text":"logitbinarycrossentropy(Å·, y; agg = mean)\n\nMathematically equivalent to binarycrossentropy(Ïƒ(Å·), y) but is more numerically stable.\n\nSee also: crossentropy, logitcrossentropy.\n\nExamples\n\njulia> y_bin = Bool[1,0,1];\n\njulia> y_model = Float32[2, -1, pi]\n3-element Vector{Float32}:\n  2.0\n -1.0\n  3.1415927\n\njulia> Flux.logitbinarycrossentropy(y_model, y_bin)\n0.160832f0\n\njulia> Flux.binarycrossentropy(sigmoid.(y_model), y_bin)\n0.16083185f0\n\n\n\n\n\n","category":"function"},{"location":"models/losses/#Flux.Losses.kldivergence","page":"Loss Functions ðŸ“š","title":"Flux.Losses.kldivergence","text":"kldivergence(yÌ‚, y; agg = mean, Ïµ = eps(yÌ‚))\n\nReturn the Kullback-Leibler divergence between the given probability distributions.\n\nThe KL divergence is a measure of how much one probability distribution is different from the other. It is always non-negative, and zero only when both the distributions are equal.\n\nExample\n\njulia> p1 = [1 0; 0 1]\n2Ã—2 Matrix{Int64}:\n 1  0\n 0  1\n\njulia> p2 = fill(0.5, 2, 2)\n2Ã—2 Matrix{Float64}:\n 0.5  0.5\n 0.5  0.5\n\njulia> Flux.kldivergence(p2, p1) â‰ˆ log(2)\ntrue\n\njulia> Flux.kldivergence(p2, p1; agg = sum) â‰ˆ 2log(2)\ntrue\n\njulia> Flux.kldivergence(p2, p2; Ïµ = 0)  # about -2e-16 with the regulator\n0.0\n\njulia> Flux.kldivergence(p1, p2; Ïµ = 0)  # about 17.3 with the regulator\nInf\n\n\n\n\n\n","category":"function"},{"location":"models/losses/#Flux.Losses.poisson_loss","page":"Loss Functions ðŸ“š","title":"Flux.Losses.poisson_loss","text":"poisson_loss(yÌ‚, y; agg = mean)\n\nReturn how much the predicted distribution yÌ‚ diverges from the expected Poisson distribution y; calculated as -\n\nsum(yÌ‚ .- y .* log.(yÌ‚)) / size(y, 2)\n\nMore information..\n\nExample\n\njulia> y_model = [1, 3, 3];  # data should only take integral values\n\njulia> Flux.poisson_loss(y_model, 1:3)\n0.5023128522198171\n\n\n\n\n\n","category":"function"},{"location":"models/losses/#Flux.Losses.hinge_loss","page":"Loss Functions ðŸ“š","title":"Flux.Losses.hinge_loss","text":"hinge_loss(yÌ‚, y; agg = mean)\n\nReturn the hinge_loss given the prediction yÌ‚ and true labels y (containing 1 or -1); calculated as\n\nsum(max.(0, 1 .- yÌ‚ .* y)) / size(y, 2)\n\nUsually used with classifiers like Support Vector Machines. See also: squared_hinge_loss\n\nExample\n\njulia> y_true = [1, -1, 1, 1];\n\njulia> y_pred = [0.1, 0.3, 1, 1.5];\n\njulia> Flux.hinge_loss(y_pred, y_true)\n0.55\n\njulia> Flux.hinge_loss(y_pred[1], y_true[1]) != 0  # same sign but |yÌ‚| < 1\ntrue\n\njulia> Flux.hinge_loss(y_pred[end], y_true[end]) == 0  # same sign but |yÌ‚| >= 1\ntrue\n\njulia> Flux.hinge_loss(y_pred[2], y_true[2]) != 0 # opposite signs\ntrue\n\n\n\n\n\n","category":"function"},{"location":"models/losses/#Flux.Losses.squared_hinge_loss","page":"Loss Functions ðŸ“š","title":"Flux.Losses.squared_hinge_loss","text":"squared_hinge_loss(yÌ‚, y)\n\nReturn the squared hinge_loss loss given the prediction yÌ‚ and true labels y (containing 1 or -1); calculated as\n\nsum((max.(0, 1 .- yÌ‚ .* y)).^2) / size(y, 2)\n\nUsually used with classifiers like Support Vector Machines. See also: hinge_loss\n\nExample\n\njulia> y_true = [1, -1, 1, 1];\n\njulia> y_pred = [0.1, 0.3, 1, 1.5];\n\njulia> Flux.squared_hinge_loss(y_pred, y_true)\n0.625\n\njulia> Flux.squared_hinge_loss(y_pred[1], y_true[1]) != 0\ntrue\n\njulia> Flux.squared_hinge_loss(y_pred[end], y_true[end]) == 0\ntrue\n\njulia> Flux.squared_hinge_loss(y_pred[2], y_true[2]) != 0\ntrue\n\n\n\n\n\n","category":"function"},{"location":"models/losses/#Flux.Losses.dice_coeff_loss","page":"Loss Functions ðŸ“š","title":"Flux.Losses.dice_coeff_loss","text":"dice_coeff_loss(yÌ‚, y; smooth = 1)\n\nReturn a loss based on the dice coefficient. Used in the V-Net image segmentation architecture. The dice coefficient is similar to the F1_score. Loss calculated as:\n\n1 - 2*sum(|yÌ‚ .* y| + smooth) / (sum(yÌ‚.^2) + sum(y.^2) + smooth)\n\nExample\n\njulia> y_pred = [1.1, 2.1, 3.1];\n\njulia> Flux.dice_coeff_loss(y_pred, 1:3)\n0.000992391663909964\n\njulia> 1 - Flux.dice_coeff_loss(y_pred, 1:3)  # ~ F1 score for image segmentation\n0.99900760833609\n\n\n\n\n\n","category":"function"},{"location":"models/losses/#Flux.Losses.tversky_loss","page":"Loss Functions ðŸ“š","title":"Flux.Losses.tversky_loss","text":"tversky_loss(yÌ‚, y; Î² = 0.7)\n\nReturn the Tversky loss. Used with imbalanced data to give more weight to false negatives. Larger Î² weigh recall more than precision (by placing more emphasis on false negatives). Calculated as:\n\n1 - sum(|y .* yÌ‚| + 1) / (sum(y .* yÌ‚ + (1 - Î²)*(1 .- y) .* yÌ‚ + Î²*y .* (1 .- yÌ‚)) + 1)\n\n\n\n\n\n","category":"function"},{"location":"models/losses/#Flux.Losses.binary_focal_loss","page":"Loss Functions ðŸ“š","title":"Flux.Losses.binary_focal_loss","text":"binary_focal_loss(yÌ‚, y; agg=mean, Î³=2, Ïµ=eps(yÌ‚))\n\nReturn the binaryfocalloss The input, 'yÌ‚', is expected to be normalized (i.e. softmax output).\n\nFor Î³ == 0, the loss is mathematically equivalent to Losses.binarycrossentropy.\n\nSee also: Losses.focal_loss for multi-class setting\n\nExample\n\njulia> y = [0  1  0\n            1  0  1]\n2Ã—3 Matrix{Int64}:\n 0  1  0\n 1  0  1\n\njulia> Å· = [0.268941  0.5  0.268941\n            0.731059  0.5  0.731059]\n2Ã—3 Matrix{Float64}:\n 0.268941  0.5  0.268941\n 0.731059  0.5  0.731059\n\njulia> Flux.binary_focal_loss(Å·, y) â‰ˆ 0.0728675615927385\ntrue\n\n\n\n\n\n","category":"function"},{"location":"models/losses/#Flux.Losses.focal_loss","page":"Loss Functions ðŸ“š","title":"Flux.Losses.focal_loss","text":"focal_loss(yÌ‚, y; dims=1, agg=mean, Î³=2, Ïµ=eps(yÌ‚))\n\nReturn the focal_loss which can be used in classification tasks with highly imbalanced classes. It down-weights well-classified examples and focuses on hard examples. The input, 'yÌ‚', is expected to be normalized (i.e. softmax output).\n\nThe modulating factor, Î³, controls the down-weighting strength. For Î³ == 0, the loss is mathematically equivalent to Losses.crossentropy.\n\nExample\n\njulia> y = [1  0  0  0  1\n            0  1  0  1  0\n            0  0  1  0  0]\n3Ã—5 Matrix{Int64}:\n 1  0  0  0  1\n 0  1  0  1  0\n 0  0  1  0  0\n\njulia> Å· = softmax(reshape(-7:7, 3, 5) .* 1f0)\n3Ã—5 Matrix{Float32}:\n 0.0900306  0.0900306  0.0900306  0.0900306  0.0900306\n 0.244728   0.244728   0.244728   0.244728   0.244728\n 0.665241   0.665241   0.665241   0.665241   0.665241\n\njulia> Flux.focal_loss(Å·, y) â‰ˆ 1.1277571935622628\ntrue\n\nSee also: Losses.binary_focal_loss for binary (not one-hot) labels\n\n\n\n\n\n","category":"function"},{"location":"models/losses/#Flux.Losses.siamese_contrastive_loss","page":"Loss Functions ðŸ“š","title":"Flux.Losses.siamese_contrastive_loss","text":"siamese_contrastive_loss(yÌ‚, y; margin = 1, agg = mean)\n\nReturn the contrastive loss which can be useful for training Siamese Networks. It is given by\n\nagg(@. (1 - y) * Å·^2 + y * max(0, margin - Å·)^2)\n\nSpecify margin to set the baseline for distance at which pairs are dissimilar.\n\nExample\n\njulia> yÌ‚ = [0.5, 1.5, 2.5];\n\njulia> Flux.siamese_contrastive_loss(yÌ‚, 1:3)\n-4.833333333333333\n\njulia> Flux.siamese_contrastive_loss(yÌ‚, 1:3, margin = 2)\n-4.0\n\n\n\n\n\n","category":"function"},{"location":"models/recurrence/#Recurrent-Models","page":"Recurrence","title":"Recurrent Models","text":"","category":"section"},{"location":"models/recurrence/#Recurrent-cells","page":"Recurrence","title":"Recurrent cells","text":"","category":"section"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"To introduce Flux's recurrence functionalities, we will consider the following vanilla recurrent neural network structure:","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"(Image: )","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"In the above, we have a sequence of length 3, where x1 to x3 represent the input at each step (could be a timestamp or a word in a sentence), and y1 to y3 are their respective outputs.","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"An aspect to recognize is that in such a model, the recurrent cells A all refer to the same structure. What distinguishes it from a simple dense layer is that the cell A is fed, in addition to an input x, with information from the previous state of the model (hidden state denoted as h1 & h2 in the diagram).","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"In the most basic RNN case, cell A could be defined by the following: ","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"output_size = 5\ninput_size = 2\nWxh = randn(Float32, output_size, input_size)\nWhh = randn(Float32, output_size, output_size)\nb   = randn(Float32, output_size)\n\nfunction rnn_cell(h, x)\n    h = tanh.(Wxh * x .+ Whh * h .+ b)\n    return h, h\nend\n\nx = rand(Float32, input_size) # dummy input data\nh = rand(Float32, output_size) # random initial hidden state\n\nh, y = rnn_cell(h, x)","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"Notice how the above is essentially a Dense layer that acts on two inputs, h and x.","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"If you run the last line a few times, you'll notice the output y changing slightly even though the input x is the same.","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"There are various recurrent cells available in Flux, notably RNNCell, LSTMCell and GRUCell, which are documented in the layer reference. The hand-written example above can be replaced with:","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"using Flux\n\nrnn = Flux.RNNCell(2, 5)\n\nx = rand(Float32, 2) # dummy data\nh = rand(Float32, 5)  # initial hidden state\n\nh, y = rnn(h, x)","category":"page"},{"location":"models/recurrence/#Stateful-Models","page":"Recurrence","title":"Stateful Models","text":"","category":"section"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"For the most part, we don't want to manage hidden states ourselves, but to treat our models as being stateful. Flux provides the Recur wrapper to do this.","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"x = rand(Float32, 2)\nh = rand(Float32, 5)\n\nm = Flux.Recur(rnn, h)\n\ny = m(x)","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"The Recur wrapper stores the state between runs in the m.state field.","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"If we use the RNN(2, 5) constructor â€“ as opposed to RNNCell â€“ you'll see that it's simply a wrapped cell.","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"julia> using Flux\n\njulia> RNN(2, 5)  # or equivalently RNN(2 => 5)\nRecur(\n  RNNCell(2 => 5, tanh),                # 45 parameters\n)         # Total: 4 trainable arrays, 45 parameters,\n          # plus 1 non-trainable, 5 parameters, summarysize 412 bytes.","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"Equivalent to the RNN stateful constructor, LSTM and GRU are also available. ","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"Using these tools, we can now build the model shown in the above diagram with: ","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"julia> m = Chain(RNN(2 => 5), Dense(5 => 1))\nChain(\n  Recur(\n    RNNCell(2 => 5, tanh),              # 45 parameters\n  ),\n  Dense(5 => 1),                        # 6 parameters\n)         # Total: 6 trainable arrays, 51 parameters,\n          # plus 1 non-trainable, 5 parameters, summarysize 580 bytes.   ","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"In this example, each output has only one component.","category":"page"},{"location":"models/recurrence/#Working-with-sequences","page":"Recurrence","title":"Working with sequences","text":"","category":"section"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"Using the previously defined m recurrent model, we can now apply it to a single step from our sequence:","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"julia> x = rand(Float32, 2);\n\njulia> m(x)\n1-element Vector{Float32}:\n 0.45860028","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"The m(x) operation would be represented by x1 -> A -> y1 in our diagram. If we perform this operation a second time, it will be equivalent to x2 -> A -> y2  since the model m has stored the state resulting from the x1 step.","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"Now, instead of computing a single step at a time, we can get the full y1 to y3 sequence in a single pass by  iterating the model on a sequence of data. ","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"To do so, we'll need to structure the input data as a Vector of observations at each time step. This Vector will therefore be of length = seq_length and each of its elements will represent the input features for a given step. In our example, this translates into a Vector of length 3, where each element is a Matrix of size (features, batch_size), or just a Vector of length features if dealing with a single observation.  ","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"julia> x = [rand(Float32, 2) for i = 1:3];\n\njulia> [m(xi) for xi in x]\n3-element Vector{Vector{Float32}}:\n [0.36080405]\n [-0.13914406]\n [0.9310162]","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"warning: Use of map and broadcast\nMapping and broadcasting operations with stateful layers such are discouraged, since the julia language doesn't guarantee a specific execution order. Therefore, avoid  y = m.(x)\n# or \ny = map(m, x)and use explicit loops y = [m(x) for x in x]","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"If for some reason one wants to exclude the first step of the RNN chain for the computation of the loss, that can be handled with:","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"using Flux.Losses: mse\n\nfunction loss(x, y)\n  m(x[1]) # ignores the output but updates the hidden states\n  sum(mse(m(xi), yi) for (xi, yi) in zip(x[2:end], y))\nend\n\ny = [rand(Float32, 1) for i=1:2]\nloss(x, y)","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"In such a model, only the last two outputs are used to compute the loss, hence the target y being of length 2. This is a strategy that can be used to easily handle a seq-to-one kind of structure, compared to the seq-to-seq assumed so far.   ","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"Alternatively, if one wants to perform some warmup of the sequence, it could be performed once, followed with a regular training where all the steps of the sequence would be considered for the gradient update:","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"function loss(x, y)\n  sum(mse(m(xi), yi) for (xi, yi) in zip(x, y))\nend\n\nseq_init = [rand(Float32, 2)]\nseq_1 = [rand(Float32, 2) for i = 1:3]\nseq_2 = [rand(Float32, 2) for i = 1:3]\n\ny1 = [rand(Float32, 1) for i = 1:3]\ny2 = [rand(Float32, 1) for i = 1:3]\n\nX = [seq_1, seq_2]\nY = [y1, y2]\ndata = zip(X,Y)\n\nFlux.reset!(m)\n[m(x) for x in seq_init]\n\nps = Flux.params(m)\nopt= Adam(1e-3)\nFlux.train!(loss, ps, data, opt)","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"In this previous example, model's state is first reset with Flux.reset!. Then, there's a warmup that is performed over a sequence of length 1 by feeding it with seq_init, resulting in a warmup state. The model can then be trained for 1 epoch, where 2 batches are provided (seq_1 and seq_2) and all the timesteps outputs are considered for the loss.","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"In this scenario, it is important to note that a single continuous sequence is considered. Since the model state is not reset between the 2 batches, the state of the model flows through the batches, which only makes sense in the context where seq_1 is the continuation of seq_init and so on.","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"Batch size would be 1 here as there's only a single sequence within each batch. If the model was to be trained on multiple independent sequences, then these sequences could be added to the input data as a second dimension. For example, in a language model, each batch would contain multiple independent sentences. In such scenario, if we set the batch size to 4, a single batch would be of the shape:","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"x = [rand(Float32, 2, 4) for i = 1:3]\ny = [rand(Float32, 1, 4) for i = 1:3]","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"That would mean that we have 4 sentences (or samples), each with 2 features (let's say a very small embedding!) and each with a length of 3 (3 words per sentence). Computing m(batch[1]), would still represent x1 -> y1 in our diagram and returns the first word output, but now for each of the 4 independent sentences (second dimension of the input matrix). We do not need to use Flux.reset!(m) here; each sentence in the batch will output in its own \"column\", and the outputs of the different sentences won't mix. ","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"To illustrate, we go through an example of batching with our implementation of rnn_cell. The implementation doesn't need to change; the batching comes for \"free\" from the way Julia does broadcasting and the rules of matrix multiplication.","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"output_size = 5\ninput_size = 2\nWxh = randn(Float32, output_size, input_size)\nWhh = randn(Float32, output_size, output_size)\nb   = randn(Float32, output_size)\n\nfunction rnn_cell(h, x)\n    h = tanh.(Wxh * x .+ Whh * h .+ b)\n    return h, h\nend","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"Here, we use the last dimension of the input and the hidden state as the batch dimension. I.e., h[:, n] would be the hidden state of the nth sentence in the batch.","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"batch_size = 4\nx = rand(Float32, input_size, batch_size) # dummy input data\nh = rand(Float32, output_size, batch_size) # random initial hidden state\n\nh, y = rnn_cell(h, x)","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"julia> size(h) == size(y) == (output_size, batch_size)\ntrue","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"In many situations, such as when dealing with a language model, the sentences in each batch are independent (i.e. the last item of the first sentence of the first batch is independent from the first item of the first sentence of the second batch), so we cannot handle the model as if each batch was the direct continuation of the previous one. To handle such situations, we need to reset the state of the model between each batch, which can be conveniently performed within the loss function:","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"function loss(x, y)\n  Flux.reset!(m)\n  sum(mse(m(xi), yi) for (xi, yi) in zip(x, y))\nend","category":"page"},{"location":"models/recurrence/","page":"Recurrence","title":"Recurrence","text":"A potential source of ambiguity with RNN in Flux can come from the different data layout compared to some common frameworks where data is typically a 3 dimensional array: (features, seq length, samples). In Flux, those 3 dimensions are provided through a vector of seq length containing a matrix (features, samples).","category":"page"},{"location":"training/callbacks/#Callback-Helpers","page":"Callback Helpers ðŸ“š","title":"Callback Helpers","text":"","category":"section"},{"location":"training/callbacks/","page":"Callback Helpers ðŸ“š","title":"Callback Helpers ðŸ“š","text":"Flux.throttle\nFlux.stop\nFlux.skip","category":"page"},{"location":"training/callbacks/#Flux.throttle","page":"Callback Helpers ðŸ“š","title":"Flux.throttle","text":"throttle(f, timeout; leading=true, trailing=false)\n\nReturn a function that when invoked, will only be triggered at most once during timeout seconds.\n\nNormally, the throttled function will run as much as it can, without ever going more than once per wait duration; but if you'd like to disable the execution on the leading edge, pass leading=false. To enable execution on the trailing edge, pass trailing=true.\n\nExamples\n\njulia> a = Flux.throttle(() -> println(\"Flux\"), 2);\n\njulia> for i = 1:4  # a called in alternate iterations\n           a()\n           sleep(1)\n       end\nFlux\nFlux\n\n\n\n\n\n","category":"function"},{"location":"training/callbacks/#Flux.Optimise.stop","page":"Callback Helpers ðŸ“š","title":"Flux.Optimise.stop","text":"stop()\n\nCall Flux.stop() in a callback to indicate when a callback condition is met. This will trigger the train loop to stop and exit.\n\nnote: Note\nFlux.stop() will be removed from Flux 0.14. It should be replaced with break in an ordinary for loop.\n\nExamples\n\ncb = function ()\n  accuracy() > 0.9 && Flux.stop()\nend\n\n\n\n\n\n","category":"function"},{"location":"training/callbacks/#Flux.Optimise.skip","page":"Callback Helpers ðŸ“š","title":"Flux.Optimise.skip","text":"skip()\n\nCall Flux.skip() in a callback to indicate when a callback condition is met. This will trigger the train loop to skip the current data point and not update with the calculated gradient.\n\nnote: Note\nFlux.skip() will be removed from Flux 0.14\n\nExamples\n\ncb = function ()\n  loss() > 1e7 && Flux.skip()\nend\n\n\n\n\n\n","category":"function"},{"location":"training/callbacks/#Patience-Helpers","page":"Callback Helpers ðŸ“š","title":"Patience Helpers","text":"","category":"section"},{"location":"training/callbacks/","page":"Callback Helpers ðŸ“š","title":"Callback Helpers ðŸ“š","text":"Flux provides utilities for controlling your training procedure according to some monitored condition and a maximum patience. For example, you can use early_stopping to stop training when the model is converging or deteriorating, or you can use plateau to check if the model is stagnating.","category":"page"},{"location":"training/callbacks/","page":"Callback Helpers ðŸ“š","title":"Callback Helpers ðŸ“š","text":"For example, below we create a pseudo-loss function that decreases, bottoms out, and then increases. The early stopping trigger will break the loop before the loss increases too much.","category":"page"},{"location":"training/callbacks/","page":"Callback Helpers ðŸ“š","title":"Callback Helpers ðŸ“š","text":"# create a pseudo-loss that decreases for 4 calls, then starts increasing\n# we call this like loss()\nloss = let t = 0\n  () -> begin\n    t += 1\n    (t - 4) ^ 2\n  end\nend\n\n# create an early stopping trigger\n# returns true when the loss increases for two consecutive steps\nes = early_stopping(loss, 2; init_score = 9)\n\n# this will stop at the 6th (4 decreasing + 2 increasing calls) epoch\n@epochs 10 begin\n  es() && break\nend","category":"page"},{"location":"training/callbacks/","page":"Callback Helpers ðŸ“š","title":"Callback Helpers ðŸ“š","text":"The keyword argument distance of early_stopping is a function of the form distance(best_score, score). By default distance is -, which implies that the monitored metric f is expected to be decreasing and minimized. If you use some increasing metric (e.g. accuracy), you can customize the distance function: (best_score, score) -> score - best_score.","category":"page"},{"location":"training/callbacks/","page":"Callback Helpers ðŸ“š","title":"Callback Helpers ðŸ“š","text":"# create a pseudo-accuracy that increases by 0.01 each time from 0 to 1\n# we call this like acc()\nacc = let v = 0\n  () -> v = max(1, v + 0.01)\nend\n\n# create an early stopping trigger for accuracy\nes = early_stopping(acc, 3; delta = (best_score, score) -> score - best_score)\n\n# this will iterate until the 10th epoch\n@epochs 10 begin\n  es() && break\nend","category":"page"},{"location":"training/callbacks/","page":"Callback Helpers ðŸ“š","title":"Callback Helpers ðŸ“š","text":"early_stopping and plateau are both built on top of patience. You can use patience to build your own triggers that use a patient counter. For example, if you want to trigger when the loss is below a threshold for several consecutive iterations:","category":"page"},{"location":"training/callbacks/","page":"Callback Helpers ðŸ“š","title":"Callback Helpers ðŸ“š","text":"threshold(f, thresh, delay) = patience(delay) do\n  f() < thresh\nend","category":"page"},{"location":"training/callbacks/","page":"Callback Helpers ðŸ“š","title":"Callback Helpers ðŸ“š","text":"Both predicate in patience and f in early_stopping / plateau can accept extra arguments. You can pass such extra arguments to predicate or f through the returned function:","category":"page"},{"location":"training/callbacks/","page":"Callback Helpers ðŸ“š","title":"Callback Helpers ðŸ“š","text":"trigger = patience((a; b) -> a > b, 3)\n\n# this will iterate until the 10th epoch\n@epochs 10 begin\n  trigger(1; b = 2) && break\nend\n\n# this will stop at the 3rd epoch\n@epochs 10 begin\n  trigger(3; b = 2) && break\nend","category":"page"},{"location":"training/callbacks/","page":"Callback Helpers ðŸ“š","title":"Callback Helpers ðŸ“š","text":"Flux.patience\nFlux.early_stopping\nFlux.plateau","category":"page"},{"location":"training/callbacks/#Flux.patience","page":"Callback Helpers ðŸ“š","title":"Flux.patience","text":"patience(predicate, wait)\n\nReturn a function that internally counts by one when predicate(...) == true, otherwise the count is reset to zero. If the count is greater than or equal to wait, the function returns true, otherwise it returns false.\n\nExamples\n\njulia> loss() = rand();\n\njulia> trigger = Flux.patience(() -> loss() < 1, 3);\n\n\njulia> for i in 1:10\n         @info \"Epoch $i\"\n         trigger() && break\n       end\n[ Info: Epoch 1\n[ Info: Epoch 2\n[ Info: Epoch 3\n\n\n\n\n\n","category":"function"},{"location":"training/callbacks/#Flux.early_stopping","page":"Callback Helpers ðŸ“š","title":"Flux.early_stopping","text":"early_stopping(f, delay; distance = -, init_score = 0, min_dist = 0)\n\nReturn a function that internally counts by one when distance(best_score, f(...)) <= min_dist, where best_score is the last seen best value of f(...). If the count is greater than or equal to delay, the function returns true, otherwise it returns false. The count is reset when distance(best_score, f(...)) > min_dist.\n\nExamples\n\njulia> loss = let l = 0\n         () -> l += 1\n       end; # pseudo loss function that returns increasing values\n\njulia> es = Flux.early_stopping(loss, 3);\n\n\njulia> for i in 1:10\n         @info \"Epoch $i\"\n         es() && break\n       end\n[ Info: Epoch 1\n[ Info: Epoch 2\n[ Info: Epoch 3\n\n\n\n\n\n","category":"function"},{"location":"training/callbacks/#Flux.plateau","page":"Callback Helpers ðŸ“š","title":"Flux.plateau","text":"plateau(f, width; distance = -, init_score = 0, min_dist = 1f-6)\n\nReturn a function that internally counts by one when abs(distance(last_score, f(...))) <= min_dist, where last_score holds the last value of f(...). If the count is greater than or equal to width, the function returns true, otherwise it returns false. The count is reset when abs(distance(last_score, f(...))) > min_dist.\n\nExamples\n\njulia> f = let v = 10\n         () -> v = v / abs(v) - v\n       end; # -9, 8, -7, 6, ...\n\njulia> trigger = Flux.plateau(f, 3; init_score=10, min_dist=18);\n\n\njulia> for i in 1:10\n         @info \"Epoch $i\"\n         trigger() && break\n       end\n[ Info: Epoch 1\n[ Info: Epoch 2\n[ Info: Epoch 3\n[ Info: Epoch 4\n\n\n\n\n\n","category":"function"},{"location":"models/activation/#man-activations","page":"Activation Functions ðŸ“š","title":"Activation Functions from NNlib.jl","text":"","category":"section"},{"location":"models/activation/","page":"Activation Functions ðŸ“š","title":"Activation Functions ðŸ“š","text":"These non-linearities used between layers of your model are exported by the NNlib package.","category":"page"},{"location":"models/activation/","page":"Activation Functions ðŸ“š","title":"Activation Functions ðŸ“š","text":"Note that, unless otherwise stated, activation functions operate on scalars. To apply them to an array you can call Ïƒ.(xs), relu.(xs) and so on. Alternatively, they can be passed to a layer like Dense(784 => 1024, relu) which will handle this broadcasting.","category":"page"},{"location":"models/activation/","page":"Activation Functions ðŸ“š","title":"Activation Functions ðŸ“š","text":"Functions like softmax are sometimes described as activation functions, but not by Flux. They must see all the outputs, and hence cannot be broadcasted. See the next page for details.","category":"page"},{"location":"models/activation/#Alphabetical-Listing","page":"Activation Functions ðŸ“š","title":"Alphabetical Listing","text":"","category":"section"},{"location":"models/activation/","page":"Activation Functions ðŸ“š","title":"Activation Functions ðŸ“š","text":"celu\nelu\ngelu\nhardsigmoid\nsigmoid_fast\nhardtanh\ntanh_fast\nleakyrelu\nlisht\nlogcosh\nlogsigmoid\nmish\nrelu\nrelu6\nrrelu\nselu\nsigmoid\nsoftplus\nsoftshrink\nsoftsign\nswish\nhardswish\ntanhshrink\ntrelu","category":"page"},{"location":"models/activation/#NNlib.celu","page":"Activation Functions ðŸ“š","title":"NNlib.celu","text":"celu(x, Î±=1) = x â‰¥ 0 ? x : Î± * (exp(x/Î±) - 1)\n\nActivation function from \"Continuously Differentiable Exponential Linear Units\".\n\njulia> lineplot(celu, -2, 2, height=7)\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        \n         2 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â ¤â ’â ‰â”‚ celu(x)\n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â¢€â¡ â ”â Šâ ‰â €â €â €â €â”‚        \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â£€â¡¤â –â Šâ â €â €â €â €â €â €â €â €â”‚        \n   f(x)    â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â£€â ¤â –â ‹â â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n           â”‚â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â¢¤â¡¤â¡§â ¶â ­â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â”‚        \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â¢€â£€â£€â ¤â ”â ’â ‹â â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n        -1 â”‚â ¤â ¤â ¤â ¤â ”â ’â ’â ’â Šâ ‰â ‰â â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        \n           â €-2â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €2â €        \n           â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €        \n\njulia> celu(-10f0)\n-0.9999546f0\n\n\n\n\n\n","category":"function"},{"location":"models/activation/#NNlib.elu","page":"Activation Functions ðŸ“š","title":"NNlib.elu","text":"elu(x, Î±=1) = x > 0 ? x : Î± * (exp(x) - 1)\n\nExponential Linear Unit activation function. See \"Fast and Accurate Deep Network Learning by Exponential Linear Units\". You can also specify the coefficient explicitly, e.g. elu(x, 1).\n\njulia> lineplot(elu, -2, 2, height=7)\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       \n         2 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â ¤â ’â ‰â”‚ elu(x)\n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â¢€â¡ â ”â Šâ ‰â €â €â €â €â”‚       \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â£€â¡¤â –â Šâ â €â €â €â €â €â €â €â €â”‚       \n   f(x)    â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â£€â ¤â –â ‹â â €â €â €â €â €â €â €â €â €â €â €â €â”‚       \n           â”‚â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â¢¤â¡¤â¡§â ¶â ­â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â”‚       \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â¢€â£€â£€â ¤â ”â ’â ‹â â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚       \n        -1 â”‚â ¤â ¤â ¤â ¤â ”â ’â ’â ’â Šâ ‰â ‰â â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚       \n           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       \n           â €-2â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €2â €       \n           â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €       \n\njulia> elu(-10f0)\n-0.9999546f0\n\njulia> elu(-10f0, 2)\n-1.9999092f0\n\n\n\n\n\n","category":"function"},{"location":"models/activation/#NNlib.gelu","page":"Activation Functions ðŸ“š","title":"NNlib.gelu","text":"gelu(x) = 0.5x * (1 + tanh(âˆš(2/Ï€) * (x + 0.044715x^3)))\n\nActivation function from \"Gaussian Error Linear Units\".\n\njulia> lineplot(gelu, -2, 2, height=7)\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        \n         2 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢€â¡ â ”â Šâ”‚ gelu(x)\n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â£€â ”â Šâ â €â €â €â”‚        \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â£€â ¤â ’â ‰â €â €â €â €â €â €â €â”‚        \n   f(x)    â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â£€â¡ â ¤â ’â ‰â €â €â €â €â €â €â €â €â €â €â €â”‚        \n           â”‚â£¤â£¤â£¤â£¤â£¤â£¤â£¤â£¤â¡¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â£¤â£¤â£¤â¡¤â¡§â ¶â ¶â ­â ¥â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â”‚        \n           â”‚â €â €â €â €â €â €â €â €â ˆâ ‰â ‰â ‰â ‰â ‰â ‰â ‰â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n        -1 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        \n           â €-2â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €2â €        \n           â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €        \n\njulia> lineplot(gelu, -5, 0, height=7);\n\njulia> lineplot!(ans, swish)\n             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         \n           0 â”‚â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ’â ’â ¤â£„â¡€â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢¸â”‚ gelu(x) \n             â”‚â ‘â ’â ¢â ¤â£„â¡€â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â ‰â “â¢„â¡€â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â”‚ swish(x)\n             â”‚â €â €â €â €â €â ˆâ ‰â ’â ¤â£€â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â ‘â¢†â¡€â €â €â €â €â €â €â €â €â €â €â£¸â â”‚         \n   f(x)      â”‚â €â €â €â €â €â €â €â €â €â €â ‰â ’â¢„â¡€â €â €â €â €â €â €â €â €â €â €â €â €â €â ‘â¢„â €â €â €â €â €â €â €â €â¢ â¡‡â €â”‚         \n             â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â ‰â “â¢„â¡€â €â €â €â €â €â €â €â €â €â €â €â €â “â£„â €â €â €â €â €â¢ â¡žâ €â €â”‚         \n             â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â ‰â ¦â¡€â €â €â €â €â €â €â €â €â €â €â €â €â “â¢„â£€â£€â¡¤â¢£â ƒâ €â €â”‚         \n        -0.2 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â ˆâ “â£„â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢ â ‡â €â €â €â”‚         \n             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         \n             â €-5â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €0â €         \n             â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €         \n\n\n\n\n\n","category":"function"},{"location":"models/activation/#NNlib.hardsigmoid","page":"Activation Functions ðŸ“š","title":"NNlib.hardsigmoid","text":"hardÏƒ(x) = max(0, min(1, (x + 3) / 6))\n\nPiecewise linear approximation of sigmoid.\n\njulia> lineplot(hardsigmoid, -5, 5, height=7)\n          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         \n        1 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â¢€â¡ â –â ‹â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â”‚ hardÏƒ(x)\n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â£€â¡¤â ’â â €â €â €â €â €â €â €â €â €â €â €â”‚         \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â¡ â ”â ‹â â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚         \n   f(x)   â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â¡¤â¡—â ‰â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚         \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â ”â Šâ â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚         \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â¢€â¡¤â –â ‹â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚         \n        0 â”‚â£€â£€â£€â£€â£€â£€â£€â£€â£€â ¤â Šâ â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚         \n          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         \n          â €-5â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €5â €         \n          â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €         \n\njulia> lineplot(sigmoid, -5, 5, height=7)\n          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     \n        1 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â£€â¡ â ¤â –â ’â ’â ‹â ‰â ‰â ‰â ‰â ‰â ‰â”‚ Ïƒ(x)\n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â¢€â¡ â –â ‹â â €â €â €â €â €â €â €â €â €â €â €â €â”‚     \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â£€â ”â â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚     \n   f(x)   â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡ â¡â â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚     \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢€â¡”â ‹â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚     \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â ¤â Šâ â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚     \n        0 â”‚â£€â£€â£€â£€â£€â£€â£€â ¤â ¤â ¤â ’â Šâ ‰â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚     \n          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     \n          â €-5â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €5â €     \n          â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €     \n\n\n\n\n\n","category":"function"},{"location":"models/activation/#NNlib.sigmoid_fast","page":"Activation Functions ðŸ“š","title":"NNlib.sigmoid_fast","text":"sigmoid_fast(x)\n\nThis is a faster, and very slightly less accurate, version of sigmoid. For `x::Float32, perhaps 3 times faster, and maximum errors 2 eps instead of 1.\n\nSee also tanh_fast.\n\njulia> sigmoid(0.2f0)\n0.54983395f0\n\njulia> sigmoid_fast(0.2f0)\n0.54983395f0\n\njulia> hardÏƒ(0.2f0)\n0.53333336f0\n\n\n\n\n\n","category":"function"},{"location":"models/activation/#NNlib.hardtanh","page":"Activation Functions ðŸ“š","title":"NNlib.hardtanh","text":"hardtanh(x) = max(-1, min(1, x))\n\nSegment-wise linear approximation of tanh, much cheaper to compute. See \"Large Scale Machine Learning\".\n\nSee also tanh_fast.\n\njulia> lineplot(hardtanh, -2, 2, height=7)\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            \n         1 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â£€â ”â ‹â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â”‚ hardtanh(x)\n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â£€â¡¤â Šâ â €â €â €â €â €â €â €â €â €â €â €â €â”‚            \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â¢€â¡¤â Šâ â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚            \n   f(x)    â”‚â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â¢¤â¡¤â¡·â ¥â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â”‚            \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡ â –â â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚            \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â¡ â –â ‹â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚            \n        -1 â”‚â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â ”â ‹â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚            \n           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            \n           â €-2â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €2â €            \n           â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €x\n\njulia> lineplot(tanh, -2, 2, height=7)\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        \n         1 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â£€â¡ â ¤â ¤â ’â ’â ’â Šâ ‰â ‰â ‰â”‚ tanh(x)\n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â¢€â¡ â ”â Šâ ‰â €â €â €â €â €â €â €â €â €â €â €â”‚        \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â¢€â¡¤â ’â â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n   f(x)    â”‚â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â¢¤â¡¤â¡·â ¥â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â”‚        \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢€â¡¤â –â â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â£€â¡ â ”â Šâ â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n        -1 â”‚â£€â£€â£€â¡ â ¤â ¤â ¤â –â ’â Šâ ‰â â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        \n           â €-2â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €2â €        \n           â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €        \n\n\n\n\n\n","category":"function"},{"location":"models/activation/#NNlib.tanh_fast","page":"Activation Functions ðŸ“š","title":"NNlib.tanh_fast","text":"tanh_fast(x)\n\nThis is a faster but slighly less accurate version of tanh.\n\nWhere Julia's tanh function has an error under 2 eps, this may be wrong by 5 eps, a reduction by less than one decimal digit. \n\nFor x::Float32 this is usually about 10 times faster, with a smaller speedup for x::Float64. For any other number types, it just calls tanh.\n\nSee also sigmoid_fast.\n\njulia> tanh(0.5f0)\n0.46211717f0\n\njulia> tanh_fast(0.5f0)\n0.46211714f0\n\njulia> hard_tanh(0.5f0)\n0.5f0\n\n\n\n\n\n","category":"function"},{"location":"models/activation/#NNlib.leakyrelu","page":"Activation Functions ðŸ“š","title":"NNlib.leakyrelu","text":"leakyrelu(x, a=0.01) = max(a*x, x)\n\nLeaky Rectified Linear Unit activation function. You can also specify the coefficient explicitly, e.g. leakyrelu(x, 0.01).\n\njulia> lineplot(x -> leakyrelu(x, 0.5), -2, 2, height=7)\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       \n         2 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â ¤â ’â ‰â”‚ #42(x)\n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â¢€â¡ â ”â Šâ ‰â €â €â €â €â”‚       \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â£€â¡¤â –â Šâ â €â €â €â €â €â €â €â €â”‚       \n   f(x)    â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â£€â ¤â –â ‹â â €â €â €â €â €â €â €â €â €â €â €â €â”‚       \n           â”‚â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â£¤â£¤â¡¤â¡§â ¶â ­â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â”‚       \n           â”‚â €â €â €â €â €â €â €â €â¢€â£€â£€â ¤â ¤â ’â ’â ‹â ‰â â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚       \n        -1 â”‚â£€â£€â ¤â ¤â ’â ’â Šâ ‰â â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚       \n           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       \n           â €-2â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €2â €       \n           â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €       \n\njulia> leakyrelu(-10f0, 0.2)\n-2.0f0\n\njulia> leakyrelu(-10f0, 0.02)\n-0.5f0\n\n\n\n\n\n","category":"function"},{"location":"models/activation/#NNlib.lisht","page":"Activation Functions ðŸ“š","title":"NNlib.lisht","text":"lisht(x) = x * tanh(x)\n\nActivation function from  \"LiSHT: Non-Parametric Linearly Scaled Hyperbolic Tangent ...\"\n\njulia> lineplot(lisht, -2, 2, height=7)\n          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         \n        2 â”‚â ¢â£„â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â ”â”‚ lisht(x)\n          â”‚â €â ˆâ ‘â¢¦â¡€â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢€â¡¤â Šâ â €â”‚         \n          â”‚â €â €â €â €â ˆâ £â£„â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â£€â ”â â €â €â €â €â”‚         \n   f(x)   â”‚â €â €â €â €â €â €â €â ‘â¢†â¡€â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â¡ â Šâ â €â €â €â €â €â €â”‚         \n          â”‚â €â €â €â €â €â €â €â €â €â ‰â ¢â¡„â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â¢€â ”â ‹â €â €â €â €â €â €â €â €â €â”‚         \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â ˆâ “â¢„â¡€â €â €â €â €â €â¡‡â €â €â €â €â¢€â¡ â –â â €â €â €â €â €â €â €â €â €â €â €â”‚         \n        0 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â ˆâ “â ¦â£„â£€â£€â£‡â£€â£€â ¤â ’â â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚         \n          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         \n          â €-2â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €2â €         \n          â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €         \n\njulia> lineplot!(ans, logcosh)\n          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           \n        2 â”‚â ¢â£„â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â ”â”‚ lisht(x)  \n          â”‚â €â ˆâ ‘â¢¦â¡€â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢€â¡¤â Šâ â €â”‚ logcosh(x)\n          â”‚â ¢â£„â €â €â ˆâ £â£„â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â£€â ”â â €â €â£€â ”â”‚           \n   f(x)   â”‚â €â ˆâ ‘â ¢â£€â €â €â ‘â¢†â¡€â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â¡ â Šâ â €â£€â ”â Šâ â €â”‚           \n          â”‚â €â €â €â €â €â ‰â ¢â¢„â¡€â ‰â ¢â¡„â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â¢€â ”â ‹â €â¡ â ”â ‹â â €â €â €â €â”‚           \n          â”‚â €â €â €â €â €â €â €â €â ‰â “â ¦â£Œâ¡“â¢„â¡€â €â €â €â €â €â¡‡â €â €â €â €â¢€â¡ â –â£â ¤â ’â ‰â €â €â €â €â €â €â €â €â”‚           \n        0 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â ‰â “â ªâ ·â£¦â£„â£€â£€â£‡â£€â£€â£¤â ¶â •â ’â ‰â €â €â €â €â €â €â €â €â €â €â €â €â”‚           \n          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           \n          â €-2â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €2â €           \n          â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €           \n\n\n\n\n\n","category":"function"},{"location":"models/activation/#NNlib.logcosh","page":"Activation Functions ðŸ“š","title":"NNlib.logcosh","text":"logcosh(x)\n\nReturn log(cosh(x)) which is computed in a numerically stable way.\n\njulia> lineplot(logcosh, -5, 5, height=7)\n          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           \n        5 â”‚â¡€â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚ logcosh(x)\n          â”‚â ‰â ¢â£„â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â ”â ‹â”‚           \n          â”‚â €â €â €â ‘â ¢â£„â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â ”â Šâ â €â €â”‚           \n   f(x)   â”‚â €â €â €â €â €â €â ‘â ¦â£€â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â£€â¡¤â Šâ â €â €â €â €â €â”‚           \n          â”‚â €â €â €â €â €â €â €â €â €â ‘â ¦â¡€â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â¢€â¡¤â Šâ â €â €â €â €â €â €â €â €â”‚           \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â ˆâ “â ¦â¡€â €â €â €â €â €â¡‡â €â €â €â €â¢€â¡¤â ’â â €â €â €â €â €â €â €â €â €â €â €â”‚           \n        0 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â ˆâ ‘â ¢â¢„â£€â£€â£‡â£€â¡ â ”â Šâ â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚           \n          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           \n          â €-5â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €5â €           \n          â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €           \n\n\n\n\n\n","category":"function"},{"location":"models/activation/#NNlib.logsigmoid","page":"Activation Functions ðŸ“š","title":"NNlib.logsigmoid","text":"logÏƒ(x)\n\nReturn log(Ïƒ(x)) which is computed in a numerically stable way.\n\njulia> lineplot(logsigmoid, -5, 5, height=7)\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        \n         0 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â¡§â ¤â ”â ’â ’â ’â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â”‚ logÏƒ(x)\n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â¡¤â –â Šâ ‰â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n           â”‚â €â €â €â €â €â €â €â €â €â €â£€â ¤â ’â ‰â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n   f(x)    â”‚â €â €â €â €â €â €â¢€â¡¤â –â ‹â â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n           â”‚â €â €â €â£€â ”â Šâ â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n           â”‚â¡¤â –â ‹â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n        -6 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        \n           â €-5â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €5â €        \n           â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €        \n\n\n\n\n\n","category":"function"},{"location":"models/activation/#NNlib.mish","page":"Activation Functions ðŸ“š","title":"NNlib.mish","text":"mish(x) = x * tanh(softplus(x))\n\nActivation function from \"Mish: A Self Regularized Non-Monotonic Neural Activation Function\".\n\njulia> lineplot(mish, -5, 5, height=7)\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        \n         5 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢€â¡ â –â ‹â”‚ mish(x)\n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â£€â¡¤â ’â â €â €â €â”‚        \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â¡ â ”â ‹â â €â €â €â €â €â €â”‚        \n   f(x)    â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â¢€â¡ â –â ‹â €â €â €â €â €â €â €â €â €â €â”‚        \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â¢€â¡¤â –â â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n           â”‚â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£§â£”â£Šâ£â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â”‚        \n        -1 â”‚â €â €â €â €â €â €â €â €â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        \n           â €-5â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €5â €        \n           â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €        \n\n\n\n\n\n","category":"function"},{"location":"models/activation/#NNlib.relu","page":"Activation Functions ðŸ“š","title":"NNlib.relu","text":"relu(x) = max(0, x)\n\nRectified Linear Unit activation function.\n\njulia> lineplot(relu, -2, 2, height=7)\n          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        \n        2 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â ”â ‹â”‚ relu(x)\n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â¡¤â Šâ â €â €â”‚        \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â¢€â¡¤â Šâ â €â €â €â €â €â”‚        \n   f(x)   â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â¢€â¡¤â –â â €â €â €â €â €â €â €â €â”‚        \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â¡ â –â â €â €â €â €â €â €â €â €â €â €â €â”‚        \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â¡ â –â ‹â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n        0 â”‚â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£‡â ”â ‹â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        \n          â €-2â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €2â €        \n          â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €        \n\n\n\n\n\n","category":"function"},{"location":"models/activation/#NNlib.relu6","page":"Activation Functions ðŸ“š","title":"NNlib.relu6","text":"relu6(x) = min(max(0, x), 6)\n\nRectified Linear Unit activation function capped at 6. See \"Convolutional Deep Belief Networks\" from CIFAR-10.\n\njulia> lineplot(relu6, -10, 10, height=7)\n          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         \n        6 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â£€â Žâ ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â”‚ relu6(x)\n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â¢€â¡”â â €â €â €â €â €â €â €â €â €â”‚         \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â¡¤â ƒâ €â €â €â €â €â €â €â €â €â €â €â”‚         \n   f(x)   â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â¡ â Žâ €â €â €â €â €â €â €â €â €â €â €â €â €â”‚         \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â¢€â –â â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚         \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â¡”â ƒâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚         \n        0 â”‚â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â¡§â ‹â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚         \n          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         \n          â €-10â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €10â €         \n          â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €         \n\n\n\n\n\n","category":"function"},{"location":"models/activation/#NNlib.rrelu","page":"Activation Functions ðŸ“š","title":"NNlib.rrelu","text":"rrelu(x, lo=1/8, hi=1/3) = max(a*x, x)\n# where `a` is randomly sampled from uniform distribution `U(lo, hi)`\n\nRandomized Leaky Rectified Linear Unit activation function. See \"Empirical Evaluation of Rectified Activations\" You can also specify the bound explicitly, e.g. rrelu(x, 0.0, 1.0).\n\njulia> lineplot(rrelu, -20, 10, height=7)\n            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         \n         10 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢¸â €â €â €â €â €â €â €â €â €â£€â¡¤â –â ‹â”‚ rrelu(x)\n            â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢¸â €â €â €â €â €â¢€â¡ â –â ‹â â €â €â €â”‚         \n            â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢¸â €â¢€â¡ â ”â Šâ â €â €â €â €â €â €â €â”‚         \n   f(x)     â”‚â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â¢¤â¡¤â ¤â£¤â£¤â¢¤â£¤â£¤â ¤â ¤â ¤â¢¼â ®â ¥â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â”‚         \n            â”‚â£°â¢€â£†â¡„â£„â¡„â¡ â¡°â ¦â ·â¡œâ¢¢â ·â ³â ¢â Šâ ‰â ‰â €â €â â €â €â €â €â €â¢¸â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚         \n            â”‚â ƒâ ‰â ™â ˜â ƒâ ˆâ â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢¸â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚         \n        -10 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢¸â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚         \n            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         \n            â €-20â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €10â €         \n            â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €         \n\njulia> extrema(rrelu.(fill(-10f0, 1000)))\n(-3.3316886f0, -1.2548422f0)\n\n\n\n\n\n","category":"function"},{"location":"models/activation/#NNlib.selu","page":"Activation Functions ðŸ“š","title":"NNlib.selu","text":"selu(x) = Î» * (x â‰¥ 0 ? x : Î± * (exp(x) - 1))\n\nÎ» â‰ˆ 1.05070...\nÎ± â‰ˆ 1.67326...\n\nScaled exponential linear units. See \"Self-Normalizing Neural Networks\".\n\njulia> lineplot(selu, -3, 2, height=7)\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        \n         3 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚ selu(x)\n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â£€â¡¤â ¤â ’â”‚        \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â¢€â£€â ¤â –â Šâ ‰â €â €â €â €â”‚        \n   f(x)    â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â£€â¡ â ¤â ’â ‹â â €â €â €â €â €â €â €â €â €â”‚        \n           â”‚â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â£‰â ­â ›â¡â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â”‚        \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â¢€â£€â£€â¡¤â ¤â ’â Šâ ‰â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n        -2 â”‚â ¤â ¤â –â ’â ’â ’â ’â ’â ’â ’â ‰â ‰â ‰â â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        \n           â €-3â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €2â €        \n           â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €        \n\njulia> selu(-10f0)\n-1.7580194f0\n\n\n\n\n\n","category":"function"},{"location":"models/activation/#NNlib.sigmoid","page":"Activation Functions ðŸ“š","title":"NNlib.sigmoid","text":"Ïƒ(x) = 1 / (1 + exp(-x))\n\nClassic sigmoid activation function. Unicode Ïƒ can be entered as \\sigma then tab, in many editors. The ascii name sigmoid is also exported.\n\nSee also sigmoid_fast.\n\njulia> using UnicodePlots\n\njulia> lineplot(sigmoid, -5, 5, height=7)\n          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     \n        1 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â£€â¡ â ¤â –â ’â ’â ‹â ‰â ‰â ‰â ‰â ‰â ‰â”‚ Ïƒ(x)\n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â¢€â¡ â –â ‹â â €â €â €â €â €â €â €â €â €â €â €â €â”‚     \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â£€â ”â â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚     \n   f(x)   â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡ â¡â â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚     \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢€â¡”â ‹â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚     \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â ¤â Šâ â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚     \n        0 â”‚â£€â£€â£€â£€â£€â£€â£€â ¤â ¤â ¤â ’â Šâ ‰â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚     \n          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     \n          â €-5â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €5â €     \n          â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €     \n\njulia> sigmoid === Ïƒ\ntrue\n\n\n\n\n\n","category":"function"},{"location":"models/activation/#NNlib.softplus","page":"Activation Functions ðŸ“š","title":"NNlib.softplus","text":"softplus(x) = log(exp(x) + 1)\n\nSee \"Deep Sparse Rectifier Neural Networks\", JMLR 2011.\n\njulia> lineplot(softplus, -3, 3, height=7)\n          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            \n        4 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚ softplus(x)\n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢€â¡ â”‚            \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â¢€â¡ â ”â Šâ â €â”‚            \n   f(x)   â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â£€â¡¤â ”â Šâ â €â €â €â €â €â”‚            \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â£€â¡ â ¤â ’â ‰â â €â €â €â €â €â €â €â €â €â”‚            \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â£€â¡§â ¤â ’â Šâ ‰â â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚            \n        0 â”‚â£€â£€â£€â£€â£€â£€â£€â¡ â ¤â ¤â ¤â ¤â ”â ’â ’â šâ ‰â ‰â â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚            \n          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            \n          â €-3â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €3â €            \n          â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €            \n\njulia> lineplot!(ans, relu)\n          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            \n        4 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚ softplus(x)\n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢€â£ â”‚ relu(x)    \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â¢€â£ â¡´â žâ ‹â â”‚            \n   f(x)   â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â£€â£¤â¡´â žâ ‹â â €â €â €â €â”‚            \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â£€â¡ â¢¤â¡²â â ‹â â €â €â €â €â €â €â €â €â”‚            \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â£€â¡§â ¤â ’â Šâ£‰â ¥â šâ â €â €â €â €â €â €â €â €â €â €â €â €â”‚            \n        0 â”‚â£€â£€â£€â£€â£€â£€â£€â£ â£¤â£¤â£¤â£¤â£”â£’â£’â£šâ£‰â£‰â£â£€â£‡â ´â ’â ‰â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚            \n          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            \n          â €-3â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €3â €            \n          â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €            \n\njulia> softplus(16f0)\n16.0f0\n\n\n\n\n\n","category":"function"},{"location":"models/activation/#NNlib.softshrink","page":"Activation Functions ðŸ“š","title":"NNlib.softshrink","text":"softshrink(x, Î»=0.5) =\n    (x â‰¥ Î» ? x - Î» : (-Î» â‰¥ x ? x + Î» : 0))\n\nSee \"Softshrink Activation Function\".\n\njulia> lineplot(softshrink, -2, 2, height=7)\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              \n         2 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â”‚ softshrink(x)\n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â¢€â£€â¡¤â ”â ’â ‰â â”‚              \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â£€â¡¤â ¤â ’â ‹â â €â €â €â €â €â €â”‚              \n   f(x)    â”‚â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â£¤â¡¤â ¤â ¤â ¤â ¤â ¤â ¤â¡§â ¤â ¤â ¤â ¤â ¶â ®â ­â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â”‚              \n           â”‚â €â €â €â €â €â €â¢€â£€â ¤â –â ’â ‰â â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚              \n           â”‚â €â£€â ¤â ”â ’â ‹â â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚              \n        -2 â”‚â ‰â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚              \n           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              \n           â €-2â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €2â €              \n           â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €              \n\njulia> lineplot!(ans, tanhshrink)\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              \n         2 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â”‚ softshrink(x)\n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â¢€â£€â¡¤â ”â ’â£‰â¡¡â”‚ tanhshrink(x)\n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â£€â¡¤â ¤â£’â£‹â ¥â ¤â ’â Šâ ‰â â €â”‚              \n   f(x)    â”‚â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â£¤â£¤â£¤â£¤â¡¤â ¤â ¤â ¤â ¤â ¤â ¤â¡·â ¶â ¶â ¶â ¶â ¶â ¾â ¿â ¯â ­â ­â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â”‚              \n           â”‚â €â¢€â£€â¡ â ¤â –â¢’â£‹â ­â —â ’â ‰â â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚              \n           â”‚â Šâ£‰â ¤â ”â ’â ‹â â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚              \n        -2 â”‚â ‰â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚              \n           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              \n           â €-2â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €2â €              \n           â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €\n\njulia> softshrink.((-10f0, 10f0))\n(-9.5f0, 9.5f0)\n\n\n\n\n\n","category":"function"},{"location":"models/activation/#NNlib.softsign","page":"Activation Functions ðŸ“š","title":"NNlib.softsign","text":"softsign(x) = x / (1 + |x|)\n\nSee \"Quadratic Polynomials Learn Better Image Features\" (2009).\n\njulia> lineplot(softsign, -5, 5, height=7)\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            \n         1 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â£€â£€â£€â£€â£€â ¤â ¤â ¤â ¤â ¤â”‚ softsign(x)\n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â£€â¡¤â –â ’â ‹â ‰â ‰â â €â €â €â €â €â €â €â €â €â”‚            \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â¡”â ‹â â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚            \n   f(x)    â”‚â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â¢¤â¡¯â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â”‚            \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â ”â â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚            \n           â”‚â €â €â €â €â €â €â €â €â €â €â£€â£€â£€â ¤â ¤â ’â ‹â â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚            \n        -1 â”‚â ’â ’â ’â ’â ’â Šâ ‰â ‰â ‰â ‰â â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚            \n           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            \n           â €-5â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €5â €            \n           â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €            \n\njulia> lineplot!(ans, tanh)\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            \n         1 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â¢€â¡¤â –â Šâ ‰â ‰â ‰â£‰â£‰â£‰â£‰â£‰â ­â ­â ­â ­â ­â”‚ softsign(x)\n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â¡”â£ƒâ¡¤â –â ’â ‹â ‰â ‰â â €â €â €â €â €â €â €â €â €â”‚ tanh(x)    \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£§â¡žâ ‹â â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚            \n   f(x)    â”‚â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â¢¤â¡¯â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â”‚            \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â¡´â ƒâ¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚            \n           â”‚â €â €â €â €â €â €â €â €â €â €â£€â£€â£€â ¤â ¤â ’â¢‹â •â â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚            \n        -1 â”‚â£’â£’â£’â£’â£’â£Šâ£‰â£‰â£‰â£‰â£â£€â£€â¡ â ¤â ’â â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚            \n           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            \n           â €-5â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €5â €            \n           â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €            \n\njulia> softsign(1f0)\n0.5f0\n\njulia> softsign(100f0)\n0.990099f0\n\n\n\n\n\n","category":"function"},{"location":"models/activation/#NNlib.swish","page":"Activation Functions ðŸ“š","title":"NNlib.swish","text":"swish(x) = x * Ïƒ(x)\n\nSelf-gated activation function. See \"Swish: a Self-Gated Activation Function\".\n\njulia> lineplot(swish, -2, 2, height=7)\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         \n         2 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â¡¤â”‚ swish(x)\n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â¡¤â –â ‹â â €â”‚         \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â£€â ¤â –â ‹â â €â €â €â €â €â”‚         \n   f(x)    â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â¢€â£€â¡¤â ”â Šâ ‰â €â €â €â €â €â €â €â €â €â €â”‚         \n           â”‚â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â¢¤â£¤â£¤â¡¤â¡§â ´â ¶â ¯â ¥â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â”‚         \n           â”‚â ‰â ‘â ’â ’â ’â ’â ’â ’â ’â ’â ’â ’â ‰â ‰â ‰â ‰â â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚         \n        -1 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚         \n           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         \n           â €-2â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €2â €         \n           â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €         \n\n\n\n\n\n","category":"function"},{"location":"models/activation/#NNlib.hardswish","page":"Activation Functions ðŸ“š","title":"NNlib.hardswish","text":"hardswish(x) = x * hardÏƒ(x)\n\nHard-Swish activation function. See \"Searching for MobileNetV3\".\n\njulia> lineplot(hardswish, -2, 5, height = 7)\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             \n         5 â”‚â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â¡¤â ”â ’â ‰â”‚ hardswish(x)\n           â”‚â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â¡ â ”â ’â ‰â â €â €â €â €â”‚             \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â ¤â –â ‰â â €â €â €â €â €â €â €â €â €â”‚             \n   f(x)    â”‚â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â¢€â¡ â ”â Šâ â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚             \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â¢€â£€â ¤â –â ‹â â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚             \n           â”‚â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£‡â£¤â£¤â£–â£šâ£‰â£â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â”‚             \n        -1 â”‚â ‰â ’â ’â ’â ’â ‰â ‰â ‰â ‰â â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚             \n           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             \n           â €-2â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €5â €             \n           â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €             \n\njulia> lineplot(hardswish, -4, 0, height = 7);\n\njulia> lineplot!(ans, swish)\n             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             \n           0 â”‚â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â¢£â¡€â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢€â¡œâ”‚ hardswish(x)\n             â”‚â ’â ’â ¢â ¤â¢„â£€â¡€â €â €â €â €â ±â¡„â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢€â Žâ €â”‚ swish(x)    \n             â”‚â €â €â €â €â €â €â ˆâ ‰â ‘â ’â ¦â¢„â£˜â¢„â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢€â¡´â ƒâ €â €â”‚             \n   f(x)      â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â ‰â ‘â¡–â ¦â¢„â£€â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â¢”â â â €â €â €â”‚             \n             â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â ˆâ £â£„â €â ‰â ‘â ’â ¦â ¤â¢„â£€â£€â£€â£€â¡ â ¤â –â£Šâ •â â €â €â €â €â €â”‚             \n             â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â “â ¤â¡€â €â €â €â €â €â €â €â €â¢€â¡¤â –â â €â €â €â €â €â €â €â”‚             \n        -0.4 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â ˆâ ‰â ’â ¢â ¤â ¤â ”â ’â ‹â â €â €â €â €â €â €â €â €â €â €â”‚             \n             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             \n             â €-4â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €0â €             \n             â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €             \n\njulia> hardswish.(-5:5)'\n1Ã—11 adjoint(::Vector{Float64}) with eltype Float64:\n -0.0  -0.0  -0.0  -0.333333  -0.333333  0.0  0.666667  1.66667  3.0  4.0  5.0\n\n\n\n\n\n","category":"function"},{"location":"models/activation/#NNlib.tanhshrink","page":"Activation Functions ðŸ“š","title":"NNlib.tanhshrink","text":"tanhshrink(x) = x - tanh(x)\n\nSee \"Tanhshrink Activation Function\".\n\njulia> lineplot(tanhshrink, -3, 3, height=7)\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              \n         3 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚ tanhshrink(x)\n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â¡ â ¤â –â Šâ”‚              \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â¢€â£€â¡ â ¤â ’â Šâ ‰â â €â €â €â €â”‚              \n   f(x)    â”‚â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â¢¤â£¤â¡¤â ¤â ¤â ¤â ¤â ¤â ¤â¡·â ¶â ¶â ¶â ¶â ¶â ®â ­â ¥â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â”‚              \n           â”‚â €â €â €â €â €â£€â¡ â ´â ’â Šâ ‰â â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚              \n           â”‚â¡ â ´â ’â Šâ ‰â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚              \n        -3 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚              \n           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              \n           â €-3â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €3â €              \n           â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €              \n\njulia> tanhshrink.((-10f0, 10f0))\n(-9.0f0, 9.0f0)\n\n\n\n\n\n","category":"function"},{"location":"models/activation/#NNlib.trelu","page":"Activation Functions ðŸ“š","title":"NNlib.trelu","text":"trelu(x, theta=1) = x > theta ? x : 0\n\nThreshold gated rectified linear activation function. See \"Zero-bias autoencoders and the benefits of co-adapting features\"\n\njulia> lineplot(trelu, -2, 4, height=7)\n          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         \n        4 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â¡¤â –â ‹â”‚ trelu(x)\n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢€â¡ â –â ‹â â €â €â €â”‚         \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¢€â¡ â ”â Šâ â €â €â €â €â €â €â €â”‚         \n   f(x)   â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â£€â ´â Šâ â €â €â €â €â €â €â €â €â €â €â €â”‚         \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â£ â ¤â ’â ‰â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚         \n          â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â €â¡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚         \n        0 â”‚â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£€â£‡â£€â£€â£€â£€â£€â£€â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚         \n          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         \n          â €-2â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €4â €         \n          â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €         \n\n\n\n\n\n","category":"function"},{"location":"models/activation/#One-More","page":"Activation Functions ðŸ“š","title":"One More","text":"","category":"section"},{"location":"models/activation/","page":"Activation Functions ðŸ“š","title":"Activation Functions ðŸ“š","text":"Julia's Base.Math also provides tanh, which can be used as an activation function.","category":"page"},{"location":"models/activation/","page":"Activation Functions ðŸ“š","title":"Activation Functions ðŸ“š","text":"Note that many Flux layers will automatically replace this with NNlib.tanh_fast when called, as Base's tanh is slow enough to sometimes be a bottleneck.","category":"page"},{"location":"models/activation/","page":"Activation Functions ðŸ“š","title":"Activation Functions ðŸ“š","text":"julia> using UnicodePlots\n\njulia> lineplot(tanh, -3, 3, height=7)\n           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        \n         1 â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â €â €â €â£€â ¤â ”â ’â ’â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â ‰â”‚ tanh(x)\n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â €â €â¡ â –â ‹â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡‡â¡°â Šâ â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n   f(x)    â”‚â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â¡¤â¡¯â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â ¤â”‚        \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â¡ â Žâ â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n           â”‚â €â €â €â €â €â €â €â €â €â €â €â €â €â €â£€â ´â Šâ €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n        -1 â”‚â£€â£€â£€â£€â£€â£€â£€â£€â£€â¡¤â ¤â ”â ’â ‰â â €â €â €â €â €â¡‡â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â”‚        \n           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        \n           â €-3â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €3â €        \n           â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €xâ €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €â €        ","category":"page"},{"location":"models/activation/","page":"Activation Functions ðŸ“š","title":"Activation Functions ðŸ“š","text":"tanh","category":"page"},{"location":"models/activation/#Base.tanh","page":"Activation Functions ðŸ“š","title":"Base.tanh","text":"tanh(x)\n\nCompute hyperbolic tangent of x.\n\n\n\n\n\n","category":"function"},{"location":"training/training/#man-training","page":"Training","title":"Training","text":"","category":"section"},{"location":"training/training/","page":"Training","title":"Training","text":"To actually train a model we need four things:","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"A objective function, that evaluates how well a model is doing given some input data.\nThe trainable parameters of the model.\nA collection of data points that will be provided to the objective function.\nAn optimiser that will update the model parameters appropriately.","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"Training a model is typically an iterative process, where we go over the data set, calculate the objective function over the data points, and optimise that. This can be visualised in the form of a simple loop.","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"for d in datapoints\n\n  # `d` should produce a collection of arguments\n  # to the loss function\n\n  # Calculate the gradients of the parameters\n  # with respect to the loss function\n  grads = Flux.gradient(parameters) do\n    loss(d...)\n  end\n\n  # Update the parameters based on the chosen\n  # optimiser (opt)\n  Flux.Optimise.update!(opt, parameters, grads)\nend","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"To make it easy, Flux defines train!:","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"Flux.Optimise.train!","category":"page"},{"location":"training/training/#Flux.Optimise.train!","page":"Training","title":"Flux.Optimise.train!","text":"train!(loss, pars::Params, data, opt::AbstractOptimiser; [cb])\n\nUses a loss function and training data to improve the  model's parameters according to a particular optimisation rule opt.\n\nnote: Note\nThis method with implicit Params will be removed from Flux 0.14. It should be replaced with the explicit method train!(loss, model, data, opt).\n\nFor each d in data, first the gradient of the loss is computed like this:\n\n    gradient(() -> loss(d...), pars)  # if d isa Tuple\n    gradient(() -> loss(d), pars)     # otherwise\n\nHere pars is produced by calling Flux.params on your model. (Or just on the layers you want to train, like train!(loss, params(model[1:end-2]), data, opt).) This is the \"implicit\" style of parameter handling.\n\nThis gradient is then used by optimizer opt to update the parameters:\n\n    update!(opt, pars, grads)\n\nThe optimiser should be from the Flux.Optimise module (see Optimisers). Different optimisers can be combined using Flux.Optimise.Optimiser.\n\nThis training loop iterates through data once. It will stop with a DomainError if the loss is NaN or infinite.\n\nYou can use @epochs to do this several times, or  use for instance Itertools.ncycle to make a longer data iterator.\n\nCallbacks\n\nCallbacks are given with the keyword argument cb. For example, this will print \"training\" every 10 seconds (using Flux.throttle):\n\n    train!(loss, params, data, opt, cb = throttle(() -> println(\"training\"), 10))\n\nThe callback can call Flux.stop to interrupt the training loop.\n\nMultiple callbacks can be passed to cb as array.\n\n\n\n\n\ntrain!(loss, model, data, opt)\n\nUses a loss function and training data to improve the model's parameters according to a particular optimisation rule opt. Iterates through data once, evaluating for each d in data either loss(model, d...) if d isa Tuple, or else loss(model, d) for other d.\n\nFor example, with these definitions...\n\ndata = [(x1, y1), (x2, y2), (x3, y3)]\n\nloss3(m, x, y) = norm(m(x) .- y)        # the model is the first argument\n\nopt = Flux.setup(Adam(), model)         # explicit setup of optimiser momenta\n\n...calling Flux.train!(loss3, model, data, opt) runs a loop much like this, using Zygote's \"explicit\" mode for the gradient:\n\nfor d in data\n    âˆ‚Lâˆ‚m = gradient(loss3, model, d...)[1]\n    update!(opt, model, âˆ‚Lâˆ‚m)           # method for \"explicit\" gradient\nend\n\nYou can also write this loop yourself, if you need more flexibility. For this reason train! is not highly extensible. It adds only a few features to the loop above:\n\nStop with a DomainError if the loss is infinite or NaN at any point.\nShow a progress bar using @withprogress.\n\nnote: Note\nThis method has significant changes from the one in Flux â‰¤ 0.13:It now takes the model itself, not the result of Flux.params. (This is to move away from Zygote's \"implicit\" parameter handling, with Grads.)\nInstead of loss being a function which accepts only the data, now it must also accept the model itself, as the first argument.\nopt should be the result of Flux.setup. Using an optimiser such as Adam() without this step should give you a warning.\nCallback functions are not supported. But any code can be included in the above for loop.\n\n\n\n\n\n","category":"function"},{"location":"training/training/","page":"Training","title":"Training","text":"There are plenty of examples in the model zoo, and more information can be found on Custom Training Loops.","category":"page"},{"location":"training/training/#Loss-Functions","page":"Training","title":"Loss Functions","text":"","category":"section"},{"location":"training/training/","page":"Training","title":"Training","text":"The objective function must return a number representing how far the model is from its target â€“ the loss of the model. The loss function that we defined in basics will work as an objective. In addition to custom losses, model can be trained in conjuction with the commonly used losses that are grouped under the Flux.Losses module. We can also define an objective in terms of some model:","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"m = Chain(\n  Dense(784 => 32, Ïƒ),\n  Dense(32 => 10), softmax)\n\nloss(x, y) = Flux.Losses.mse(m(x), y)\nps = Flux.params(m)\n\n# later\nFlux.train!(loss, ps, data, opt)","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"The objective will almost always be defined in terms of some cost function that measures the distance of the prediction m(x) from the target y. Flux has several of these built-in, like mse for mean squared error or crossentropy for cross-entropy loss, but you can calculate it however you want. For a list of all built-in loss functions, check out the losses reference.","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"At first glance, it may seem strange that the model that we want to train is not part of the input arguments of Flux.train! too. However the target of the optimizer is not the model itself, but the objective function that represents the departure between modelled and observed data. In other words, the model is implicitly defined in the objective function, and there is no need to give it explicitly. Passing the objective function instead of the model and a cost function separately provides more flexibility and the possibility of optimizing the calculations.","category":"page"},{"location":"training/training/#Model-parameters","page":"Training","title":"Model parameters","text":"","category":"section"},{"location":"training/training/","page":"Training","title":"Training","text":"The model to be trained must have a set of tracked parameters that are used to calculate the gradients of the objective function. In the basics section it is explained how to create models with such parameters. The second argument of the function Flux.train! must be an object containing those parameters, which can be obtained from a model m as Flux.params(m).","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"Such an object contains a reference to the model's parameters, not a copy, such that after their training, the model behaves according to their updated values.","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"Handling all the parameters on a layer by layer basis is explained in the Layer Helpers section. Also, for freezing model parameters, see the Advanced Usage Guide.","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"Flux.params","category":"page"},{"location":"training/training/#Flux.params","page":"Training","title":"Flux.params","text":"params(model)\nparams(layers...)\n\nGiven a model or specific layers from a model, create a Params object pointing to its trainable parameters.\n\nThis can be used with the gradient function, see Taking Gradients, or as input to the Flux.train! function.\n\nThe behaviour of params on custom types can be customized using Functors.@functor or Flux.trainable.\n\nExamples\n\njulia> using Flux: params\n\njulia> params(Chain(Dense(ones(2,3)), softmax))  # unpacks Flux models\nParams([[1.0 1.0 1.0; 1.0 1.0 1.0], [0.0, 0.0]])\n\njulia> bn = BatchNorm(2, relu)\nBatchNorm(2, relu)  # 4 parameters, plus 4 non-trainable\n\njulia> params(bn)  # only the trainable parameters\nParams([Float32[0.0, 0.0], Float32[1.0, 1.0]])\n\njulia> params([1, 2, 3], [4])  # one or more arrays of numbers\nParams([[1, 2, 3], [4]])\n\njulia> params([[1, 2, 3], [4]])  # unpacks array of arrays\nParams([[1, 2, 3], [4]])\n\njulia> params(1, [2 2], (alpha=[3,3,3], beta=Ref(4), gamma=sin))  # ignores scalars, unpacks NamedTuples\nParams([[2 2], [3, 3, 3]])\n\n\n\n\n\n","category":"function"},{"location":"training/training/#Datasets","page":"Training","title":"Datasets","text":"","category":"section"},{"location":"training/training/","page":"Training","title":"Training","text":"The data argument of train! provides a collection of data to train with (usually a set of inputs x and target outputs y). For example, here's a dummy dataset with only one data point:","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"x = rand(784)\ny = rand(10)\ndata = [(x, y)]","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"Flux.train! will call loss(x, y), calculate gradients, update the weights and then move on to the next data point if there is one. We can train the model on the same data three times:","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"data = [(x, y), (x, y), (x, y)]\n# Or equivalently\nusing IterTools: ncycle\ndata = ncycle([(x, y)], 3)","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"It's common to load the xs and ys separately. Here you can use zip:","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"xs = [rand(784), rand(784), rand(784)]\nys = [rand( 10), rand( 10), rand( 10)]\ndata = zip(xs, ys)","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"Training data can be conveniently  partitioned for mini-batch training using the Flux.Data.DataLoader type:","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"X = rand(28, 28, 60000)\nY = rand(0:9, 60000)\ndata = DataLoader((X, Y), batchsize=128) ","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"Note that, by default, train! only loops over the data once (a single \"epoch\"). A convenient way to run multiple epochs from the REPL is provided by @epochs.","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"julia> using Flux: @epochs\n\njulia> @epochs 2 println(\"hello\")\n[ Info: Epoch 1\nhello\n[ Info: Epoch 2\nhello\n\njulia> @epochs 2 Flux.train!(...)\n# Train for two epochs","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"Flux.@epochs","category":"page"},{"location":"training/training/#Flux.Optimise.@epochs","page":"Training","title":"Flux.Optimise.@epochs","text":"@epochs N body\n\nRun body N times. Mainly useful for quickly doing multiple epochs of training in a REPL.\n\nnote: Note\nThe macro @epochs will be removed from Flux 0.14. Please just write an ordinary for loop.\n\nExamples\n\njulia> Flux.@epochs 2 println(\"hello\")\n[ Info: Epoch 1\nhello\n[ Info: Epoch 2\nhello\n\n\n\n\n\n","category":"macro"},{"location":"training/training/#Callbacks","page":"Training","title":"Callbacks","text":"","category":"section"},{"location":"training/training/","page":"Training","title":"Training","text":"train! takes an additional argument, cb, that's used for callbacks so that you can observe the training process. For example:","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"train!(objective, ps, data, opt, cb = () -> println(\"training\"))","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"Callbacks are called for every batch of training data. You can slow this down using Flux.throttle(f, timeout) which prevents f from being called more than once every timeout seconds.","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"A more typical callback might look like this:","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"test_x, test_y = # ... create single batch of test data ...\nevalcb() = @show(loss(test_x, test_y))\nthrottled_cb = throttle(evalcb, 5)\nFlux.@epochs 20 Flux.train!(objective, ps, data, opt, cb = throttled_cb)","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"Calling Flux.stop() in a callback will exit the training loop early.","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"cb = function ()\n  accuracy() > 0.9 && Flux.stop()\nend","category":"page"},{"location":"training/training/#Custom-Training-loops","page":"Training","title":"Custom Training loops","text":"","category":"section"},{"location":"training/training/","page":"Training","title":"Training","text":"The Flux.train! function can be very convenient, especially for simple problems. For some problems, however, it's much cleaner to write your own custom training loop. An example follows that works similar to the default Flux.train but with no callbacks. You don't need callbacks if you just code the calls to your functions directly into the loop. E.g. in the places marked with comments.","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"function my_custom_train!(loss, ps, data, opt)\n  # training_loss is declared local so it will be available for logging outside the gradient calculation.\n  local training_loss\n  ps = Params(ps)\n  for d in data\n    gs = gradient(ps) do\n      training_loss = loss(d...)\n      # Code inserted here will be differentiated, unless you need that gradient information\n      # it is better to do the work outside this block.\n      return training_loss\n    end\n    # Insert whatever code you want here that needs training_loss, e.g. logging.\n    # logging_callback(training_loss)\n    # Insert whatever code you want here that needs gradients.\n    # e.g. logging histograms with TensorBoardLogger.jl to check for exploding gradients.\n    update!(opt, ps, gs)\n    # Here you might like to check validation set accuracy, and break out to do early stopping.\n  end\nend","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"You could simplify this further, for example by hard-coding in the loss function.","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"Another possibility is to use Zygote.pullback to access the training loss and the gradient simultaneously.","category":"page"},{"location":"training/training/","page":"Training","title":"Training","text":"function my_custom_train!(loss, ps, data, opt)\n  ps = Params(ps)\n  for d in data\n    # back is a method that computes the product of the gradient so far with its argument.\n    train_loss, back = Zygote.pullback(() -> loss(d...), ps)\n    # Insert whatever code you want here that needs training_loss, e.g. logging.\n    # logging_callback(training_loss)\n    # Apply back() to the correct type of 1.0 to get the gradient of loss.\n    gs = back(one(train_loss))\n    # Insert whatever code you want here that needs gradient.\n    # E.g. logging with TensorBoardLogger.jl as histogram so you can see if it is becoming huge.\n    update!(opt, ps, gs)\n    # Here you might like to check validation set accuracy, and break out to do early stopping.\n  end\nend","category":"page"},{"location":"gpu/#GPU-Support","page":"GPU Support","title":"GPU Support","text":"","category":"section"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"NVIDIA GPU support should work out of the box on systems with CUDA and CUDNN installed. For more details see the CUDA.jl readme.","category":"page"},{"location":"gpu/#Checking-GPU-Availability","page":"GPU Support","title":"Checking GPU Availability","text":"","category":"section"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"By default, Flux will run the checks on your system to see if it can support GPU functionality. You can check if Flux identified a valid GPU setup by typing the following:","category":"page"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"julia> using CUDA\n\njulia> CUDA.functional()\ntrue","category":"page"},{"location":"gpu/#GPU-Usage","page":"GPU Support","title":"GPU Usage","text":"","category":"section"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"Support for array operations on other hardware backends, like GPUs, is provided by external packages like CUDA. Flux is agnostic to array types, so we simply need to move model weights and data to the GPU and Flux will handle it.","category":"page"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"For example, we can use CUDA.CuArray (with the cu converter) to run our basic example on an NVIDIA GPU.","category":"page"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"(Note that you need to have CUDA available to use CUDA.CuArray â€“ please see the CUDA.jl instructions for more details.)","category":"page"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"using CUDA\n\nW = cu(rand(2, 5)) # a 2Ã—5 CuArray\nb = cu(rand(2))\n\npredict(x) = W*x .+ b\nloss(x, y) = sum((predict(x) .- y).^2)\n\nx, y = cu(rand(5)), cu(rand(2)) # Dummy data\nloss(x, y) # ~ 3","category":"page"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"Note that we convert both the parameters (W, b) and the data set (x, y) to cuda arrays. Taking derivatives and training works exactly as before.","category":"page"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"If you define a structured model, like a Dense layer or Chain, you just need to convert the internal parameters. Flux provides fmap, which allows you to alter all parameters of a model at once.","category":"page"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"d = Dense(10 => 5, Ïƒ)\nd = fmap(cu, d)\nd.weight # CuArray\nd(cu(rand(10))) # CuArray output\n\nm = Chain(Dense(10 => 5, Ïƒ), Dense(5 => 2), softmax)\nm = fmap(cu, m)\nd(cu(rand(10)))","category":"page"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"As a convenience, Flux provides the gpu function to convert models and data to the GPU if one is available. By default, it'll do nothing. So, you can safely call gpu on some data or model (as shown below), and the code will not error, regardless of whether the GPU is available or not. If the GPU library (CUDA.jl) loads successfully, gpu will move data from the CPU to the GPU. As is shown below, this will change the type of something like a regular array to a CuArray.","category":"page"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"julia> using Flux, CUDA\n\njulia> m = Dense(10, 5) |> gpu\nDense(10 => 5)      # 55 parameters\n\njulia> x = rand(10) |> gpu\n10-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n 0.066846445\n â‹®\n 0.76706964\n\njulia> m(x)\n5-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n -0.99992573\n â‹®\n -0.547261","category":"page"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"The analogue cpu is also available for moving models and data back off of the GPU.","category":"page"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"julia> x = rand(10) |> gpu\n10-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n 0.8019236\n â‹®\n 0.7766742\n\njulia> x |> cpu\n10-element Vector{Float32}:\n 0.8019236\n â‹®\n 0.7766742","category":"page"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"cpu\ngpu","category":"page"},{"location":"gpu/#Flux.cpu","page":"GPU Support","title":"Flux.cpu","text":"cpu(m)\n\nMoves m onto the CPU, the opposite of gpu. Recurses into structs marked @functor.\n\njulia> m = Dense(1,2)\nDense(1, 2)\n\njulia> m_gpu = gpu(m)\nDense(1, 2)\n\njulia> typeof(m_gpu.W)\nCuArray{Float32, 2}\n\njulia> m_cpu = cpu(m_gpu)\nDense(1, 2)\n\njulia> typeof(m_cpu.W)\nMatrix{Float32}\n\n\n\n\n\n","category":"function"},{"location":"gpu/#Flux.gpu","page":"GPU Support","title":"Flux.gpu","text":"gpu(x)\n\nMoves m to the current GPU device, if available. It is a no-op otherwise. See the CUDA.jl docs  to help identify the current device.\n\nThis works for functions, and any struct marked with @functor.\n\njulia> m = Dense(1,2)\nDense(1, 2)\n\njulia> typeof(m.W)\nMatrix{Float32}\n\njulia> m_gpu = gpu(m)\nDense(1, 2)\n\njulia> typeof(m_gpu.W) # notice the type of the array changed to a CuArray\nCuArray{Float32, 2}\n\n\n\n\n\n","category":"function"},{"location":"gpu/#Common-GPU-Workflows","page":"GPU Support","title":"Common GPU Workflows","text":"","category":"section"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"Some of the common workflows involving the use of GPUs are presented below.","category":"page"},{"location":"gpu/#Transferring-Training-Data","page":"GPU Support","title":"Transferring Training Data","text":"","category":"section"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"In order to train the model using the GPU both model and the training data have to be transferred to GPU memory. This process can be done with the gpu function in two different ways:","category":"page"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"Iterating over the batches in a DataLoader object transferring each one of the training batches at a time to the GPU. \ntrain_loader = Flux.DataLoader((xtrain, ytrain), batchsize = 64, shuffle = true)\n# ... model, optimizer and loss definitions\nfor epoch in 1:nepochs\n    for (xtrain_batch, ytrain_batch) in train_loader\n        x, y = gpu(xtrain_batch), gpu(ytrain_batch)\n        gradients = gradient(() -> loss(x, y), parameters)\n        Flux.Optimise.update!(optimizer, parameters, gradients)\n    end\nend\nTransferring all training data to the GPU at once before creating the DataLoader object. This is usually performed for smaller datasets which are sure to fit in the available GPU memory. Some possibilities are:\ngpu_train_loader = Flux.DataLoader((xtrain |> gpu, ytrain |> gpu), batchsize = 32)\ngpu_train_loader = Flux.DataLoader((xtrain, ytrain) |> gpu, batchsize = 32)\nNote that both gpu and cpu are smart enough to recurse through tuples and namedtuples. Another possibility is to use MLUtils.mapsobs to push the data movement invocation into the background thread:\nusing MLUtils: mapobs\n# ...\ngpu_train_loader = Flux.DataLoader(mapobs(gpu, (xtrain, ytrain)), batchsize = 16)\nWrapping the DataLoader in CUDA.CuIterator to efficiently move data to GPU on demand:\nusing CUDA: CuIterator\ntrain_loader = Flux.DataLoader((xtrain, ytrain), batchsize = 64, shuffle = true)\n# ... model, optimizer and loss definitions\nfor epoch in 1:nepochs\n    for (xtrain_batch, ytrain_batch) in CuIterator(train_loader)\n       # ...\n    end\nend\nNote that this works with a limited number of data types. If iterate(train_loader) returns anything other than arrays, approach 1 or 2 is preferred.","category":"page"},{"location":"gpu/#Saving-GPU-Trained-Models","page":"GPU Support","title":"Saving GPU-Trained Models","text":"","category":"section"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"After the training process is done, one must always transfer the trained model back to the cpu memory scope before serializing or saving to disk. This can be done, as described in the previous section, with:","category":"page"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"model = cpu(model) # or model = model |> cpu","category":"page"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"and then","category":"page"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"using BSON\n# ...\nBSON.@save \"./path/to/trained_model.bson\" model\n\n# in this approach the cpu-transferred model (referenced by the variable `model`)\n# only exists inside the `let` statement\nlet model = cpu(model)\n   # ...\n   BSON.@save \"./path/to/trained_model.bson\" model\nend\n\n# is equivalent to the above, but uses `key=value` storing directive from BSON.jl\nBSON.@save \"./path/to/trained_model.bson\" model = cpu(model)","category":"page"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"The reason behind this is that models trained in the GPU but not transferred to the CPU memory scope will expect CuArrays as input. In other words, Flux models expect input data coming from the same kind device in which they were trained on.","category":"page"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"In controlled scenarios in which the data fed to the loaded models is garanteed to be in the GPU there's no need to transfer them back to CPU memory scope, however in production environments, where artifacts are shared among different processes, equipments or configurations, there is no garantee that the CUDA.jl package will be available for the process performing inference on the model loaded from the disk.","category":"page"},{"location":"gpu/#Disabling-CUDA-or-choosing-which-GPUs-are-visible-to-Flux","page":"GPU Support","title":"Disabling CUDA or choosing which GPUs are visible to Flux","text":"","category":"section"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"Sometimes it is required to control which GPUs are visible to julia on a system with multiple GPUs or disable GPUs entirely. This can be achieved with an environment variable CUDA_VISIBLE_DEVICES.","category":"page"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"To disable all devices:","category":"page"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"$ export CUDA_VISIBLE_DEVICES='-1'","category":"page"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"To select specific devices by device id:","category":"page"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"$ export CUDA_VISIBLE_DEVICES='0,1'","category":"page"},{"location":"gpu/","page":"GPU Support","title":"GPU Support","text":"More information for conditional use of GPUs in CUDA.jl can be found in its documentation, and information about the specific use of the variable is described in the Nvidia CUDA blog post.","category":"page"},{"location":"tutorials/linear_regression/#man-linear-regression","page":"Linear Regression","title":"Linear Regression","text":"","category":"section"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Flux is a pure Julia ML stack that allows you to build predictive models. Here are the steps for a typical Flux program:","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Provide training and test data\nBuild a model with configurable parameters to make predictions\nIteratively train the model by tweaking the parameters to improve predictions\nVerify your model","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Under the hood, Flux uses a technique called automatic differentiation to take gradients that help improve predictions. Flux is also fully written in Julia so you can easily replace any layer of Flux with your own code to improve your understanding or satisfy special requirements.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"The following page contains a step-by-step walkthrough of the linear regression algorithm in Julia using Flux! We will start by creating a simple linear regression model for dummy data and then move on to a real dataset. The first part would involve writing some parts of the model on our own, which will later be replaced by Flux.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Let us start by building a simple linear regression model. This model would be trained on the data points of the form (xâ‚, yâ‚), (xâ‚‚, yâ‚‚), ... , (xâ‚™, yâ‚™). In the real world, these xs can have multiple features, and the ys denote a label. In our example, each x has a single feature; hence, our data would have n data points, each point mapping a single feature to a single label.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Importing the required Julia packages -","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> using Flux, Plots","category":"page"},{"location":"tutorials/linear_regression/#Generating-a-dataset","page":"Linear Regression","title":"Generating a dataset","text":"","category":"section"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"The data usually comes from the real world, which we will be exploring in the last part of this tutorial, but we don't want to jump straight to the relatively harder part. Here we will generate the xs of our data points and map them to the respective ys using a simple function. Remember, here each x is equivalent to a feature, and each y is the corresponding label. Combining all the xs and ys would create the complete dataset.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> x = hcat(collect(Float32, -3:0.1:3)...)\n1Ã—61 Matrix{Float32}:\n -3.0  -2.9  -2.8  -2.7  -2.6  -2.5  â€¦  2.4  2.5  2.6  2.7  2.8  2.9  3.0","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"The hcat call generates a Matrix with numbers ranging from -3.0 to 3.0 with a gap of 0.1 between them. Each column of this matrix holds a single x, a total of 61 xs. The next step would be to generate the corresponding labels or the ys.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> f(x) = @. 3x + 2;\n\njulia> y = f(x)\n1Ã—61 Matrix{Float32}:\n -7.0  -6.7  -6.4  -6.1  -5.8  -5.5  â€¦  9.5  9.8  10.1  10.4  10.7  11.0","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"The function f maps each x to a y, and as x is a Matrix, the expression broadcasts the scalar values using @. macro. Our data points are ready, but they are too perfect. In a real-world scenario, we will not have an f function to generate y values, but instead, the labels would be manually added.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> x = x .* reshape(rand(Float32, 61), (1, 61));","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Visualizing the final data -","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> plot(vec(x), vec(y), lw = 3, seriestype = :scatter, label = \"\", title = \"Generated data\", xlabel = \"x\", ylabel= \"y\");","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"(Image: linear-regression-data)","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"The data looks random enough now! The x and y values are still somewhat correlated; hence, the linear regression algorithm should work fine on our dataset.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"We can now proceed ahead and build a model for our dataset!","category":"page"},{"location":"tutorials/linear_regression/#Building-a-model","page":"Linear Regression","title":"Building a model","text":"","category":"section"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"A linear regression model is defined mathematically as -","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"model(W b x) = Wx + b","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"where W is the weight matrix and b is the bias. For our case, the weight matrix (W) would constitute only a single element, as we have only a single feature. We can define our model in Julia using the exact same notation!","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> custom_model(W, b, x) = @. W*x + b\ncustom_model (generic function with 1 method)","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"The @. macro allows you to perform the calculations by broadcasting the scalar quantities (for example - the bias).","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"The next step would be to initialize the model parameters, which are the weight and the bias. There are a lot of initialization techniques available for different machine learning models, but for the sake of this example, let's pull out the weight from a uniform distribution and initialize the bias as 0.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> W = rand(Float32, 1, 1)\n1Ã—1 Matrix{Float32}:\n 0.99285793\n\njulia> b = [0.0f0]\n1-element Vector{Float32}:\n 0.0","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Time to test if our model works!","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> custom_model(W, b, x) |> size\n(1, 61)\n\njulia> custom_model(W, b, x)[1], y[1]\n(-1.6116865f0, -7.0f0)","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"It does! But the predictions are way off. We need to train the model to improve the predictions, but before training the model we need to define the loss function. The loss function would ideally output a quantity that we will try to minimize during the entire training process. Here we will use the mean sum squared error loss function.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> function custom_loss(W, b, x, y)\n           Å· = custom_model(W, b, x)\n           sum((y .- Å·).^2) / length(x)\n       end;\n\njulia> custom_loss(W, b, x, y)\n23.772217f0","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Calling the loss function on our xs and ys shows how far our predictions (Å·) are from the real labels. More precisely, it calculates the sum of the squares of residuals and divides it by the total number of data points.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"We have successfully defined our model and the loss function, but surprisingly, we haven't used Flux anywhere till now. Let's see how we can write the same code using Flux. ","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> flux_model = Dense(1 => 1)\nDense(1 => 1)       # 2 parameters","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"A Dense(1 => 1) layer denotes a layer of one neuron with one input (one feature) and one output. This layer is exactly same as the mathematical model defined by us above! Under the hood, Flux too calculates the output using the same expression! But, we don't have to initialize the parameters ourselves this time, instead Flux does it for us.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> flux_model.weight, flux_model.bias\n(Float32[1.1412252], Float32[0.0])","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Now we can check if our model is acting right. We can pass the complete data in one go, with each x having exactly one feature (one input) -","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> flux_model(x) |> size\n(1, 61)\n\njulia> flux_model(x)[1], y[1]\n(-1.8525281f0, -7.0f0)","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"It is! The next step would be defining the loss function using Flux's functions -","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> function flux_loss(flux_model, x, y)\n           Å· = flux_model(x)\n           Flux.mse(Å·, y)\n       end;\n\njulia> flux_loss(flux_model, x, y)\n22.74856f0","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Everything works as before! It almost feels like Flux provides us with smart wrappers for the functions we could have written on our own. Now, as the last step of this section, let's see how different the flux_model is from our custom_model. A good way to go about this would be to fix the parameters of both models to be the same. Let's change the parameters of our custom_model to match that of the flux_model -","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> W = Float32[1.1412252]\n1-element Vector{Float32}:\n 1.1412252","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"To check how both the models are performing on the data, let's find out the losses using the loss and flux_loss functions -","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> custom_loss(W, b, x, y), flux_loss(flux_model, x, y)\n(22.74856f0, 22.74856f0)","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"The losses are identical! This means that our model and the flux_model are identical on some level, and the loss functions are completely identical! The difference in models would be that Flux's Dense layer supports many other arguments that can be used to customize the layer further. But, for this tutorial, let us stick to our simple custom_model.","category":"page"},{"location":"tutorials/linear_regression/#Training-the-model","page":"Linear Regression","title":"Training the model","text":"","category":"section"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Let's train our model using the classic Gradient Descent algorithm. According to the gradient descent algorithm, the weights and biases should be iteratively updated using the following mathematical equations -","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"beginaligned\nW = W - eta * fracdLdW \nb = b - eta * fracdLdb\nendaligned","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Here, W is the weight matrix, b is the bias vector, eta is the learning rate, fracdLdW is the derivative of the loss function with respect to the weight, and fracdLdb is the derivative of the loss function with respect to the bias.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"The derivatives are calculated using an Automatic Differentiation tool, and Flux uses Zygote.jl for the same. Since Zygote.jl is an independent Julia package, it can be used outside of Flux as well! Refer to the documentation of Zygote.jl for more information on the same.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Our first step would be to obtain the gradient of the loss function with respect to the weights and the biases. Flux re-exports Zygote's gradient function; hence, we don't need to import Zygote explicitly to use the functionality.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> dLdW, dLdb, _, _ = gradient(custom_loss, W, b, x, y);","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"We can now update the parameters, following the gradient descent algorithm -","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> W .= W .- 0.1 .* dLdW\n1-element Vector{Float32}:\n 1.8144473\n\njulia> b .= b .- 0.1 .* dLdb\n1-element Vector{Float32}:\n 0.41325632","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"The parameters have been updated! We can now check the value of the loss function -","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> custom_loss(W, b, x, y)\n17.157953f0","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"The loss went down! This means that we successfully trained our model for one epoch. We can plug the training code written above into a loop and train the model for a higher number of epochs. It can be customized either to have a fixed number of epochs or to stop when certain conditions are met, for example, change in loss < 0.1. The loop can be tailored to suit the user's needs, and the conditions can be specified in plain Julia!","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Let's plug our super training logic inside a function and test it again -","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> function train_custom_model()\n           dLdW, dLdb, _, _ = gradient(custom_loss, W, b, x, y)\n           @. W = W - 0.1 * dLdW\n           @. b = b - 0.1 * dLdb\n       end;\n\njulia> train_custom_model();\n\njulia> W, b, custom_loss(W, b, x, y)\n(Float32[2.340657], Float32[0.7516814], 13.64972f0)","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"It works, and the loss went down again! This was the second epoch of our training procedure. Let's plug this in a for loop and train the model for 30 epochs.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> for i = 1:40\n          train_custom_model()\n       end\n\njulia> W, b, custom_loss(W, b, x, y)\n(Float32[4.2422233], Float32[2.2460847], 7.6680417f0)","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"There was a significant reduction in loss, and the parameters were updated!","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"We can train the model even more or tweak the hyperparameters to achieve the desired result faster, but let's stop here. We trained our model for 42 epochs, and loss went down from 22.74856 to 7.6680417f. Time for some visualization!","category":"page"},{"location":"tutorials/linear_regression/#Results","page":"Linear Regression","title":"Results","text":"","category":"section"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"The main objective of this tutorial was to fit a line to our dataset using the linear regression algorithm. The training procedure went well, and the loss went down significantly! Let's see what the fitted line looks like. Remember, Wx + b is nothing more than a line's equation, with slope = W[1] and y-intercept = b[1] (indexing at 1 as W and b are iterable).","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Plotting the line and the data points using Plot.jl -","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> plot(reshape(x, (61, 1)), reshape(y, (61, 1)), lw = 3, seriestype = :scatter, label = \"\", title = \"Simple Linear Regression\", xlabel = \"x\", ylabel= \"y\");\n\njulia> plot!((x) -> b[1] + W[1] * x, -3, 3, label=\"Custom model\", lw=2);","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"(Image: linear-regression-line)","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"The line fits well! There is room for improvement, but we leave that up to you! You can play with the optimisers, the number of epochs, learning rate, etc. to improve the fitting and reduce the loss!","category":"page"},{"location":"tutorials/linear_regression/#Linear-regression-model-on-a-real-dataset","page":"Linear Regression","title":"Linear regression model on a real dataset","text":"","category":"section"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"We now move on to a relatively complex linear regression model. Here we will use a real dataset from MLDatasets.jl, which will not confine our data points to have only one feature. Let's start by importing the required packages -","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> using Flux, Statistics, MLDatasets, DataFrames","category":"page"},{"location":"tutorials/linear_regression/#Gathering-real-data","page":"Linear Regression","title":"Gathering real data","text":"","category":"section"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Let's start by initializing our dataset. We will be using the BostonHousing dataset consisting of 506 data points. Each of these data points has 13 features and a corresponding label, the house's price. The xs are still mapped to a single y, but now, a single x data point has 13 features. ","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> dataset = BostonHousing();\n\njulia> x, y = BostonHousing(as_df=false)[:];","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"We can now split the obtained data into training and testing data -","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> x_train, x_test, y_train, y_test = x[:, 1:400], x[:, 401:end], y[:, 1:400], y[:, 401:end];\n\njulia> x_train |> size, x_test |> size, y_train |> size, y_test |> size\n((13, 400), (13, 106), (1, 400), (1, 106))","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"This data contains a diverse number of features, which means that the features have different scales. A wise option here would be to normalise the data, making the training process more efficient and fast. Let's check the standard deviation of the training data before normalising it.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> std(x_train)\n134.06784844377117","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"The data is indeed not normalised. We can use the Flux.normalise function to normalise the training data.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> x_train_n = Flux.normalise(x_train);\n\njulia> std(x_train_n)\n1.0000843694328236","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"The standard deviation is now close to one! Our data is ready!","category":"page"},{"location":"tutorials/linear_regression/#Building-a-Flux-model","page":"Linear Regression","title":"Building a Flux model","text":"","category":"section"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"We can now directly use Flux and let it do all the work internally! Let's define a model that takes in 13 inputs (13 features) and gives us a single output (the label). We will then pass our entire data through this model in one go, and Flux will handle everything for us! Remember, we could have declared a model in plain Julia as well. The model will have 14 parameters: 13 weights and 1 bias.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> model = Dense(13 => 1)\nDense(13 => 1)      # 14 parameters","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Same as before, our next step would be to define a loss function to quantify our accuracy somehow. The lower the loss, the better the model!","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> function loss(model, x, y)\n           Å· = model(x)\n           Flux.mse(Å·, y)\n       end;\n\njulia> loss(model, x_train_n, y_train)\n676.165591625047","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"We can now proceed to the training phase!","category":"page"},{"location":"tutorials/linear_regression/#Training-the-Flux-model","page":"Linear Regression","title":"Training the Flux model","text":"","category":"section"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"The training procedure would make use of the same mathematics, but now we can pass in the model inside the gradient call and let Flux and Zygote handle the derivatives!","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> function train_model()\n           dLdm, _, _ = gradient(loss, model, x_train_n, y_train)\n           @. model.weight = model.weight - 0.000001 * dLdm.weight\n           @. model.bias = model.bias - 0.000001 * dLdm.bias\n       end;","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Contrary to our last training procedure, let's say that this time we don't want to hardcode the number of epochs. We want the training procedure to stop when the loss converges, that is, when change in loss < Î´. The quantity Î´ can be altered according to a user's need, but let's fix it to 10â»Â³ for this tutorial.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"We can write such custom training loops effortlessly using Flux and plain Julia!","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> loss_init = Inf;\n\njulia> while true\n           train_model()\n           if loss_init == Inf\n               loss_init = loss(model, x_train_n, y_train)\n               continue\n           end\n           if abs(loss_init - loss(model, x_train_n, y_train)) < 1e-4\n               break\n           else\n               loss_init = loss(model, x_train_n, y_train)\n           end\n       end;","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"The code starts by initializing an initial value for the loss, infinity. Next, it runs an infinite loop that breaks if change in loss < 10â»Â³, or the code changes the value of loss_init to the current loss and moves on to the next iteration.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"This custom loop works! This shows how easily a user can write down any custom training routine using Flux and Julia!","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Let's have a look at the loss -","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> loss(model, x_train_n, y_train)\n27.127200028562164","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"The loss went down significantly! It can be minimized further by choosing an even smaller Î´.","category":"page"},{"location":"tutorials/linear_regression/#Testing-the-Flux-model","page":"Linear Regression","title":"Testing the Flux model","text":"","category":"section"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"The last step of this tutorial would be to test our model using the testing data. We will first normalise the testing data and then calculate the corresponding loss.","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"julia> x_test_n = Flux.normalise(x_test);\n\njulia> loss(model, x_test_n, y_test)\n66.91014769713368","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"The loss is not as small as the loss of the training data, but it looks good! This also shows that our model is not overfitting!","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Summarising this tutorial, we started by generating a random yet correlated dataset for our custom model. We then saw how a simple linear regression model could be built with and without Flux, and how they were almost identical. ","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"Next, we trained the model by manually writing down the Gradient Descent algorithm and optimising the loss. We also saw how Flux provides various wrapper functionalities and keeps the API extremely intuitive and simple for the users. ","category":"page"},{"location":"tutorials/linear_regression/","page":"Linear Regression","title":"Linear Regression","text":"After getting familiar with the basics of Flux and Julia, we moved ahead to build a machine learning model for a real dataset. We repeated the exact same steps, but this time with a lot more features and data points, and by harnessing Flux's full capabilities. In the end, we developed a training loop that was smarter than the hardcoded one and ran the model on our normalised dataset to conclude the tutorial.","category":"page"},{"location":"#Flux:-The-Julia-Machine-Learning-Library","page":"Welcome","title":"Flux: The Julia Machine Learning Library","text":"","category":"section"},{"location":"","page":"Welcome","title":"Welcome","text":"Flux is a library for machine learning. It comes \"batteries-included\" with many useful tools built in, but also lets you use the full power of the Julia language where you need it. We follow a few key principles:","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"Doing the obvious thing. Flux has relatively few explicit APIs for features like regularisation or embeddings. Instead, writing down the mathematical form will work â€“ and be fast.\nExtensible by default. Flux is written to be highly extensible and flexible while being performant. Extending Flux is as simple as using your own code as part of the model you want - it is all high-level Julia code. When in doubt, itâ€™s well worth looking at the source. If you need something different, you can easily roll your own.\nPlay nicely with others. Flux works well with Julia libraries from images to differential equation solvers, so you can easily build complex data processing pipelines that integrate Flux models.","category":"page"},{"location":"#Installation","page":"Welcome","title":"Installation","text":"","category":"section"},{"location":"","page":"Welcome","title":"Welcome","text":"Download Julia 1.6 or later, preferably the current stable release. You can add Flux using Julia's package manager, by typing ] add Flux in the Julia prompt.","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"This will automatically install several other packages, including CUDA.jl which supports Nvidia GPUs. To directly access some of its functionality, you may want to add ] add CUDA too. The page on GPU support has more details.","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"Other closely associated packages, also installed automatically, include Zygote, Optimisers, NNlib, Functors and MLUtils.","category":"page"},{"location":"#Learning-Flux","page":"Welcome","title":"Learning Flux","text":"","category":"section"},{"location":"","page":"Welcome","title":"Welcome","text":"The quick start page trains a simple neural network.","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"This rest of this documentation provides a from-scratch introduction to Flux's take on models and how they work, starting with fitting a line. Once you understand these docs, congratulations, you also understand Flux's source code, which is intended to be concise, legible and a good reference for more advanced concepts.","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"Sections with ðŸ“š contain API listings. The same text is avalable at the Julia prompt, by typing for example ?gpu.","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"If you just want to get started writing models, the model zoo gives good starting points for many common ones.","category":"page"},{"location":"#Community","page":"Welcome","title":"Community","text":"","category":"section"},{"location":"","page":"Welcome","title":"Welcome","text":"Everyone is welcome to join our community on the Julia discourse forum, or the slack chat (channel #machine-learning). If you have questions or issues we'll try to help you out.","category":"page"},{"location":"","page":"Welcome","title":"Welcome","text":"If you're interested in hacking on Flux, the source code is open and easy to understand â€“ it's all just the same Julia code you work with normally. You might be interested in our intro issues to get started, or our contributing guide.","category":"page"},{"location":"models/basics/#man-basics","page":"Gradients and Layers","title":"How Flux Works: Gradients and Layers","text":"","category":"section"},{"location":"models/basics/#Taking-Gradients","page":"Gradients and Layers","title":"Taking Gradients","text":"","category":"section"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"Flux's core feature is taking gradients of Julia code. The gradient function takes another Julia function f and a set of arguments, and returns the gradient with respect to each argument. (It's a good idea to try pasting these examples in the Julia terminal.)","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"julia> using Flux\n\njulia> f(x) = 3x^2 + 2x + 1;\n\njulia> df(x) = gradient(f, x)[1]; # df/dx = 6x + 2\n\njulia> df(2)\n14.0\n\njulia> d2f(x) = gradient(df, x)[1]; # dÂ²f/dxÂ² = 6\n\njulia> d2f(2)\n6.0","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"When a function has many parameters, we can get gradients of each one at the same time:","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"julia> f(x, y) = sum((x .- y).^2);\n\njulia> gradient(f, [2, 1], [2, 0])\n([0.0, 2.0], [-0.0, -2.0])","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"These gradients are based on x and y. Flux works by instead taking gradients based on the weights and biases that make up the parameters of a model. ","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"Machine learning often can have hundreds of parameters, so Flux lets you work with collections of parameters, via the params functions. You can get the gradient of all parameters used in a program without explicitly passing them in.","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"julia> x = [2, 1];\n\njulia> y = [2, 0];\n\njulia> gs = gradient(Flux.params(x, y)) do\n         f(x, y)\n       end\nGrads(...)\n\njulia> gs[x]\n2-element Vector{Float64}:\n 0.0\n 2.0\n\njulia> gs[y]\n2-element Vector{Float64}:\n -0.0\n -2.0","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"Here, gradient takes a zero-argument function; no arguments are necessary because the params tell it what to differentiate.","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"This will come in really handy when dealing with big, complicated models. For now, though, let's start with something simple.","category":"page"},{"location":"models/basics/#Building-Simple-Models","page":"Gradients and Layers","title":"Building Simple Models","text":"","category":"section"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"Consider a simple linear regression, which tries to predict an output array y from an input x.","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"W = rand(2, 5)\nb = rand(2)\n\npredict(x) = W*x .+ b\n\nfunction loss(x, y)\n  Å· = predict(x)\n  sum((y .- Å·).^2)\nend\n\nx, y = rand(5), rand(2) # Dummy data\nloss(x, y) # ~ 3","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"To improve the prediction we can take the gradients of the loss with respect to W and b and perform gradient descent.","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"using Flux\n\ngs = gradient(() -> loss(x, y), Flux.params(W, b))","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"Now that we have gradients, we can pull them out and update W to train the model.","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"WÌ„ = gs[W]\n\nW .-= 0.1 .* WÌ„\n\nloss(x, y) # ~ 2.5","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"The loss has decreased a little, meaning that our prediction x is closer to the target y. If we have some data we can already try training the model.","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"All deep learning in Flux, however complex, is a simple generalisation of this example. Of course, models can look very different â€“ they might have millions of parameters or complex control flow. Let's see how Flux handles more complex models.","category":"page"},{"location":"models/basics/#Building-Layers","page":"Gradients and Layers","title":"Building Layers","text":"","category":"section"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"It's common to create more complex models than the linear regression above. For example, we might want to have two linear layers with a nonlinearity like sigmoid (Ïƒ) in between them. In the above style we could write this as:","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"using Flux\n\nW1 = rand(3, 5)\nb1 = rand(3)\nlayer1(x) = W1 * x .+ b1\n\nW2 = rand(2, 3)\nb2 = rand(2)\nlayer2(x) = W2 * x .+ b2\n\nmodel(x) = layer2(Ïƒ.(layer1(x)))\n\nmodel(rand(5)) # => 2-element vector","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"This works but is fairly unwieldy, with a lot of repetition â€“ especially as we add more layers. One way to factor this out is to create a function that returns linear layers.","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"function linear(in, out)\n  W = randn(out, in)\n  b = randn(out)\n  x -> W * x .+ b\nend\n\nlinear1 = linear(5, 3) # we can access linear1.W etc\nlinear2 = linear(3, 2)\n\nmodel(x) = linear2(Ïƒ.(linear1(x)))\n\nmodel(rand(5)) # => 2-element vector","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"Another (equivalent) way is to create a struct that explicitly represents the affine layer.","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"struct Affine\n  W\n  b\nend\n\nAffine(in::Integer, out::Integer) =\n  Affine(randn(out, in), randn(out))\n\n# Overload call, so the object can be used as a function\n(m::Affine)(x) = m.W * x .+ m.b\n\na = Affine(10, 5)\n\na(rand(10)) # => 5-element vector","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"Congratulations! You just built the Dense layer that comes with Flux. Flux has many interesting layers available, but they're all things you could have built yourself very easily.","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"(There is one small difference with Dense â€“ for convenience it also takes an activation function, like Dense(10 => 5, Ïƒ).)","category":"page"},{"location":"models/basics/#Stacking-It-Up","page":"Gradients and Layers","title":"Stacking It Up","text":"","category":"section"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"It's pretty common to write models that look something like:","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"layer1 = Dense(10 => 5, Ïƒ)\n# ...\nmodel(x) = layer3(layer2(layer1(x)))","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"For long chains, it might be a bit more intuitive to have a list of layers, like this:","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"using Flux\n\nlayers = [Dense(10 => 5, Ïƒ), Dense(5 => 2), softmax]\n\nmodel(x) = foldl((x, m) -> m(x), layers, init = x)\n\nmodel(rand(10)) # => 2-element vector","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"Handily, this is also provided for in Flux:","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"model2 = Chain(\n  Dense(10 => 5, Ïƒ),\n  Dense(5 => 2),\n  softmax)\n\nmodel2(rand(10)) # => 2-element vector","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"This quickly starts to look like a high-level deep learning library; yet you can see how it falls out of simple abstractions, and we lose none of the power of Julia code.","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"A nice property of this approach is that because \"models\" are just functions (possibly with trainable parameters), you can also see this as simple function composition.","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"m = Dense(5 => 2) âˆ˜ Dense(10 => 5, Ïƒ)\n\nm(rand(10))","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"Likewise, Chain will happily work with any Julia function.","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"m = Chain(x -> x^2, x -> x+1)\n\nm(5) # => 26","category":"page"},{"location":"models/basics/#Layer-Helpers","page":"Gradients and Layers","title":"Layer Helpers","text":"","category":"section"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"There is still one problem with this Affine layer, that Flux does not know to look inside it. This means that Flux.train! won't see its parameters, nor will gpu be able to move them to your GPU. These features are enabled by the @functor macro:","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"Flux.@functor Affine","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"Finally, most Flux layers make bias optional, and allow you to supply the function used for generating random weights. We can easily add these refinements to the Affine layer as follows:","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"function Affine((in, out)::Pair; bias=true, init=Flux.randn32)\n  W = init(out, in)\n  b = Flux.create_bias(W, bias, out)\n  Affine(W, b)\nend\n\nAffine(3 => 1, bias=false, init=ones) |> gpu","category":"page"},{"location":"models/basics/","page":"Gradients and Layers","title":"Gradients and Layers","text":"Functors.@functor\nFlux.create_bias","category":"page"},{"location":"models/basics/#Functors.@functor","page":"Gradients and Layers","title":"Functors.@functor","text":"@functor T\n@functor T (x,)\n\nAdds methods to functor allowing recursion into objects of type T, and reconstruction. Assumes that T has a constructor accepting all of its fields, which is true unless you have provided an inner constructor which does not.\n\nBy default all fields of T are considered children;  this can be restricted be restructed by providing a tuple of field names.\n\nExamples\n\njulia> struct Foo; x; y; end\n\njulia> @functor Foo\n\njulia> Functors.children(Foo(1,2))\n(x = 1, y = 2)\n\njulia> _, re = Functors.functor(Foo(1,2));\n\njulia> re((10, 20))\nFoo(10, 20)\n\njulia> struct TwoThirds a; b; c; end\n\njulia> @functor TwoThirds (a, c)\n\njulia> ch2, re3 = Functors.functor(TwoThirds(10,20,30));\n\njulia> ch2\n(a = 10, c = 30)\n\njulia> re3((\"ten\", \"thirty\"))\nTwoThirds(\"ten\", 20, \"thirty\")\n\njulia> fmap(x -> 10x, TwoThirds(Foo(1,2), Foo(3,4), 56))\nTwoThirds(Foo(10, 20), Foo(3, 4), 560)\n\n\n\n\n\n","category":"macro"},{"location":"models/basics/#Flux.create_bias","page":"Gradients and Layers","title":"Flux.create_bias","text":"create_bias(weights, bias, size...)\n\nReturn a bias parameter for a layer, based on the value given to the constructor's keyword bias=bias.\n\nbias == true creates a trainable array of the given size, of the same type as weights, initialised to zero.\nbias == false returns false, which is understood by AD to be non-differentiable.\nbias::AbstractArray uses the array provided, provided it has the correct size. It does not at present correct the eltype to match that of weights.\n\n\n\n\n\n","category":"function"},{"location":"data/onehot/#One-Hot-Encoding-with-OneHotArrays.jl","page":"OneHotArrays.jl ðŸ“š (onehot, ...)","title":"One-Hot Encoding with OneHotArrays.jl","text":"","category":"section"},{"location":"data/onehot/","page":"OneHotArrays.jl ðŸ“š (onehot, ...)","title":"OneHotArrays.jl ðŸ“š (onehot, ...)","text":"It's common to encode categorical variables (like true, false or cat, dog) in \"one-of-k\" or \"one-hot\" form. OneHotArrays.jl provides the onehot function to make this easy.","category":"page"},{"location":"data/onehot/","page":"OneHotArrays.jl ðŸ“š (onehot, ...)","title":"OneHotArrays.jl ðŸ“š (onehot, ...)","text":"julia> using OneHotArrays\n\njulia> onehot(:b, [:a, :b, :c])\n3-element OneHotVector(::UInt32) with eltype Bool:\n â‹…\n 1\n â‹…\n\njulia> onehot(:c, [:a, :b, :c])\n3-element OneHotVector(::UInt32) with eltype Bool:\n â‹…\n â‹…\n 1","category":"page"},{"location":"data/onehot/","page":"OneHotArrays.jl ðŸ“š (onehot, ...)","title":"OneHotArrays.jl ðŸ“š (onehot, ...)","text":"The inverse is onecold (which can take a general probability distribution, as well as just booleans).","category":"page"},{"location":"data/onehot/","page":"OneHotArrays.jl ðŸ“š (onehot, ...)","title":"OneHotArrays.jl ðŸ“š (onehot, ...)","text":"julia> onecold(ans, [:a, :b, :c])\n:c\n\njulia> onecold([true, false, false], [:a, :b, :c])\n:a\n\njulia> onecold([0.3, 0.2, 0.5], [:a, :b, :c])\n:c","category":"page"},{"location":"data/onehot/","page":"OneHotArrays.jl ðŸ“š (onehot, ...)","title":"OneHotArrays.jl ðŸ“š (onehot, ...)","text":"For multiple samples at once, onehotbatch creates a batch (matrix) of one-hot vectors, and onecold treats matrices as batches.","category":"page"},{"location":"data/onehot/","page":"OneHotArrays.jl ðŸ“š (onehot, ...)","title":"OneHotArrays.jl ðŸ“š (onehot, ...)","text":"julia> using OneHotArrays\n\njulia> onehotbatch([:b, :a, :b], [:a, :b, :c])\n3Ã—3 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n â‹…  1  â‹…\n 1  â‹…  1\n â‹…  â‹…  â‹…\n\njulia> onecold(ans, [:a, :b, :c])\n3-element Vector{Symbol}:\n :b\n :a\n :b","category":"page"},{"location":"data/onehot/","page":"OneHotArrays.jl ðŸ“š (onehot, ...)","title":"OneHotArrays.jl ðŸ“š (onehot, ...)","text":"Note that these operations returned OneHotVector and OneHotMatrix rather than Arrays. OneHotVectors behave like normal vectors but avoid any unnecessary cost compared to using an integer index directly. For example, multiplying a matrix with a one-hot vector simply slices out the relevant row of the matrix under the hood.","category":"page"},{"location":"data/onehot/#Function-listing","page":"OneHotArrays.jl ðŸ“š (onehot, ...)","title":"Function listing","text":"","category":"section"},{"location":"data/onehot/","page":"OneHotArrays.jl ðŸ“š (onehot, ...)","title":"OneHotArrays.jl ðŸ“š (onehot, ...)","text":"OneHotArrays.onehot\nOneHotArrays.onecold\nOneHotArrays.onehotbatch\nOneHotArrays.OneHotVector\nOneHotArrays.OneHotMatrix","category":"page"},{"location":"data/onehot/#OneHotArrays.onehot","page":"OneHotArrays.jl ðŸ“š (onehot, ...)","title":"OneHotArrays.onehot","text":"onehot(x, labels, [default])\n\nReturns a OneHotVector which is roughly a sparse representation of x .== labels.\n\nInstead of storing say Vector{Bool}, it stores the index of the first occurrence  of x in labels. If x is not found in labels, then it either returns onehot(default, labels), or gives an error if no default is given.\n\nSee also onehotbatch to apply this to many xs,  and onecold to reverse either of these, as well as to generalise argmax.\n\nExamples\n\njulia> Î² = onehot(:b, (:a, :b, :c))\n3-element OneHotVector(::UInt32) with eltype Bool:\n â‹…\n 1\n â‹…\n\njulia> Î±Î²Î³ = (onehot(0, 0:2), Î², onehot(:z, [:a, :b, :c], :c))  # uses default\n(Bool[1, 0, 0], Bool[0, 1, 0], Bool[0, 0, 1])\n\njulia> hcat(Î±Î²Î³...)  # preserves sparsity\n3Ã—3 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n 1  â‹…  â‹…\n â‹…  1  â‹…\n â‹…  â‹…  1\n\n\n\n\n\n","category":"function"},{"location":"data/onehot/#OneHotArrays.onecold","page":"OneHotArrays.jl ðŸ“š (onehot, ...)","title":"OneHotArrays.onecold","text":"onecold(y::AbstractArray, labels = 1:size(y,1))\n\nRoughly the inverse operation of onehot or onehotbatch:  This finds the index of the largest element of y, or each column of y,  and looks them up in labels.\n\nIf labels are not specified, the default is integers 1:size(y,1) â€“ the same operation as argmax(y, dims=1) but sometimes a different return type.\n\nExamples\n\njulia> onecold([false, true, false])\n2\n\njulia> onecold([0.3, 0.2, 0.5], (:a, :b, :c))\n:c\n\njulia> onecold([ 1  0  0  1  0  1  0  1  0  0  1\n                 0  1  0  0  0  0  0  0  1  0  0\n                 0  0  0  0  1  0  0  0  0  0  0\n                 0  0  0  0  0  0  1  0  0  0  0\n                 0  0  1  0  0  0  0  0  0  1  0 ], 'a':'e') |> String\n\"abeacadabea\"\n\n\n\n\n\n","category":"function"},{"location":"data/onehot/#OneHotArrays.onehotbatch","page":"OneHotArrays.jl ðŸ“š (onehot, ...)","title":"OneHotArrays.onehotbatch","text":"onehotbatch(xs, labels, [default])\n\nReturns a OneHotMatrix where kth column of the matrix is onehot(xs[k], labels). This is a sparse matrix, which stores just a Vector{UInt32} containing the indices of the nonzero elements.\n\nIf one of the inputs in xs is not found in labels, that column is onehot(default, labels) if default is given, else an error.\n\nIf xs has more dimensions, N = ndims(xs) > 1, then the result is an  AbstractArray{Bool, N+1} which is one-hot along the first dimension,  i.e. result[:, k...] == onehot(xs[k...], labels).\n\nNote that xs can be any iterable, such as a string. And that using a tuple for labels will often speed up construction, certainly for less than 32 classes.\n\nExamples\n\njulia> oh = onehotbatch(\"abracadabra\", 'a':'e', 'e')\n5Ã—11 OneHotMatrix(::Vector{UInt32}) with eltype Bool:\n 1  â‹…  â‹…  1  â‹…  1  â‹…  1  â‹…  â‹…  1\n â‹…  1  â‹…  â‹…  â‹…  â‹…  â‹…  â‹…  1  â‹…  â‹…\n â‹…  â‹…  â‹…  â‹…  1  â‹…  â‹…  â‹…  â‹…  â‹…  â‹…\n â‹…  â‹…  â‹…  â‹…  â‹…  â‹…  1  â‹…  â‹…  â‹…  â‹…\n â‹…  â‹…  1  â‹…  â‹…  â‹…  â‹…  â‹…  â‹…  1  â‹…\n\njulia> reshape(1:15, 3, 5) * oh  # this matrix multiplication is done efficiently\n3Ã—11 Matrix{Int64}:\n 1  4  13  1  7  1  10  1  4  13  1\n 2  5  14  2  8  2  11  2  5  14  2\n 3  6  15  3  9  3  12  3  6  15  3\n\n\n\n\n\n","category":"function"},{"location":"data/onehot/#OneHotArrays.OneHotVector","page":"OneHotArrays.jl ðŸ“š (onehot, ...)","title":"OneHotArrays.OneHotVector","text":"OneHotVector{T} = OneHotArray{T, 0, 1, T}\nOneHotVector(indices, L)\n\nA one-hot vector with L labels (i.e. length(A) == L and count(A) == 1) typically constructed by onehot. Stored efficiently as a single index of type T, usually UInt32.\n\n\n\n\n\n","category":"type"},{"location":"data/onehot/#OneHotArrays.OneHotMatrix","page":"OneHotArrays.jl ðŸ“š (onehot, ...)","title":"OneHotArrays.OneHotMatrix","text":"OneHotMatrix{T, I} = OneHotArray{T, 1, 2, I}\nOneHotMatrix(indices, L)\n\nA one-hot matrix (with L labels) typically constructed using onehotbatch. Stored efficiently as a vector of indices with type I and eltype T.\n\n\n\n\n\n","category":"type"}]
}
