<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>NNlib · Flux</title><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-36890222-9', 'auto');
ga('send', 'pageview', {'page': location.pathname + location.search + location.hash});
</script><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/flux.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">Flux</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Building Models</span><ul><li><a class="tocitem" href="../overview/">Overview</a></li><li><a class="tocitem" href="../basics/">Basics</a></li><li><a class="tocitem" href="../recurrence/">Recurrence</a></li><li><a class="tocitem" href="../layers/">Model Reference</a></li><li><a class="tocitem" href="../losses/">Loss Functions</a></li><li><a class="tocitem" href="../regularisation/">Regularisation</a></li><li><a class="tocitem" href="../advanced/">Advanced Model Building</a></li><li class="is-active"><a class="tocitem" href>NNlib</a><ul class="internal"><li><a class="tocitem" href="#Activation-Functions"><span>Activation Functions</span></a></li><li><a class="tocitem" href="#Softmax"><span>Softmax</span></a></li><li><a class="tocitem" href="#Pooling"><span>Pooling</span></a></li><li><a class="tocitem" href="#Convolution"><span>Convolution</span></a></li><li><a class="tocitem" href="#Upsampling"><span>Upsampling</span></a></li><li><a class="tocitem" href="#Batched-Operations"><span>Batched Operations</span></a></li><li><a class="tocitem" href="#Gather-and-Scatter"><span>Gather and Scatter</span></a></li></ul></li><li><a class="tocitem" href="../functors/">Functors</a></li></ul></li><li><span class="tocitem">Handling Data</span><ul><li><a class="tocitem" href="../../data/onehot/">One-Hot Encoding</a></li><li><a class="tocitem" href="../../data/dataloader/">DataLoader</a></li></ul></li><li><span class="tocitem">Training Models</span><ul><li><a class="tocitem" href="../../training/optimisers/">Optimisers</a></li><li><a class="tocitem" href="../../training/training/">Training</a></li></ul></li><li><a class="tocitem" href="../../gpu/">GPU Support</a></li><li><a class="tocitem" href="../../saving/">Saving &amp; Loading</a></li><li><a class="tocitem" href="../../ecosystem/">The Julia Ecosystem</a></li><li><a class="tocitem" href="../../utilities/">Utility Functions</a></li><li><a class="tocitem" href="../../performance/">Performance Tips</a></li><li><a class="tocitem" href="../../datasets/">Datasets</a></li><li><a class="tocitem" href="../../community/">Community</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Building Models</a></li><li class="is-active"><a href>NNlib</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>NNlib</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/FluxML/Flux.jl/blob/master/docs/src/models/nnlib.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="NNlib"><a class="docs-heading-anchor" href="#NNlib">NNlib</a><a id="NNlib-1"></a><a class="docs-heading-anchor-permalink" href="#NNlib" title="Permalink"></a></h1><p>Flux re-exports all of the functions exported by the <a href="https://github.com/FluxML/NNlib.jl">NNlib</a> package.</p><h2 id="Activation-Functions"><a class="docs-heading-anchor" href="#Activation-Functions">Activation Functions</a><a id="Activation-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Activation-Functions" title="Permalink"></a></h2><p>Non-linearities that go between layers of your model. Note that, unless otherwise stated, activation functions operate on scalars. To apply them to an array you can call <code>σ.(xs)</code>, <code>relu.(xs)</code> and so on.</p><article class="docstring"><header><a class="docstring-binding" id="NNlib.celu" href="#NNlib.celu"><code>NNlib.celu</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">celu(x, α=1) = x ≥ 0 ? x : α * (exp(x/α) - 1)</code></pre><p>Continuously Differentiable Exponential Linear Units See <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.elu" href="#NNlib.elu"><code>NNlib.elu</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">elu(x, α=1) = x &gt; 0 ? x : α * (exp(x) - 1)</code></pre><p>Exponential Linear Unit activation function. See <a href="https://arxiv.org/abs/1511.07289">Fast and Accurate Deep Network Learning by Exponential Linear Units</a>. You can also specify the coefficient explicitly, e.g. <code>elu(x, 1)</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.gelu" href="#NNlib.gelu"><code>NNlib.gelu</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">gelu(x) = 0.5x * (1 + tanh(√(2/π) * (x + 0.044715x^3)))</code></pre><p><a href="https://arxiv.org/abs/1606.08415">Gaussian Error Linear Unit</a> activation function.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.hardsigmoid" href="#NNlib.hardsigmoid"><code>NNlib.hardsigmoid</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">hardσ(x) = max(0, min(1, (x + 3) / 6)</code></pre><p>Piecewise linear approximation of sigmoid.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.hardtanh" href="#NNlib.hardtanh"><code>NNlib.hardtanh</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">hardtanh(x) = max(-1, min(1, x))</code></pre><p>Segment-wise linear approximation of tanh. Cheaper  and  more  computational  efficient version of tanh. See <a href="https://ronan.collobert.com/pub/matos/2004_phdthesis_lip6.pdf">Large Scale Machine Learning</a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.leakyrelu" href="#NNlib.leakyrelu"><code>NNlib.leakyrelu</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">leakyrelu(x, a=0.01) = max(a*x, x)</code></pre><p>Leaky <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">Rectified Linear Unit</a> activation function. You can also specify the coefficient explicitly, e.g. <code>leakyrelu(x, 0.01)</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.lisht" href="#NNlib.lisht"><code>NNlib.lisht</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">lisht(x) = x * tanh(x)</code></pre><p>Non-Parametric Linearly Scaled Hyperbolic Tangent Activation Function. See <a href="https://arxiv.org/abs/1901.05894">LiSHT</a></p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.logcosh" href="#NNlib.logcosh"><code>NNlib.logcosh</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">logcosh(x)</code></pre><p>Return <code>log(cosh(x))</code> which is computed in a numerically stable way.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.logsigmoid" href="#NNlib.logsigmoid"><code>NNlib.logsigmoid</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">logσ(x)</code></pre><p>Return <code>log(σ(x))</code> which is computed in a numerically stable way.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.mish" href="#NNlib.mish"><code>NNlib.mish</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">mish(x) = x * tanh(softplus(x))</code></pre><p>Self Regularized Non-Monotonic Neural Activation Function. See <a href="https://arxiv.org/abs/1908.08681">Mish: A Self Regularized Non-Monotonic Neural Activation Function</a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.relu" href="#NNlib.relu"><code>NNlib.relu</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">relu(x) = max(0, x)</code></pre><p><a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">Rectified Linear Unit</a> activation function.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.relu6" href="#NNlib.relu6"><code>NNlib.relu6</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">relu6(x) = min(max(0, x), 6)</code></pre><p><a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">Rectified Linear Unit</a> activation function capped at 6. See <a href="https://www.cs.toronto.edu/~kriz/conv-cifar10-aug2010.pdf">Convolutional Deep Belief Networks on CIFAR-10</a></p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.rrelu" href="#NNlib.rrelu"><code>NNlib.rrelu</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">rrelu(x, l=1/8, u=1/3) = max(a*x, x)

a = randomly sampled from uniform distribution U(l, u)</code></pre><p>Randomized Leaky <a href="https://arxiv.org/abs/1505.00853">Rectified Linear Unit</a> activation function. You can also specify the bound explicitly, e.g. <code>rrelu(x, 0.0, 1.0)</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.selu" href="#NNlib.selu"><code>NNlib.selu</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">selu(x) = λ * (x ≥ 0 ? x : α * (exp(x) - 1))

λ ≈ 1.05070...
α ≈ 1.67326...</code></pre><p>Scaled exponential linear units. See <a href="https://arxiv.org/abs/1706.02515">Self-Normalizing Neural Networks</a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.sigmoid" href="#NNlib.sigmoid"><code>NNlib.sigmoid</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">σ(x) = 1 / (1 + exp(-x))</code></pre><p>Classic <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid</a> activation function.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.softplus" href="#NNlib.softplus"><code>NNlib.softplus</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">softplus(x) = log(exp(x) + 1)</code></pre><p>See <a href="http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf">Deep Sparse Rectifier Neural Networks</a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.softshrink" href="#NNlib.softshrink"><code>NNlib.softshrink</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">softshrink(x, λ=0.5) =
    (x ≥ λ ? x - λ : (-λ ≥ x ? x + λ : 0))</code></pre><p>See <a href="https://www.gabormelli.com/RKB/Softshrink_Activation_Function">Softshrink Activation Function</a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.softsign" href="#NNlib.softsign"><code>NNlib.softsign</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">softsign(x) = x / (1 + |x|)</code></pre><p>See <a href="http://www.iro.umontreal.ca/~lisa/publications2/index.php/attachments/single/205">Quadratic Polynomials Learn Better Image Features</a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.swish" href="#NNlib.swish"><code>NNlib.swish</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">swish(x) = x * σ(x)</code></pre><p>Self-gated activation function. See <a href="https://arxiv.org/abs/1710.05941">Swish: a Self-Gated Activation Function</a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.tanhshrink" href="#NNlib.tanhshrink"><code>NNlib.tanhshrink</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">tanhshrink(x) = x - tanh(x)</code></pre><p>See <a href="https://www.gabormelli.com/RKB/Tanhshrink_Activation_Function">Tanhshrink Activation Function</a>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.trelu" href="#NNlib.trelu"><code>NNlib.trelu</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">trelu(x, theta=1) = x &gt; theta ? x : 0</code></pre><p>Threshold Gated Rectified Linear. See <a href="https://arxiv.org/abs/1402.3337">ThresholdRelu</a></p></div></section></article><h2 id="Softmax"><a class="docs-heading-anchor" href="#Softmax">Softmax</a><a id="Softmax-1"></a><a class="docs-heading-anchor-permalink" href="#Softmax" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNlib.softmax" href="#NNlib.softmax"><code>NNlib.softmax</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">softmax(x; dims=1)</code></pre><p><a href="https://en.wikipedia.org/wiki/Softmax_function">Softmax</a> turns input array <code>x</code> into probability distributions that sum to 1 along the dimensions specified by <code>dims</code>. It is semantically equivalent to the following:</p><pre><code class="language-none">softmax(x; dims=1) = exp.(x) ./ sum(exp.(x), dims=dims)</code></pre><p>with additional manipulations enhancing numerical stability.</p><p>For a matrix input <code>x</code> it will by default (<code>dims=1</code>) treat it as a batch of vectors, with each column independent. Keyword <code>dims=2</code> will instead treat rows independently, etc...</p><p>See also <a href="#NNlib.logsoftmax"><code>logsoftmax</code></a>.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; softmax([1, 2, 3])
3-element Array{Float64,1}:
  0.0900306
  0.244728
  0.665241

julia&gt; softmax([1 2 3; 2 2 2])  # dims=1
2×3 Array{Float64,2}:
 0.268941  0.5  0.731059
 0.731059  0.5  0.268941

julia&gt; softmax([1 2 3; 2 2 2]; dims=2)
2×3 Array{Float64,2}:
 0.0900306  0.244728  0.665241
 0.333333   0.333333  0.333333</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.logsoftmax" href="#NNlib.logsoftmax"><code>NNlib.logsoftmax</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">logsoftmax(x; dims=1)</code></pre><p>Computes the log of softmax in a more numerically stable way than directly taking <code>log.(softmax(xs))</code>. Commonly used in computing cross entropy loss.</p><p>It is semantically equivalent to the following:</p><pre><code class="language-none">logsoftmax(x; dims=1) = x .- log.(sum(exp.(x), dims=dims))</code></pre><p>See also <a href="#NNlib.softmax"><code>softmax</code></a>.</p></div></section></article><h2 id="Pooling"><a class="docs-heading-anchor" href="#Pooling">Pooling</a><a id="Pooling-1"></a><a class="docs-heading-anchor-permalink" href="#Pooling" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNlib.maxpool" href="#NNlib.maxpool"><code>NNlib.maxpool</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">maxpool(x, k::NTuple; pad=0, stride=k)</code></pre><p>Perform max pool operation with window size <code>k</code> on input tensor <code>x</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.meanpool" href="#NNlib.meanpool"><code>NNlib.meanpool</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">meanpool(x, k::NTuple; pad=0, stride=k)</code></pre><p>Perform mean pool operation with window size <code>k</code> on input tensor <code>x</code>.</p></div></section></article><h2 id="Convolution"><a class="docs-heading-anchor" href="#Convolution">Convolution</a><a id="Convolution-1"></a><a class="docs-heading-anchor-permalink" href="#Convolution" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNlib.conv" href="#NNlib.conv"><code>NNlib.conv</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">conv(x, w; stride = 1, pad = 0, dilation = 1, flipped = false, groups = 1)</code></pre><p>Apply convolution filter <code>w</code> to input <code>x</code>. <code>x</code> and <code>w</code> are 3d/4d/5d tensors  in 1d/2d/3d convolutions respectively. </p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.depthwiseconv" href="#NNlib.depthwiseconv"><code>NNlib.depthwiseconv</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">depthwiseconv(x, w; stride=1, pad=0, dilation=1, flipped=false)</code></pre><p>Depthwise convolution operation with filter <code>w</code> on input <code>x</code>. <code>x</code> and <code>w</code>  are 3d/4d/5d tensors in 1d/2d/3d convolutions respectively. </p></div></section></article><h2 id="Upsampling"><a class="docs-heading-anchor" href="#Upsampling">Upsampling</a><a id="Upsampling-1"></a><a class="docs-heading-anchor-permalink" href="#Upsampling" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNlib.upsample_nearest" href="#NNlib.upsample_nearest"><code>NNlib.upsample_nearest</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">upsample_nearest(x, scale::NTuple{S,Int})
upsample_nearest(x; size::NTuple{S,Int})</code></pre><p>Upsamples the array <code>x</code> by integer multiples along the first <code>S</code> dimensions. Subsequent dimensions of <code>x</code> are not altered.</p><p>Either the <code>scale</code> factors or the final output <code>size</code> can be specified.</p><p>See also <a href="#NNlib.upsample_bilinear"><code>upsample_bilinear</code></a>, for two dimensions of an <code>N=4</code> array.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl">julia&gt; upsample_nearest([1 2 3; 4 5 6], (2, 3))
4×9 Array{Int64,2}:
 1  1  1  2  2  2  3  3  3
 1  1  1  2  2  2  3  3  3
 4  4  4  5  5  5  6  6  6
 4  4  4  5  5  5  6  6  6

julia&gt; ans == upsample_nearest([1 2 3; 4 5 6]; size=(4, 9))  # equivalent
true

julia&gt; upsample_nearest([1 2 3; 4 5 6], (2,))
4×3 Array{Int64,1}:
 1  2  3
 1  2  3
 4  5  6
 4  5  6

julia&gt; ans == upsample_nearest([1 2 3; 4 5 6], size=(4,))
true</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.upsample_bilinear" href="#NNlib.upsample_bilinear"><code>NNlib.upsample_bilinear</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">upsample_bilinear(x::AbstractArray{T,4}, scale::NTuple{2,Real})
upsample_bilinear(x::AbstractArray{T,4}; size::NTuple{2,Integer})</code></pre><p>Upsamples the first 2 dimensions of the array <code>x</code> by the upsample factors stored in <code>scale</code>, using bilinear interpolation. As an alternative to using <code>scale</code>, the resulting image <code>size</code> can be directly specified with a keyword argument.</p><p>The size of the output is equal to <code>(scale[1]*S1, scale[2]*S2, S3, S4)</code>, where <code>S1, S2, S3, S4 = size(x)</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; x = reshape(Float32[1 2 3; 4 5 6], (2,3,1,1))
2×3×1×1 Array{Float32,4}:
[:, :, 1, 1] =
 1.0  2.0  3.0
 4.0  5.0  6.0

julia&gt; upsample_bilinear(x, (2, 3))
4×9×1×1 Array{Float32,4}:
[:, :, 1, 1] =
 1.0  1.25  1.5  1.75  2.0  2.25  2.5  2.75  3.0
 2.0  2.25  2.5  2.75  3.0  3.25  3.5  3.75  4.0
 3.0  3.25  3.5  3.75  4.0  4.25  4.5  4.75  5.0
 4.0  4.25  4.5  4.75  5.0  5.25  5.5  5.75  6.0

julia&gt; ans == upsample_bilinear(x; size=(4, 9))  # specify ouput size instead
true

julia&gt; upsample_bilinear(x, (2.5, 3.5))  # non-integer scaling factors are allowed
5×10×1×1 Array{Float32,4}:
[:, :, 1, 1] =
 1.0   1.22222  1.44444  1.66667  1.88889  2.11111  2.33333  2.55556  2.77778  3.0
 1.75  1.97222  2.19444  2.41667  2.63889  2.86111  3.08333  3.30556  3.52778  3.75
 2.5   2.72222  2.94444  3.16667  3.38889  3.61111  3.83333  4.05556  4.27778  4.5
 3.25  3.47222  3.69444  3.91667  4.13889  4.36111  4.58333  4.80556  5.02778  5.25
 4.0   4.22222  4.44444  4.66667  4.88889  5.11111  5.33333  5.55556  5.77778  6.0</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.pixel_shuffle" href="#NNlib.pixel_shuffle"><code>NNlib.pixel_shuffle</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">pixel_shuffle(x, r::Integer)</code></pre><p>Pixel shuffling operation, upscaling by a factor <code>r</code>.</p><p>For 4-arrays representing <code>N</code> images, the operation converts input <code>size(x) == (W, H, r^2*C, N)</code> to output of size <code>(r*W, r*H, C, N)</code>. For <code>D</code>-dimensional data, it expects <code>ndims(x) == D+2</code> with channel and batch dimensions, and divides the number of channels by <code>r^D</code>.</p><p>Used in super-resolution networks to upsample towards high resolution features. Reference: Shi et. al., &quot;Real-Time Single Image and Video Super-Resolution ...&quot;, CVPR 2016, https://arxiv.org/abs/1609.05158</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl">julia&gt; x = [10i + j + channel/10 for i in 1:2, j in 1:3, channel in 1:4, batch in 1:1]
2×3×4×1 Array{Float64,4}:
[:, :, 1, 1] =
 11.1  12.1  13.1
 21.1  22.1  23.1

[:, :, 2, 1] =
 11.2  12.2  13.2
 21.2  22.2  23.2

[:, :, 3, 1] =
 11.3  12.3  13.3
 21.3  22.3  23.3

[:, :, 4, 1] =
 11.4  12.4  13.4
 21.4  22.4  23.4

julia&gt; pixel_shuffle(x, 2)  # 4 channels used up as 2x upscaling of image dimensions
4×6×1×1 Array{Float64,4}:
[:, :, 1, 1] =
 11.1  11.3  12.1  12.3  13.1  13.3
 11.2  11.4  12.2  12.4  13.2  13.4
 21.1  21.3  22.1  22.3  23.1  23.3
 21.2  21.4  22.2  22.4  23.2  23.4

julia&gt; y = [i + channel/10 for i in 1:3, channel in 1:6, batch in 1:1]
3×6×1 Array{Float64, 3}:
[:, :, 1] =
 1.1  1.2  1.3  1.4  1.5  1.6
 2.1  2.2  2.3  2.4  2.5  2.6
 3.1  3.2  3.3  3.4  3.5  3.6

julia&gt; pixel_shuffle(y, 2)  # 1D image, with 6 channels reduced to 3
6×3×1 Array{Float64,3}:
[:, :, 1] =
 1.1  1.3  1.5
 1.2  1.4  1.6
 2.1  2.3  2.5
 2.2  2.4  2.6
 3.1  3.3  3.5
 3.2  3.4  3.6</code></pre></div></section></article><h2 id="Batched-Operations"><a class="docs-heading-anchor" href="#Batched-Operations">Batched Operations</a><a id="Batched-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Batched-Operations" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNlib.batched_mul" href="#NNlib.batched_mul"><code>NNlib.batched_mul</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">batched_mul(A, B) -&gt; C
A ⊠ B  # \boxtimes</code></pre><p>Batched matrix multiplication. Result has <code>C[:,:,k] == A[:,:,k] * B[:,:,k]</code> for all <code>k</code>. If <code>size(B,3) == 1</code> then instead <code>C[:,:,k] == A[:,:,k] * B[:,:,1]</code>, and similarly for <code>A</code>.</p><p>To transpose each matrix, apply <code>batched_transpose</code> to the array, or <code>batched_adjoint</code> for conjugate-transpose:</p><pre><code class="language-julia-repl">julia&gt; A, B = randn(2,5,17), randn(5,9,17);

julia&gt; A ⊠ B |&gt; size
(2, 9, 17)

julia&gt; batched_adjoint(A) |&gt; size
(5, 2, 17)

julia&gt; batched_mul(A, batched_adjoint(randn(9,5,17))) |&gt; size
(2, 9, 17)

julia&gt; A ⊠ randn(5,9,1) |&gt; size
(2, 9, 17)

julia&gt; batched_transpose(A) == PermutedDimsArray(A, (2,1,3))
true</code></pre><p>The equivalent <code>PermutedDimsArray</code> may be used in place of <code>batched_transpose</code>. Other permutations are also handled by BLAS, provided that the batch index <code>k</code> is not the first dimension of the underlying array. Thus <code>PermutedDimsArray(::Array, (1,3,2))</code> and <code>PermutedDimsArray(::Array, (3,1,2))</code> are fine.</p><p>However, <code>A = PermutedDimsArray(::Array, (3,2,1))</code> is not acceptable to BLAS, since the batch dimension is the contiguous one: <code>stride(A,3) == 1</code>. This will be copied, as doing so is faster than <code>batched_mul_generic!</code>.</p><p>Both this <code>copy</code> and <code>batched_mul_generic!</code> produce <code>@debug</code> messages, and setting for instance <code>ENV[&quot;JULIA_DEBUG&quot;] = NNlib</code> will display them.</p></div></section><section><div><pre><code class="language-none">batched_mul(A::Array{T,3}, B::Matrix)
batched_mul(A::Matrix, B::Array{T,3})
A ⊠ B</code></pre><p>This is always matrix-matrix multiplication, but either <code>A</code> or <code>B</code> may lack a batch index.</p><ul><li><p>When <code>B</code> is a matrix, result has <code>C[:,:,k] == A[:,:,k] * B[:,:]</code> for all <code>k</code>.</p></li><li><p>When <code>A</code> is a matrix, then <code>C[:,:,k] == A[:,:] * B[:,:,k]</code>. This can also be done by reshaping and calling <code>*</code>, for instance <code>A ⊡ B</code> using TensorCore.jl, but is implemented here using <code>batched_gemm</code> instead of <code>gemm</code>.</p></li></ul><pre><code class="language-julia-repl">julia&gt; randn(16,8,32) ⊠ randn(8,4) |&gt; size
(16, 4, 32)

julia&gt; randn(16,8,32) ⊠ randn(8,4,1) |&gt; size  # equivalent
(16, 4, 32)

julia&gt; randn(16,8) ⊠ randn(8,4,32) |&gt; size
(16, 4, 32)</code></pre><p>See also <code>batched_vec</code> to regard <code>B</code> as a batch of vectors, <code>A[:,:,k] * B[:,k]</code>.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.batched_mul!" href="#NNlib.batched_mul!"><code>NNlib.batched_mul!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">batched_mul!(C, A, B) -&gt; C
batched_mul!(C, A, B, α=1, β=0)</code></pre><p>In-place batched matrix multiplication, equivalent to <code>mul!(C[:,:,k], A[:,:,k], B[:,:,k], α, β)</code> for all <code>k</code>. If <code>size(B,3) == 1</code> then every batch uses <code>B[:,:,1]</code> instead.</p><p>This will call <code>batched_gemm!</code> whenever possible. For real arrays this means that, for <code>X ∈ [A,B,C]</code>, either <code>strides(X,1)==1</code> or <code>strides(X,2)==1</code>, the latter may be caused by <code>batched_transpose</code> or by for instance <code>PermutedDimsArray(::Array, (3,1,2))</code>. Unlike <code>batched_mul</code> this will never make a copy.</p><p>For complex arrays, the wrapper made by <code>batched_adjoint</code> must be outermost to be seen. In this case the strided accepted by BLAS are more restricted, if <code>stride(C,1)==1</code> then only <code>stride(AorB::BatchedAdjoint,2) == 1</code> is accepted.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.batched_adjoint" href="#NNlib.batched_adjoint"><code>NNlib.batched_adjoint</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">batched_transpose(A::AbstractArray{T,3})
batched_adjoint(A)</code></pre><p>Equivalent to applying <code>transpose</code> or <code>adjoint</code> to each matrix <code>A[:,:,k]</code>.</p><p>These exist to control how <code>batched_mul</code> behaves, as it operates on such matrix slices of an array with <code>ndims(A)==3</code>.</p><p><code>PermutedDimsArray(A, (2,1,3))</code> is equivalent to <code>batched_transpose(A)</code>, and is also understood by <code>batched_mul</code> (and more widely supported elsewhere).</p><pre><code class="language-none">BatchedTranspose{T, S} &lt;: AbstractBatchedMatrix{T, 3}
BatchedAdjoint{T, S}</code></pre><p>Lazy wrappers analogous to <code>Transpose</code> and <code>Adjoint</code>, returned by <code>batched_transpose</code> etc.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.batched_transpose" href="#NNlib.batched_transpose"><code>NNlib.batched_transpose</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">batched_transpose(A::AbstractArray{T,3})
batched_adjoint(A)</code></pre><p>Equivalent to applying <code>transpose</code> or <code>adjoint</code> to each matrix <code>A[:,:,k]</code>.</p><p>These exist to control how <code>batched_mul</code> behaves, as it operates on such matrix slices of an array with <code>ndims(A)==3</code>.</p><p><code>PermutedDimsArray(A, (2,1,3))</code> is equivalent to <code>batched_transpose(A)</code>, and is also understood by <code>batched_mul</code> (and more widely supported elsewhere).</p><pre><code class="language-none">BatchedTranspose{T, S} &lt;: AbstractBatchedMatrix{T, 3}
BatchedAdjoint{T, S}</code></pre><p>Lazy wrappers analogous to <code>Transpose</code> and <code>Adjoint</code>, returned by <code>batched_transpose</code> etc.</p></div></section></article><h2 id="Gather-and-Scatter"><a class="docs-heading-anchor" href="#Gather-and-Scatter">Gather and Scatter</a><a id="Gather-and-Scatter-1"></a><a class="docs-heading-anchor-permalink" href="#Gather-and-Scatter" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NNlib.gather" href="#NNlib.gather"><code>NNlib.gather</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">gather(src, idx) -&gt; dst</code></pre><p>Reverse operation of <a href="#NNlib.scatter"><code>scatter</code></a>. Gathers data from source <code>src</code>  and writes it in a destination <code>dst</code> according to the index array <code>idx</code>. For each <code>k</code> in <code>CartesianIndices(idx)</code>, assign values to <code>dst</code>  according to</p><pre><code class="language-none">dst[:, ... , k] .= src[:, ... , idx[k]...]</code></pre><p>Notice that if <code>idx</code> is a vector containing integers and <code>src</code> is a matrix, previous expression simplifies to</p><pre><code class="language-none">dst[:, k] .= src[:, idx[k]]</code></pre><p>and <code>k</code> will run over <code>1:length(idx)</code>. </p><p>The elements of <code>idx</code> can be integers or integer tuples and may be repeated.  A single <code>src</code> column can end up being copied into zero, one,  or multiple <code>dst</code> columns.</p><p>See <a href="models/@ref"><code>gather!</code></a> for an in-place version.</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="NNlib.scatter" href="#NNlib.scatter"><code>NNlib.scatter</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">scatter(op, src, idx; [init, dstsize])</code></pre><p>Scatter operation allocating a destination array <code>dst</code> and  calling <code>scatter!(op, dst, src, idx)</code> on it.</p><p>If <code>init</code> is provided, it is used to initialize the content of <code>dst</code>. Otherwise, the init values is inferred from the reduction operator <code>op</code> for some common operators (e.g. <code>init = 0</code> for <code>op = +</code>). </p><p>If <code>dstsize</code> is provided, it will be used to define the size of destination array, otherwise it will be inferred by <code>src</code> and <code>idx</code>.</p><p>See <a href="models/@ref"><code>scatter!</code></a> for the details.</p></div></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../advanced/">« Advanced Model Building</a><a class="docs-footer-nextpage" href="../functors/">Functors »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 30 July 2021 14:44">Friday 30 July 2021</span>. Using Julia version 1.6.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
