<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Weight Initialisation · Flux</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-36890222-9', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/flux.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img class="docs-light-only" src="../assets/logo.png" alt="Flux logo"/><img class="docs-dark-only" src="../assets/logo-dark.png" alt="Flux logo"/></a><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Welcome</a></li><li><span class="tocitem">Guide</span><ul><li><a class="tocitem" href="../models/quickstart/">Quick Start</a></li><li><a class="tocitem" href="../models/overview/">Fitting a Line</a></li><li><a class="tocitem" href="../models/basics/">Gradients and Layers</a></li><li><a class="tocitem" href="../training/training/">Training</a></li><li><a class="tocitem" href="../models/recurrence/">Recurrence</a></li><li><a class="tocitem" href="../gpu/">GPU Support</a></li><li><a class="tocitem" href="../saving/">Saving &amp; Loading</a></li><li><a class="tocitem" href="../performance/">Performance Tips</a></li></ul></li><li><a class="tocitem" href="../ecosystem/">Ecosystem</a></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../models/layers/">Built-in Layers</a></li><li><a class="tocitem" href="../models/activation/">Activation Functions</a></li><li class="is-active"><a class="tocitem" href>Weight Initialisation</a><ul class="internal"><li><a class="tocitem" href="#Initialisation-functions"><span>Initialisation functions</span></a></li><li><a class="tocitem" href="#Changing-the-type-of-all-parameters"><span>Changing the type of all parameters</span></a></li></ul></li><li><a class="tocitem" href="../models/losses/">Loss Functions</a></li><li><a class="tocitem" href="../training/reference/">Training API</a></li><li><a class="tocitem" href="../training/optimisers/">Optimisation Rules</a></li><li><a class="tocitem" href="../outputsize/">Shape Inference</a></li><li><a class="tocitem" href="../destructure/">Flat vs. Nested</a></li><li><a class="tocitem" href="../training/callbacks/">Callback Helpers</a></li><li><a class="tocitem" href="../training/zygote/">Gradients – Zygote.jl</a></li><li><a class="tocitem" href="../data/mlutils/">Batching Data – MLUtils.jl</a></li><li><a class="tocitem" href="../data/onehot/">OneHotArrays.jl</a></li><li><a class="tocitem" href="../models/nnlib/">Low-level Operations – NNlib.jl</a></li><li><a class="tocitem" href="../models/functors/">Nested Structures – Functors.jl</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../tutorials/linear_regression/">Linear Regression</a></li><li><a class="tocitem" href="../tutorials/logistic_regression/">Logistic Regression</a></li><li><a class="tocitem" href="../models/advanced/">Custom Layers</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Reference</a></li><li class="is-active"><a href>Weight Initialisation</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Weight Initialisation</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/FluxML/Flux.jl/blob/master/docs/src/utilities.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="man-init-funcs"><a class="docs-heading-anchor" href="#man-init-funcs">Random Weight Initialisation</a><a id="man-init-funcs-1"></a><a class="docs-heading-anchor-permalink" href="#man-init-funcs" title="Permalink"></a></h1><p>Flux initialises convolutional layers and recurrent cells with <code>glorot_uniform</code> by default. Most layers accept a function as an <code>init</code> keyword, which replaces this default. For example:</p><pre><code class="language-julia-repl hljs">julia&gt; conv = Conv((3, 3), 3 =&gt; 2, relu; init=Flux.glorot_normal)
Conv((3, 3), 3 =&gt; 2, relu)  # 56 parameters

julia&gt; conv.bias
2-element Vector{Float32}:
 0.0
 0.0</code></pre><p>Note that <code>init</code> creates the weight array, but not the bias vector.</p><p>Many of the initialisation functions accept keywords such as <code>gain</code>,  and a random number generator. To make it easy to pass these to layers, there are methods which return a function:</p><pre><code class="language-julia-repl hljs">julia&gt; Dense(4 =&gt; 5, tanh; init=Flux.glorot_uniform(gain=2))
Dense(4 =&gt; 5, tanh)  # 25 parameters

julia&gt; Dense(4 =&gt; 5, tanh; init=Flux.randn32(MersenneTwister(1)))
Dense(4 =&gt; 5, tanh)  # 25 parameters</code></pre><h2 id="Initialisation-functions"><a class="docs-heading-anchor" href="#Initialisation-functions">Initialisation functions</a><a id="Initialisation-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Initialisation-functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="Flux.glorot_uniform" href="#Flux.glorot_uniform"><code>Flux.glorot_uniform</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">glorot_uniform([rng], size...; gain = 1) -&gt; Array
glorot_uniform([rng]; kw...) -&gt; Function</code></pre><p>Return an <code>Array{Float32}</code> of the given <code>size</code> containing random numbers drawn from a uniform distribution on the interval <span>$[-x, x]$</span>, where <code>x = gain * sqrt(6 / (fan_in + fan_out))</code>.</p><p>This method is described in [1] and also known as Xavier initialization.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; Flux.glorot_uniform(3, 4) |&gt; summary
&quot;3×4 Matrix{Float32}&quot;

julia&gt; round.(extrema(Flux.glorot_uniform(10, 100)), digits=3)
(-0.232f0, 0.234f0)

julia&gt; round.(extrema(Flux.glorot_uniform(100, 10)), digits=3)
(-0.233f0, 0.233f0)

julia&gt; round.(extrema(Flux.glorot_uniform(100, 100)), digits=3)
(-0.173f0, 0.173f0)

julia&gt; Dense(3 =&gt; 2, tanh; init = Flux.glorot_uniform(MersenneTwister(1)))
Dense(3 =&gt; 2, tanh)  # 8 parameters

julia&gt; ans.bias
2-element Vector{Float32}:
 0.0
 0.0</code></pre><p><strong>References</strong></p><p>[1] Glorot, Xavier, and Yoshua Bengio. &quot;Understanding the difficulty of training deep feedforward neural networks.&quot; <em>Proceedings of the thirteenth international conference on artificial intelligence and statistics</em>. 2010.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/95880e2e3280cdd5388ad7bb70637cba3701241f/src/utils.jl#L49-L84">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.glorot_normal" href="#Flux.glorot_normal"><code>Flux.glorot_normal</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">glorot_normal([rng], size...; gain = 1) -&gt; Array
glorot_normal([rng]; kw...) -&gt; Function</code></pre><p>Return an <code>Array{Float32}</code> of the given <code>size</code> containing random numbers drawn from a normal distribution with standard deviation <code>gain * sqrt(2 / (fan_in + fan_out))</code>, using <a href="#Flux.nfan"><code>nfan</code></a>.</p><p>This method is described in [1] and also known as Xavier initialization.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using Statistics

julia&gt; round(std(Flux.glorot_normal(10, 1000)), digits=3)
0.044f0

julia&gt; round(std(Flux.glorot_normal(1000, 10)), digits=3)
0.044f0

julia&gt; round(std(Flux.glorot_normal(1000, 1000)), digits=3)
0.032f0

julia&gt; Dense(10 =&gt; 1000, tanh; init = Flux.glorot_normal(gain=100))
Dense(10 =&gt; 1000, tanh)  # 11_000 parameters

julia&gt; round(std(ans.weight), sigdigits=3)
4.45f0</code></pre><p><strong>References</strong></p><p>[1] Glorot, Xavier, and Yoshua Bengio. &quot;Understanding the difficulty of training deep feedforward neural networks.&quot; <em>Proceedings of the thirteenth international conference on artificial intelligence and statistics</em>. 2010.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/95880e2e3280cdd5388ad7bb70637cba3701241f/src/utils.jl#L94-L127">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.kaiming_uniform" href="#Flux.kaiming_uniform"><code>Flux.kaiming_uniform</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">kaiming_uniform([rng], size...; gain = √2) -&gt; Array
kaiming_uniform([rng]; kw...) -&gt; Function</code></pre><p>Return an <code>Array{Float32}</code> of the given <code>size</code> containing random numbers drawn from a uniform distribution on the interval <code>[-x, x]</code>, where <code>x = gain * sqrt(3/fan_in)</code> using <a href="#Flux.nfan"><code>nfan</code></a>.</p><p>This method is described in [1] and also known as He initialization.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; round.(extrema(Flux.kaiming_uniform(100, 10)), digits=3)
(-0.774f0, 0.774f0)

julia&gt; round.(extrema(Flux.kaiming_uniform(10, 100)), digits=3)
(-0.245f0, 0.244f0)

julia&gt; round.(extrema(Flux.kaiming_uniform(100, 100)), digits=3)
(-0.245f0, 0.245f0)</code></pre><p><strong>References</strong></p><p>[1] He, Kaiming, et al. &quot;Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.&quot; <em>Proceedings of the IEEE international conference on computer vision</em>. 2015.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/95880e2e3280cdd5388ad7bb70637cba3701241f/src/utils.jl#L137-L161">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.kaiming_normal" href="#Flux.kaiming_normal"><code>Flux.kaiming_normal</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">kaiming_normal([rng], size...; gain = √2) -&gt; Array
kaiming_normal([rng]; kw...) -&gt; Function</code></pre><p>Return an <code>Array{Float32}</code> of the given <code>size</code> containing random numbers taken from a normal distribution standard deviation <code>gain / sqrt(fan_in)</code>, using <a href="#Flux.nfan"><code>nfan</code></a>.</p><p>This method is described in [1] and also known as He initialization.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using Statistics

julia&gt; round(std(Flux.kaiming_normal(10, 1000)), digits=3)
0.045f0

julia&gt; round(std(Flux.kaiming_normal(1000, 10)), digits=3)
0.447f0

julia&gt; round(std(Flux.kaiming_normal(1000, 1000)), digits=3)
0.045f0</code></pre><p><strong>References</strong></p><p>[1] He, Kaiming, et al. &quot;Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.&quot; <em>Proceedings of the IEEE international conference on computer vision</em>. 2015.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/95880e2e3280cdd5388ad7bb70637cba3701241f/src/utils.jl#L172-L198">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.truncated_normal" href="#Flux.truncated_normal"><code>Flux.truncated_normal</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">truncated_normal([rng], size...; mean = 0, std = 1, lo = -2, hi = 2) -&gt; Array
truncated_normal([rng]; kw...) -&gt; Function</code></pre><p>Return an <code>Array{Float32}</code> of the given <code>size</code> where each element is drawn from a truncated normal distribution. The numbers are distributed like <code>filter(x -&gt; lo&lt;=x&lt;=hi, mean .+ std .* randn(100))</code>.</p><p>The values are generated by sampling a Uniform(0, 1) (<code>rand()</code>) and then applying the inverse CDF of the truncated normal distribution. This method works best when <code>lo ≤ mean ≤ hi</code>.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using Statistics

julia&gt; Flux.truncated_normal(3, 4) |&gt; summary
&quot;3×4 Matrix{Float32}&quot;

julia&gt; round.(extrema(Flux.truncated_normal(10^6)); digits=3)
(-2.0f0, 2.0f0)

julia&gt; round(std(Flux.truncated_normal(10^6; lo = -100, hi = 100)))
1.0f0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/95880e2e3280cdd5388ad7bb70637cba3701241f/src/utils.jl#L209-L233">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.orthogonal" href="#Flux.orthogonal"><code>Flux.orthogonal</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">orthogonal([rng], size...; gain = 1) -&gt; Array
orthogonal([rng]; kw...) -&gt; Function</code></pre><p>Return an <code>Array{Float32}</code> of the given <code>size</code> which is a (semi) orthogonal matrix, as described in [1].</p><p>Cannot construct a vector, i.e. <code>length(size) == 1</code> is forbidden. For <code>length(size) &gt; 2</code>, a <code>prod(size[1:(end - 1)])</code> by <code>size[end]</code> orthogonal matrix is computed before reshaping it to the original dimensions.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; W = Flux.orthogonal(5, 7);

julia&gt; summary(W)
&quot;5×7 Matrix{Float32}&quot;

julia&gt; W * W&#39; ≈ I(5)
true

julia&gt; W2 = Flux.orthogonal(7, 5);

julia&gt; W2 * W2&#39; ≈ I(7)
false

julia&gt; W2&#39; * W2 ≈ I(5)
true

julia&gt; W3 = Flux.orthogonal(3, 3, 2, 4);

julia&gt; transpose(reshape(W3, :, 4)) * reshape(W3, :, 4) ≈ I(4)
true</code></pre><p><strong>References</strong></p><p>[1] Saxe, McClelland, Ganguli. &quot;Exact solutions to the nonlinear dynamics of learning in deep linear neural networks&quot;, ICLR 2014, https://arxiv.org/abs/1312.6120</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/95880e2e3280cdd5388ad7bb70637cba3701241f/src/utils.jl#L255-L293">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.sparse_init" href="#Flux.sparse_init"><code>Flux.sparse_init</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">sparse_init([rng], rows, cols; sparsity, std = 0.01) -&gt; Array
sparse_init([rng]; kw...) -&gt; Function</code></pre><p>Return a <code>Matrix{Float32}</code> of size <code>rows, cols</code> where each column contains a fixed fraction of zero elements given by <code>sparsity</code>. Non-zero elements are normally distributed with a mean of zero and standard deviation <code>std</code>.</p><p>This method is described in [1].</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; count(iszero, Flux.sparse_init(10, 10, sparsity=1/5))
20

julia&gt; sum(0 .== Flux.sparse_init(10, 11, sparsity=0.9), dims=1)
1×11 Matrix{Int64}:
 9  9  9  9  9  9  9  9  9  9  9

julia&gt; Dense(3 =&gt; 10, tanh; init=Flux.sparse_init(sparsity=0.5))
Dense(3 =&gt; 10, tanh)  # 40 parameters

julia&gt; count(iszero, ans.weight, dims=1)
1×3 Matrix{Int64}:
 5  5  5</code></pre><p><strong>References</strong></p><p>[1] Martens, J, &quot;Deep learning via Hessian-free optimization&quot; <em>Proceedings of the 27th International Conference on International Conference on Machine Learning</em>. 2010.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/95880e2e3280cdd5388ad7bb70637cba3701241f/src/utils.jl#L316-L346">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.identity_init" href="#Flux.identity_init"><code>Flux.identity_init</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">identity_init(size...; gain=1, shift=0) -&gt; Array
identity_init(; kw...) -&gt; Function</code></pre><p>Return an <code>Array{Float32}</code> of the given <code>size</code> which yields an identity mapping when used as parameters in most Flux layers. Use <code>gain</code> to scale the identity by a constant.</p><p>Often useful in the context of transfer learning, i.e when one wants to add more capacity to a model but start from the same mapping.</p><p>Has the following behaviour</p><ul><li>1D: A <code>Vector</code> of <code>zeros</code> (useful for an identity bias)</li><li>2D: An identity matrix (useful for an identity matrix multiplication)</li><li>More than 2D: A dense block array of center tap spatial filters (useful for an identity convolution)</li></ul><p>Some caveats: </p><ul><li><p>Not all layers will be identity mapping when used with this init. Exceptions include recurrent layers and normalization layers.</p></li><li><p>Layers must have <code>input_size == output_size</code> for identity mapping to be possible. When this is not the case, extra dimensions of the array are padded with zeros.</p></li><li><p>For convolutional layers, in addition to the above, the kernel sizes must also be odd and padding must be applied so that output feature maps have the same size as input feature maps, e.g by using <a href="../models/layers/#Flux.SamePad"><code>SamePad</code></a>.</p></li></ul><p>Use keyword <code>shift</code> (integer or tuple) to apply circular shift to the output, equivalent to <code>Base.circshift(identity_init(size...), shift)</code>.</p><p>For consistency with other initialisers, it accepts <code>rng::AbstractRNG</code> as an optional first argument. But this is ignored, since the result is not random.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; Flux.identity_init(3,5)
3×5 Matrix{Float32}:
 1.0  0.0  0.0  0.0  0.0
 0.0  1.0  0.0  0.0  0.0
 0.0  0.0  1.0  0.0  0.0

julia&gt; Dense(5 =&gt; 3, relu, init=Flux.identity_init)([1,-2,3,-4,5])
3-element Vector{Float32}:
 1.0
 0.0
 3.0

julia&gt; Flux.identity_init(3,3,2; gain=100)
3×3×2 Array{Float32, 3}:
[:, :, 1] =
   0.0  0.0  0.0
 100.0  0.0  0.0
   0.0  0.0  0.0

[:, :, 2] =
 0.0    0.0  0.0
 0.0  100.0  0.0
 0.0    0.0  0.0

julia&gt; x4 = cat([1 2 3; 4 5 6; 7 8 9]; dims=4);

julia&gt; Conv((2,2), 1 =&gt; 1, init=Flux.identity_init(gain=10), pad=SamePad())(x4)
3×3×1×1 Array{Float32, 4}:
[:, :, 1, 1] =
 10.0  20.0  30.0
 40.0  50.0  60.0
 70.0  80.0  90.0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/95880e2e3280cdd5388ad7bb70637cba3701241f/src/utils.jl#L364-L431">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.ones32" href="#Flux.ones32"><code>Flux.ones32</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">ones32(size...) = ones(Float32, size...)</code></pre><p>Return an <code>Array{Float32}</code> of the given <code>size</code> filled with 1s.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/95880e2e3280cdd5388ad7bb70637cba3701241f/src/utils.jl#L454-L458">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.zeros32" href="#Flux.zeros32"><code>Flux.zeros32</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">zeros32(size...) = zeros(Float32, size...)</code></pre><p>Return an <code>Array{Float32}</code> of the given <code>size</code> filled with 0s.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/95880e2e3280cdd5388ad7bb70637cba3701241f/src/utils.jl#L461-L465">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.rand32" href="#Flux.rand32"><code>Flux.rand32</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">rand32([rng], size...)</code></pre><p>Return an <code>Array{Float32}</code> of the given <code>size</code>, filled like <code>rand</code>. When the size is not provided, <code>rand32(rng::AbstractRNG)</code> returns a function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/95880e2e3280cdd5388ad7bb70637cba3701241f/src/utils.jl#L468-L473">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.randn32" href="#Flux.randn32"><code>Flux.randn32</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">randn32([rng], size...)</code></pre><p>Return an <code>Array{Float32}</code> of the given <code>size</code>, filled like <code>randn</code>. When the size is not provided, <code>randn32(rng::AbstractRNG)</code> returns a function.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/95880e2e3280cdd5388ad7bb70637cba3701241f/src/utils.jl#L478-L483">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.create_bias" href="#Flux.create_bias"><code>Flux.create_bias</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">create_bias(weights, bias, size...)</code></pre><p>Return a bias parameter for a layer, based on the value given to the constructor&#39;s keyword <code>bias=bias</code>.</p><ul><li><code>bias == true</code> creates a trainable array of the given size, of the same type as <code>weights</code>, initialised to zero.</li><li><code>bias == false</code> returns <code>false</code>, which is understood by AD to be non-differentiable.</li><li><code>bias::AbstractArray</code> uses the array provided, provided it has the correct size. It will also correct the <code>eltype</code> to match that of <code>weights</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/95880e2e3280cdd5388ad7bb70637cba3701241f/src/utils.jl#L488-L498">source</a></section></article><p>These functions call:</p><article class="docstring"><header><a class="docstring-binding" id="Flux.rng_from_array" href="#Flux.rng_from_array"><code>Flux.rng_from_array</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">rng_from_array(x)</code></pre><p>Create an instance of the RNG most appropriate for <code>x</code>. The current defaults are:</p><ul><li><code>x isa CuArray</code>: <code>CUDA.default_rng()</code></li><li><code>x isa AbstractArray</code>: `Random.default_rng()</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/95880e2e3280cdd5388ad7bb70637cba3701241f/src/utils.jl#L36-L43">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.nfan" href="#Flux.nfan"><code>Flux.nfan</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">nfan(n_out, n_in=1) -&gt; Tuple
nfan(dims...)
nfan(dims::Tuple)</code></pre><p>For a layer characterized by dimensions <code>dims</code>, return a tuple <code>(fan_in, fan_out)</code>, where <code>fan_in</code> is the number of input neurons connected to an output one, and <code>fan_out</code> is the number of output neurons connected to an input one.</p><p>This function is mainly used by weight initializers, e.g., <a href="#Flux.kaiming_normal"><code>kaiming_normal</code></a>.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; layer = Dense(10, 20);

julia&gt; Flux.nfan(size(layer.weight))
(10, 20)

julia&gt; layer = Conv((3, 3), 2=&gt;10);

julia&gt; Flux.nfan(size(layer.weight))
(18, 90)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/95880e2e3280cdd5388ad7bb70637cba3701241f/src/utils.jl#L2-L26">source</a></section></article><h2 id="Changing-the-type-of-all-parameters"><a class="docs-heading-anchor" href="#Changing-the-type-of-all-parameters">Changing the type of all parameters</a><a id="Changing-the-type-of-all-parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Changing-the-type-of-all-parameters" title="Permalink"></a></h2><p>The default <code>eltype</code> for models is <code>Float32</code> since models are often trained/run on GPUs. The <code>eltype</code> of model <code>m</code> can be changed to <code>Float64</code> by <code>f64(m)</code>:</p><article class="docstring"><header><a class="docstring-binding" id="Flux.f64" href="#Flux.f64"><code>Flux.f64</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">f64(m)</code></pre><p>Converts the <code>eltype</code> of model&#39;s <em>floating point</em> parameters to <code>Float64</code>. Recurses into structs marked with <a href="../models/functors/#Functors.@functor"><code>@functor</code></a>.</p><p>See also <a href="#Flux.f32"><code>f32</code></a> and <a href="#Flux.f16"><code>f16</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/95880e2e3280cdd5388ad7bb70637cba3701241f/src/functor.jl#L287-L294">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.f32" href="#Flux.f32"><code>Flux.f32</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">f32(m)</code></pre><p>Converts the <code>eltype</code> of model&#39;s <em>floating point</em> parameters to <code>Float32</code> (which is Flux&#39;s default). Recurses into structs marked with <a href="../models/functors/#Functors.@functor"><code>@functor</code></a>.</p><p>See also <a href="#Flux.f64"><code>f64</code></a> and <a href="#Flux.f16"><code>f16</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/95880e2e3280cdd5388ad7bb70637cba3701241f/src/functor.jl#L277-L284">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.f16" href="#Flux.f16"><code>Flux.f16</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">f16(m)</code></pre><p>Converts the <code>eltype</code> of model&#39;s <em>floating point</em> parameters to <code>Float16</code>. Recurses into structs marked with <a href="../models/functors/#Functors.@functor"><code>@functor</code></a>.</p><p>Support for <code>Float16</code> is limited on many CPUs. Julia may convert to <code>Float32</code> for each operation, which is slow.</p><p>See also <a href="#Flux.f32"><code>f32</code></a> and <a href="#Flux.f64"><code>f64</code></a>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; m = Chain(Dense(784, 2048, relu), Dense(2048, 10))  # all Float32
Chain(
  Dense(784 =&gt; 2048, relu),             # 1_607_680 parameters
  Dense(2048 =&gt; 10),                    # 20_490 parameters
)                   # Total: 4 arrays, 1_628_170 parameters, 6.211 MiB.

julia&gt; m |&gt; f16  # takes half the memory
Chain(
  Dense(784 =&gt; 2048, relu),             # 1_607_680 parameters
  Dense(2048 =&gt; 10),                    # 20_490 parameters
)                   # Total: 4 arrays, 1_628_170 parameters, 3.106 MiB.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/95880e2e3280cdd5388ad7bb70637cba3701241f/src/functor.jl#L297-L322">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../models/activation/">« Activation Functions</a><a class="docs-footer-nextpage" href="../models/losses/">Loss Functions »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Friday 14 July 2023 11:08">Friday 14 July 2023</span>. Using Julia version 1.9.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
