<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Optimisation Rules · Flux</title><meta name="title" content="Optimisation Rules · Flux"/><meta property="og:title" content="Optimisation Rules · Flux"/><meta property="twitter:title" content="Optimisation Rules · Flux"/><meta name="description" content="Documentation for Flux."/><meta property="og:description" content="Documentation for Flux."/><meta property="twitter:description" content="Documentation for Flux."/><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-36890222-9', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../../assets/documenter.js"></script><script src="../../../search_index.js"></script><script src="../../../siteinfo.js"></script><script src="../../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../../assets/themeswap.js"></script><link href="../../../assets/flux.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../../"><img class="docs-light-only" src="../../../assets/logo.png" alt="Flux logo"/><img class="docs-dark-only" src="../../../assets/logo-dark.png" alt="Flux logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../../">Welcome</a></li><li><span class="tocitem">Guide</span><ul><li><a class="tocitem" href="../../../guide/models/quickstart/">Quick Start</a></li><li><a class="tocitem" href="../../../guide/models/overview/">Fitting a Line</a></li><li><a class="tocitem" href="../../../guide/models/basics/">Gradients and Layers</a></li><li><a class="tocitem" href="../../../guide/models/custom_layers/">Custom Layers</a></li><li><a class="tocitem" href="../../../guide/training/training/">Training</a></li><li><a class="tocitem" href="../../../guide/models/recurrence/">Recurrence</a></li><li><a class="tocitem" href="../../../guide/gpu/">GPU Support</a></li><li><a class="tocitem" href="../../../guide/saving/">Saving &amp; Loading</a></li><li><a class="tocitem" href="../../../guide/performance/">Performance Tips</a></li></ul></li><li><a class="tocitem" href="../../../ecosystem/">Ecosystem</a></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../../models/layers/">Built-in Layers</a></li><li><a class="tocitem" href="../../models/activation/">Activation Functions</a></li><li><a class="tocitem" href="../../utilities/">Weight Initialisation</a></li><li><a class="tocitem" href="../../models/losses/">Loss Functions</a></li><li><a class="tocitem" href="../reference/">Training API</a></li><li class="is-active"><a class="tocitem" href>Optimisation Rules</a><ul class="internal"><li><a class="tocitem" href="#Optimisers-Reference"><span>Optimisers Reference</span></a></li><li><a class="tocitem" href="#Composing-Optimisers"><span>Composing Optimisers</span></a></li><li><a class="tocitem" href="#Scheduling-Optimisers"><span>Scheduling Optimisers</span></a></li><li><a class="tocitem" href="#Decays"><span>Decays</span></a></li><li><a class="tocitem" href="#Gradient-Clipping"><span>Gradient Clipping</span></a></li></ul></li><li><a class="tocitem" href="../../outputsize/">Shape Inference</a></li><li><a class="tocitem" href="../../destructure/">Flat vs. Nested</a></li><li><a class="tocitem" href="../callbacks/">Callback Helpers</a></li><li><a class="tocitem" href="../zygote/">Gradients – Zygote.jl</a></li><li><a class="tocitem" href="../../data/mlutils/">Batching Data – MLUtils.jl</a></li><li><a class="tocitem" href="../../data/onehot/">OneHotArrays.jl</a></li><li><a class="tocitem" href="../../models/nnlib/">Low-level Operations – NNlib.jl</a></li><li><a class="tocitem" href="../../models/functors/">Nested Structures – Functors.jl</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../../tutorials/linear_regression/">Linear Regression</a></li><li><a class="tocitem" href="../../../tutorials/logistic_regression/">Logistic Regression</a></li><li><a class="tocitem" href="../../../tutorials/model_zoo/">Model Zoo</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Reference</a></li><li class="is-active"><a href>Optimisation Rules</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Optimisation Rules</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/FluxML/Flux.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/FluxML/Flux.jl/blob/master/docs/src/reference/training/optimisers.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="man-optimisers"><a class="docs-heading-anchor" href="#man-optimisers">Optimisation Rules</a><a id="man-optimisers-1"></a><a class="docs-heading-anchor-permalink" href="#man-optimisers" title="Permalink"></a></h1><p>Any optimization rule from Optimisers.jl can be used with <a href="../reference/#Flux.Optimise.train!-NTuple{4, Any}"><code>train!</code></a> and other training functions.</p><p>For full details of how the new interface works, see the <a href="https://fluxml.ai/Optimisers.jl/dev/">Optimisers.jl documentation</a>.</p><h2 id="Optimisers-Reference"><a class="docs-heading-anchor" href="#Optimisers-Reference">Optimisers Reference</a><a id="Optimisers-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Optimisers-Reference" title="Permalink"></a></h2><p>All optimisers return an object that, when passed to <code>train!</code>, will update the parameters passed to it.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.Descent" href="#Optimisers.Descent"><code>Optimisers.Descent</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Descent(η = 1f-1)
Descent(; eta)</code></pre><p>Classic gradient descent optimiser with learning rate <code>η</code>. For each parameter <code>p</code> and its gradient <code>dp</code>, this runs <code>p -= η*dp</code>.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η == eta</code>): Amount by which gradients are discounted before updating                      the weights.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.3.3/src/rules.jl#L9-L19">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.Momentum" href="#Optimisers.Momentum"><code>Optimisers.Momentum</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Momentum(η = 0.01, ρ = 0.9)
Momentum(; [eta, rho])</code></pre><p>Gradient descent optimizer with learning rate <code>η</code> and momentum <code>ρ</code>.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η == eta</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Momentum (<code>ρ == rho</code>): Controls the acceleration of gradient descent in the                 prominent direction, in effect dampening oscillations.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.3.3/src/rules.jl#L39-L50">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.Nesterov" href="#Optimisers.Nesterov"><code>Optimisers.Nesterov</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Nesterov(η = 0.001, ρ = 0.9)</code></pre><p>Gradient descent optimizer with learning rate <code>η</code> and Nesterov momentum <code>ρ</code>.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Nesterov momentum (<code>ρ</code>): Controls the acceleration of gradient descent in the                          prominent direction, in effect dampening oscillations.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.3.3/src/rules.jl#L65-L75">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.RMSProp" href="#Optimisers.RMSProp"><code>Optimisers.RMSProp</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RMSProp(η = 0.001, ρ = 0.9, ϵ = 1e-8; centred = false)
RMSProp(; [eta, rho, epsilon, centred])</code></pre><p>Optimizer using the <a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">RMSProp</a> algorithm. Often a good choice for recurrent networks. Parameters other than learning rate generally don&#39;t need tuning.</p><p><a href="http://arxiv.org/abs/1308.08500">Centred RMSProp</a> is a variant which normalises gradients by an estimate their variance, instead of their second moment.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η == eta</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Momentum (<code>ρ == rho</code>): Controls the acceleration of gradient descent in the                 prominent direction, in effect dampening oscillations.</li><li>Machine epsilon (<code>ϵ == epsilon</code>): Constant to prevent division by zero                        (no need to change default)</li><li>Keyword <code>centred</code> (or <code>centered</code>): Indicates whether to use centred variant                                    of the algorithm.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.3.3/src/rules.jl#L92-L113">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.Adam" href="#Optimisers.Adam"><code>Optimisers.Adam</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Adam(η = 0.001, β = (0.9, 0.999), ϵ = 1e-8)</code></pre><p><a href="https://arxiv.org/abs/1412.6980">Adam</a> optimiser.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.3.3/src/rules.jl#L194-L206">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.RAdam" href="#Optimisers.RAdam"><code>Optimisers.RAdam</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RAdam(η = 0.001, β = (0.9, 0.999), ϵ = 1e-8)</code></pre><p><a href="https://arxiv.org/abs/1908.03265">Rectified Adam</a> optimizer.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.3.3/src/rules.jl#L255-L267">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.AdaMax" href="#Optimisers.AdaMax"><code>Optimisers.AdaMax</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaMax(η = 0.001, β = (0.9, 0.999), ϵ = 1e-8)</code></pre><p><a href="https://arxiv.org/abs/1412.6980">AdaMax</a> is a variant of Adam based on the ∞-norm.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.3.3/src/rules.jl#L295-L307">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.AdaGrad" href="#Optimisers.AdaGrad"><code>Optimisers.AdaGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaGrad(η = 0.1, ϵ = 1e-8)</code></pre><p><a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">AdaGrad</a> optimizer. It has parameter specific learning rates based on how frequently it is updated. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.3.3/src/rules.jl#L362-L374">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.AdaDelta" href="#Optimisers.AdaDelta"><code>Optimisers.AdaDelta</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaDelta(ρ = 0.9, ϵ = 1e-8)</code></pre><p><a href="https://arxiv.org/abs/1212.5701">AdaDelta</a> is a version of AdaGrad adapting its learning rate based on a window of past gradient updates. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Rho (<code>ρ</code>): Factor by which the gradient is decayed at each time step.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.3.3/src/rules.jl#L392-L403">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.AMSGrad" href="#Optimisers.AMSGrad"><code>Optimisers.AMSGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AMSGrad(η = 0.001, β = (0.9, 0.999), ϵ = 1e-8)</code></pre><p>The <a href="https://openreview.net/forum?id=ryQu7f-RZ">AMSGrad</a> version of the Adam optimiser. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.3.3/src/rules.jl#L423-L436">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.NAdam" href="#Optimisers.NAdam"><code>Optimisers.NAdam</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">NAdam(η = 0.001, β = (0.9, 0.999), ϵ = 1e-8)</code></pre><p><a href="https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ">NAdam</a> is a Nesterov variant of Adam. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.3.3/src/rules.jl#L458-L471">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.AdamW" href="#Optimisers.AdamW"><code>Optimisers.AdamW</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">AdamW(η = 0.001, β = (0.9, 0.999), λ = 0, ϵ = 1e-8)
AdamW(; [eta, beta, lambda, epsilon])</code></pre><p><a href="https://arxiv.org/abs/1711.05101">AdamW</a> is a variant of Adam fixing (as in repairing) its weight decay regularization. Implemented as an <a href="#Optimisers.OptimiserChain"><code>OptimiserChain</code></a> of <a href="#Optimisers.Adam"><code>Adam</code></a> and <a href="#Optimisers.WeightDecay"><code>WeightDecay</code></a>`.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η == eta</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple == beta</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Weight decay (<code>λ == lambda</code>): Controls the strength of <span>$L_2$</span> regularisation.</li><li>Machine epsilon (<code>ϵ == epsilon</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.3.3/src/rules.jl#L493-L509">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.OAdam" href="#Optimisers.OAdam"><code>Optimisers.OAdam</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">OAdam(η = 0.001, β = (0.5, 0.9), ϵ = 1e-8)</code></pre><p><a href="https://arxiv.org/abs/1711.00141">OAdam</a> (Optimistic Adam) is a variant of Adam adding an &quot;optimistic&quot; term suitable for adversarial training.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ</code>): Constant to prevent division by zero                        (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.3.3/src/rules.jl#L327-L340">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.AdaBelief" href="#Optimisers.AdaBelief"><code>Optimisers.AdaBelief</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaBelief(η = 0.001, β = (0.9, 0.999), ϵ = 1e-16)</code></pre><p>The <a href="https://arxiv.org/abs/2010.07468">AdaBelief</a> optimiser is a variant of the well-known Adam optimiser.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li>Machine epsilon (<code>ϵ::Float32</code>): Constant to prevent division by zero                                 (no need to change default)</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.3.3/src/rules.jl#L516-L529">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.Lion" href="#Optimisers.Lion"><code>Optimisers.Lion</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Lion(η = 0.001, β = (0.9, 0.999))</code></pre><p><a href="https://arxiv.org/abs/2302.06675">Lion</a> optimiser.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Magnitude by which gradients are updating the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.3.3/src/rules.jl#L226-L235">source</a></section></article><h2 id="Composing-Optimisers"><a class="docs-heading-anchor" href="#Composing-Optimisers">Composing Optimisers</a><a id="Composing-Optimisers-1"></a><a class="docs-heading-anchor-permalink" href="#Composing-Optimisers" title="Permalink"></a></h2><p>Flux (through Optimisers.jl) defines a special kind of optimiser called <code>OptimiserChain</code> which takes in arbitrary optimisers as input. Its behaviour is similar to the usual optimisers, but differs in that it acts by calling the optimisers listed in it sequentially. Each optimiser produces a modified gradient that will be fed into the next, and the resultant update will be applied to the parameter as usual. A classic use case is where adding decays is desirable. Optimisers.jl defines the basic decay corresponding to an <span>$L_2$</span> regularization in the loss as <code>WeightDecay</code>.</p><pre><code class="language-julia hljs">opt = OptimiserChain(WeightDecay(1e-4), Descent())</code></pre><p>Here we apply the weight decay to the <code>Descent</code> optimiser.  The resulting optimiser <code>opt</code> can be used as any optimiser.</p><pre><code class="language-julia hljs">w = [randn(10, 10), randn(10, 10)]
opt_state = Flux.setup(opt, w)

loss(w, x) = Flux.mse(w[1] * x, w[2] * x)

loss(w, rand(10)) # around 0.9

for t = 1:10^5
  g = gradient(w -&gt; loss(w[1], w[2], rand(10)), w)
  Flux.update!(opt_state, w, g)
end

loss(w, rand(10)) # around 0.9</code></pre><p>It is possible to compose optimisers for some added flexibility.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.OptimiserChain" href="#Optimisers.OptimiserChain"><code>Optimisers.OptimiserChain</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">OptimiserChain(opts...)</code></pre><p>Compose a sequence of optimisers so that each <code>opt</code> in <code>opts</code> updates the gradient, in the order specified.</p><p>With an empty sequence, <code>OptimiserChain()</code> is the identity, so <code>update!</code> will subtract the full gradient from the parameters. This is equivalent to <code>Descent(1)</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; o = OptimiserChain(ClipGrad(1.0), Descent(0.1));

julia&gt; m = (zeros(3),);

julia&gt; s = Optimisers.setup(o, m)
(Leaf(OptimiserChain(ClipGrad(1.0), Descent(0.1)), (nothing, nothing)),)

julia&gt; Optimisers.update(s, m, ([0.3, 1, 7],))[2]  # clips before discounting
([-0.03, -0.1, -0.1],)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.3.3/src/rules.jl#L693-L716">source</a></section></article><h2 id="Scheduling-Optimisers"><a class="docs-heading-anchor" href="#Scheduling-Optimisers">Scheduling Optimisers</a><a id="Scheduling-Optimisers-1"></a><a class="docs-heading-anchor-permalink" href="#Scheduling-Optimisers" title="Permalink"></a></h2><p>In practice, it is fairly common to schedule the learning rate of an optimiser to obtain faster convergence. There are a variety of popular scheduling policies, and you can find implementations of them in <a href="http://fluxml.ai/ParameterSchedulers.jl/stable">ParameterSchedulers.jl</a>. The documentation for ParameterSchedulers.jl provides a more detailed overview of the different scheduling policies, and how to use them with Flux optimisers. Below, we provide a brief snippet illustrating a <a href="https://arxiv.org/pdf/1608.03983.pdf">cosine annealing</a> schedule with a momentum optimiser.</p><p>First, we import ParameterSchedulers.jl and initialize a cosine annealing schedule to vary the learning rate between <code>1e-4</code> and <code>1e-2</code> every 10 steps. We also create a new <a href="#Optimisers.Momentum"><code>Momentum</code></a> optimiser.</p><pre><code class="language-julia hljs">using ParameterSchedulers

opt = Momentum()
schedule = Cos(λ0 = 1e-4, λ1 = 1e-2, period = 10)
for (eta, epoch) in zip(schedule, 1:100)
  opt.eta = eta
  # your training code here
end</code></pre><p><code>schedule</code> can also be indexed (e.g. <code>schedule(100)</code>) or iterated like any iterator in Julia.</p><p>ParameterSchedulers.jl schedules are stateless (they don&#39;t store their iteration state). If you want a <em>stateful</em> schedule, you can use <code>ParameterSchedulers.Stateful</code>:</p><pre><code class="language-julia hljs">using ParameterSchedulers: Stateful, next!

schedule = Stateful(Cos(λ0 = 1e-4, λ1 = 1e-2, period = 10))
for epoch in 1:100
  opt.eta = next!(schedule)
  # your training code here
end</code></pre><p>ParameterSchedulers.jl allows for many more scheduling policies including arbitrary functions, looping any function with a given period, or sequences of many schedules. See the ParameterSchedulers.jl documentation for more info.</p><h2 id="Decays"><a class="docs-heading-anchor" href="#Decays">Decays</a><a id="Decays-1"></a><a class="docs-heading-anchor-permalink" href="#Decays" title="Permalink"></a></h2><p>Similar to optimisers, Flux also defines some simple decays that can be used in conjunction with other optimisers, or standalone.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.SignDecay" href="#Optimisers.SignDecay"><code>Optimisers.SignDecay</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SignDecay(λ = 1e-3)</code></pre><p>Implements <span>$L_1$</span> regularisation, also known as LASSO regression, when composed  with other rules as the first transformation in an <a href="#Optimisers.OptimiserChain"><code>OptimiserChain</code></a>.</p><p>It does this by adding <code>λ .* sign(x)</code> to the gradient. This is equivalent to adding  <code>λ * sum(abs, x) == λ * norm(x, 1)</code> to the loss.</p><p>See also [<code>WeightDecay</code>] for <span>$L_2$</span> normalisation. They can be used together: <code>OptimiserChain(SignDecay(0.012), WeightDecay(0.034), Adam())</code> is equivalent to adding <code>0.012 * norm(x, 1) + 0.017 * norm(x, 2)^2</code> to the loss function.</p><p><strong>Parameters</strong></p><ul><li>Penalty (<code>λ ≥ 0</code>): Controls the strength of the regularisation.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.3.3/src/rules.jl#L586-L601">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.WeightDecay" href="#Optimisers.WeightDecay"><code>Optimisers.WeightDecay</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">WeightDecay(λ = 5e-4)</code></pre><p>Implements <span>$L_2$</span> regularisation, also known as ridge regression,  when composed  with other rules as the first transformation in an <a href="#Optimisers.OptimiserChain"><code>OptimiserChain</code></a>.</p><p>It does this by adding <code>λ .* x</code> to the gradient. This is equivalent to adding  <code>λ/2 * sum(abs2, x) == λ/2 * norm(x)^2</code> to the loss.</p><p>See also [<code>SignDecay</code>] for <span>$L_1$</span> normalisation.</p><p><strong>Parameters</strong></p><ul><li>Penalty (<code>λ ≥ 0</code>): Controls the strength of the regularisation.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.3.3/src/rules.jl#L549-L562">source</a></section></article><h2 id="Gradient-Clipping"><a class="docs-heading-anchor" href="#Gradient-Clipping">Gradient Clipping</a><a id="Gradient-Clipping-1"></a><a class="docs-heading-anchor-permalink" href="#Gradient-Clipping" title="Permalink"></a></h2><p>Gradient clipping is useful for training recurrent neural networks, which have a tendency to suffer from the exploding gradient problem. An example usage is</p><pre><code class="language-julia hljs">opt = OptimiserChain(ClipValue(1e-3), Adam(1e-3))</code></pre><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.ClipGrad" href="#Optimisers.ClipGrad"><code>Optimisers.ClipGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ClipGrad(δ = 10)</code></pre><p>Restricts every gradient component to obey <code>-δ ≤ dx[i] ≤ δ</code>.</p><p>Typically composed with other rules using <a href="#Optimisers.OptimiserChain"><code>OptimiserChain</code></a>.</p><p>See also <a href="#Optimisers.ClipNorm"><code>ClipNorm</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.3.3/src/rules.jl#L616-L624">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.ClipNorm" href="#Optimisers.ClipNorm"><code>Optimisers.ClipNorm</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ClipNorm(ω = 10, p = 2; throw = true)</code></pre><p>Scales any gradient array for which <code>norm(dx, p) &gt; ω</code> to stay at this threshold (unless <code>p==0</code>).</p><p>Throws an error if the norm is infinite or <code>NaN</code>, which you can turn off with <code>throw = false</code>.</p><p>Typically composed with other rules using <a href="#Optimisers.OptimiserChain"><code>OptimiserChain</code></a>.</p><p>See also <a href="#Optimisers.ClipGrad"><code>ClipGrad</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.3.3/src/rules.jl#L638-L650">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../reference/">« Training API</a><a class="docs-footer-nextpage" href="../../outputsize/">Shape Inference »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.5.0 on <span class="colophon-date" title="Wednesday 14 August 2024 11:22">Wednesday 14 August 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body><div data-docstringscollapsed="true"></div></html>
