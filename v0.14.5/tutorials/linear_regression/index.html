<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Linear Regression · Flux</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-36890222-9', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/flux.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="Flux logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="Flux logo"/></a><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Welcome</a></li><li><span class="tocitem">Guide</span><ul><li><a class="tocitem" href="../../models/quickstart/">Quick Start</a></li><li><a class="tocitem" href="../../models/overview/">Fitting a Line</a></li><li><a class="tocitem" href="../../models/basics/">Gradients and Layers</a></li><li><a class="tocitem" href="../../training/training/">Training</a></li><li><a class="tocitem" href="../../models/recurrence/">Recurrence</a></li><li><a class="tocitem" href="../../gpu/">GPU Support</a></li><li><a class="tocitem" href="../../saving/">Saving &amp; Loading</a></li><li><a class="tocitem" href="../../performance/">Performance Tips</a></li></ul></li><li><a class="tocitem" href="../../ecosystem/">Ecosystem</a></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../../models/layers/">Built-in Layers</a></li><li><a class="tocitem" href="../../models/activation/">Activation Functions</a></li><li><a class="tocitem" href="../../utilities/">Weight Initialisation</a></li><li><a class="tocitem" href="../../models/losses/">Loss Functions</a></li><li><a class="tocitem" href="../../training/reference/">Training API</a></li><li><a class="tocitem" href="../../training/optimisers/">Optimisation Rules</a></li><li><a class="tocitem" href="../../outputsize/">Shape Inference</a></li><li><a class="tocitem" href="../../destructure/">Flat vs. Nested</a></li><li><a class="tocitem" href="../../training/callbacks/">Callback Helpers</a></li><li><a class="tocitem" href="../../training/zygote/">Gradients – Zygote.jl</a></li><li><a class="tocitem" href="../../data/mlutils/">Batching Data – MLUtils.jl</a></li><li><a class="tocitem" href="../../data/onehot/">OneHotArrays.jl</a></li><li><a class="tocitem" href="../../models/nnlib/">Low-level Operations – NNlib.jl</a></li><li><a class="tocitem" href="../../models/functors/">Nested Structures – Functors.jl</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li class="is-active"><a class="tocitem" href>Linear Regression</a><ul class="internal"><li><a class="tocitem" href="#Generating-a-dataset"><span>Generating a dataset</span></a></li><li><a class="tocitem" href="#Building-a-model"><span>Building a model</span></a></li><li><a class="tocitem" href="#Training-the-model"><span>Training the model</span></a></li><li><a class="tocitem" href="#Results"><span>Results</span></a></li><li><a class="tocitem" href="#Gathering-real-data"><span>Gathering real data</span></a></li><li><a class="tocitem" href="#Building-a-Flux-model"><span>Building a Flux model</span></a></li><li><a class="tocitem" href="#Training-the-Flux-model"><span>Training the Flux model</span></a></li><li><a class="tocitem" href="#Testing-the-Flux-model"><span>Testing the Flux model</span></a></li></ul></li><li><a class="tocitem" href="../logistic_regression/">Logistic Regression</a></li><li><a class="tocitem" href="../../models/advanced/">Custom Layers</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Linear Regression</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Linear Regression</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/FluxML/Flux.jl/blob/master/docs/src/tutorials/linear_regression.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="man-linear-regression"><a class="docs-heading-anchor" href="#man-linear-regression">Tutorial: Linear Regression</a><a id="man-linear-regression-1"></a><a class="docs-heading-anchor-permalink" href="#man-linear-regression" title="Permalink"></a></h1><p>Flux is a pure Julia ML stack that allows you to build predictive models. Here are the steps for a typical Flux program:</p><ul><li>Provide training and test data</li><li>Build a model with configurable parameters to make predictions</li><li>Iteratively train the model by tweaking the parameters to improve predictions</li><li>Verify your model</li></ul><p>Under the hood, Flux uses a technique called automatic differentiation to take gradients that help improve predictions. Flux is also fully written in Julia so you can easily replace any layer of Flux with your own code to improve your understanding or satisfy special requirements.</p><p>The following page contains a step-by-step walkthrough of the linear regression algorithm in <code>Julia</code> using <code>Flux</code>! We will start by creating a simple linear regression model for dummy data and then move on to a real dataset. The first part would involve writing some parts of the model on our own, which will later be replaced by <code>Flux</code>.</p><hr/><p>Let us start by building a simple linear regression model. This model would be trained on the data points of the form <code>(x₁, y₁), (x₂, y₂), ... , (xₙ, yₙ)</code>. In the real world, these <code>x</code>s can have multiple features, and the <code>y</code>s denote a label. In our example, each <code>x</code> has a single feature; hence, our data would have <code>n</code> data points, each point mapping a single feature to a single label.</p><p>Importing the required <code>Julia</code> packages -</p><pre><code class="language-julia-repl hljs">julia&gt; using Flux, Plots</code></pre><h2 id="Generating-a-dataset"><a class="docs-heading-anchor" href="#Generating-a-dataset">Generating a dataset</a><a id="Generating-a-dataset-1"></a><a class="docs-heading-anchor-permalink" href="#Generating-a-dataset" title="Permalink"></a></h2><p>The data usually comes from the real world, which we will be exploring in the last part of this tutorial, but we don&#39;t want to jump straight to the relatively harder part. Here we will generate the <code>x</code>s of our data points and map them to the respective <code>y</code>s using a simple function. Remember, here each <code>x</code> is equivalent to a feature, and each <code>y</code> is the corresponding label. Combining all the <code>x</code>s and <code>y</code>s would create the complete dataset.</p><pre><code class="language-julia-repl hljs">julia&gt; x = hcat(collect(Float32, -3:0.1:3)...)
1×61 Matrix{Float32}:
 -3.0  -2.9  -2.8  -2.7  -2.6  -2.5  …  2.4  2.5  2.6  2.7  2.8  2.9  3.0</code></pre><p>The <code>hcat</code> call generates a <code>Matrix</code> with numbers ranging from <code>-3.0</code> to <code>3.0</code> with a gap of <code>0.1</code> between them. Each column of this matrix holds a single <code>x</code>, a total of 61 <code>x</code>s. The next step would be to generate the corresponding labels or the <code>y</code>s.</p><pre><code class="language-julia-repl hljs">julia&gt; f(x) = @. 3x + 2;

julia&gt; y = f(x)
1×61 Matrix{Float32}:
 -7.0  -6.7  -6.4  -6.1  -5.8  -5.5  …  9.5  9.8  10.1  10.4  10.7  11.0</code></pre><p>The function <code>f</code> maps each <code>x</code> to a <code>y</code>, and as <code>x</code> is a <code>Matrix</code>, the expression broadcasts the scalar values using <code>@.</code> macro. Our data points are ready, but they are too perfect. In a real-world scenario, we will not have an <code>f</code> function to generate <code>y</code> values, but instead, the labels would be manually added.</p><pre><code class="language-julia-repl hljs">julia&gt; x = x .* reshape(rand(Float32, 61), (1, 61));</code></pre><p>Visualizing the final data -</p><pre><code class="language-julia-repl hljs">julia&gt; plot(vec(x), vec(y), lw = 3, seriestype = :scatter, label = &quot;&quot;, title = &quot;Generated data&quot;, xlabel = &quot;x&quot;, ylabel= &quot;y&quot;);</code></pre><p><img src="https://user-images.githubusercontent.com/74055102/177034397-d433a313-21a5-4394-97d9-5467f5cf6b72.png" alt="linear-regression-data"/></p><p>The data looks random enough now! The <code>x</code> and <code>y</code> values are still somewhat correlated; hence, the linear regression algorithm should work fine on our dataset.</p><p>We can now proceed ahead and build a model for our dataset!</p><h2 id="Building-a-model"><a class="docs-heading-anchor" href="#Building-a-model">Building a model</a><a id="Building-a-model-1"></a><a class="docs-heading-anchor-permalink" href="#Building-a-model" title="Permalink"></a></h2><p>A linear regression model is defined mathematically as -</p><p class="math-container">\[model(W, b, x) = Wx + b\]</p><p>where <code>W</code> is the weight matrix and <code>b</code> is the bias. For our case, the weight matrix (<code>W</code>) would constitute only a single element, as we have only a single feature. We can define our model in <code>Julia</code> using the exact same notation!</p><pre><code class="language-julia-repl hljs">julia&gt; custom_model(W, b, x) = @. W*x + b
custom_model (generic function with 1 method)</code></pre><p>The <code>@.</code> macro allows you to perform the calculations by broadcasting the scalar quantities (for example - the bias).</p><p>The next step would be to initialize the model parameters, which are the weight and the bias. There are a lot of initialization techniques available for different machine learning models, but for the sake of this example, let&#39;s pull out the weight from a uniform distribution and initialize the bias as <code>0</code>.</p><pre><code class="language-julia-repl hljs">julia&gt; W = rand(Float32, 1, 1)
1×1 Matrix{Float32}:
 0.99285793

julia&gt; b = [0.0f0]
1-element Vector{Float32}:
 0.0</code></pre><p>Time to test if our model works!</p><pre><code class="language-julia-repl hljs">julia&gt; custom_model(W, b, x) |&gt; size
(1, 61)

julia&gt; custom_model(W, b, x)[1], y[1]
(-1.6116865f0, -7.0f0)</code></pre><p>It does! But the predictions are way off. We need to train the model to improve the predictions, but before training the model we need to define the loss function. The loss function would ideally output a quantity that we will try to minimize during the entire training process. Here we will use the mean sum squared error loss function.</p><pre><code class="language-julia-repl hljs">julia&gt; function custom_loss(W, b, x, y)
           ŷ = custom_model(W, b, x)
           sum((y .- ŷ).^2) / length(x)
       end;

julia&gt; custom_loss(W, b, x, y)
23.772217f0</code></pre><p>Calling the loss function on our <code>x</code>s and <code>y</code>s shows how far our predictions (<code>ŷ</code>) are from the real labels. More precisely, it calculates the sum of the squares of residuals and divides it by the total number of data points.</p><p>We have successfully defined our model and the loss function, but surprisingly, we haven&#39;t used <code>Flux</code> anywhere till now. Let&#39;s see how we can write the same code using <code>Flux</code>. </p><pre><code class="language-julia-repl hljs">julia&gt; flux_model = Dense(1 =&gt; 1)
Dense(1 =&gt; 1)       # 2 parameters</code></pre><p>A <a href="../../models/layers/#Flux.Dense"><code>Dense(1 =&gt; 1)</code></a> layer denotes a layer of one neuron with one input (one feature) and one output. This layer is exactly same as the mathematical model defined by us above! Under the hood, <code>Flux</code> too calculates the output using the same expression! But, we don&#39;t have to initialize the parameters ourselves this time, instead <code>Flux</code> does it for us.</p><pre><code class="language-julia-repl hljs">julia&gt; flux_model.weight, flux_model.bias
(Float32[-1.2678515;;], Float32[0.0])</code></pre><p>Now we can check if our model is acting right. We can pass the complete data in one go, with each <code>x</code> having exactly one feature (one input) -</p><pre><code class="language-julia-repl hljs">julia&gt; flux_model(x) |&gt; size
(1, 61)

julia&gt; flux_model(x)[1], y[1]
(-1.8525281f0, -7.0f0)</code></pre><p>It is! The next step would be defining the loss function using <code>Flux</code>&#39;s functions -</p><pre><code class="language-julia-repl hljs">julia&gt; function flux_loss(flux_model, x, y)
           ŷ = flux_model(x)
           Flux.mse(ŷ, y)
       end;

julia&gt; flux_loss(flux_model, x, y)
22.74856f0</code></pre><p>Everything works as before! It almost feels like <code>Flux</code> provides us with smart wrappers for the functions we could have written on our own. Now, as the last step of this section, let&#39;s see how different the <code>flux_model</code> is from our <code>custom_model</code>. A good way to go about this would be to fix the parameters of both models to be the same. Let&#39;s change the parameters of our <code>custom_model</code> to match that of the <code>flux_model</code> -</p><pre><code class="language-julia-repl hljs">julia&gt; W = Float32[1.1412252]
1-element Vector{Float32}:
 1.1412252</code></pre><p>To check how both the models are performing on the data, let&#39;s find out the losses using the <code>loss</code> and <code>flux_loss</code> functions -</p><pre><code class="language-julia-repl hljs">julia&gt; custom_loss(W, b, x, y), flux_loss(flux_model, x, y)
(22.74856f0, 22.74856f0)</code></pre><p>The losses are identical! This means that our <code>model</code> and the <code>flux_model</code> are identical on some level, and the loss functions are completely identical! The difference in models would be that <code>Flux</code>&#39;s <a href="../../models/layers/#Flux.Dense"><code>Dense</code></a> layer supports many other arguments that can be used to customize the layer further. But, for this tutorial, let us stick to our simple <code>custom_model</code>.</p><h2 id="Training-the-model"><a class="docs-heading-anchor" href="#Training-the-model">Training the model</a><a id="Training-the-model-1"></a><a class="docs-heading-anchor-permalink" href="#Training-the-model" title="Permalink"></a></h2><p>Let&#39;s train our model using the classic Gradient Descent algorithm. According to the gradient descent algorithm, the weights and biases should be iteratively updated using the following mathematical equations -</p><p class="math-container">\[\begin{aligned}
W &amp;= W - \eta * \frac{dL}{dW} \\
b &amp;= b - \eta * \frac{dL}{db}
\end{aligned}\]</p><p>Here, <code>W</code> is the weight matrix, <code>b</code> is the bias vector, <span>$\eta$</span> is the learning rate, <span>$\frac{dL}{dW}$</span> is the derivative of the loss function with respect to the weight, and <span>$\frac{dL}{db}$</span> is the derivative of the loss function with respect to the bias.</p><p>The derivatives are calculated using an Automatic Differentiation tool, and <code>Flux</code> uses <a href="https://github.com/FluxML/Zygote.jl"><code>Zygote.jl</code></a> for the same. Since <code>Zygote.jl</code> is an independent Julia package, it can be used outside of Flux as well! Refer to the documentation of <code>Zygote.jl</code> for more information on the same.</p><p>Our first step would be to obtain the gradient of the loss function with respect to the weights and the biases. <code>Flux</code> re-exports <code>Zygote</code>&#39;s <code>gradient</code> function; hence, we don&#39;t need to import <code>Zygote</code> explicitly to use the functionality.</p><pre><code class="language-julia-repl hljs">julia&gt; dLdW, dLdb, _, _ = gradient(custom_loss, W, b, x, y);</code></pre><p>We can now update the parameters, following the gradient descent algorithm -</p><pre><code class="language-julia-repl hljs">julia&gt; W .= W .- 0.1 .* dLdW
1-element Vector{Float32}:
 1.8144473

julia&gt; b .= b .- 0.1 .* dLdb
1-element Vector{Float32}:
 0.41325632</code></pre><p>The parameters have been updated! We can now check the value of the loss function -</p><pre><code class="language-julia-repl hljs">julia&gt; custom_loss(W, b, x, y)
17.157953f0</code></pre><p>The loss went down! This means that we successfully trained our model for one epoch. We can plug the training code written above into a loop and train the model for a higher number of epochs. It can be customized either to have a fixed number of epochs or to stop when certain conditions are met, for example, <code>change in loss &lt; 0.1</code>. The loop can be tailored to suit the user&#39;s needs, and the conditions can be specified in plain <code>Julia</code>!</p><p>Let&#39;s plug our super training logic inside a function and test it again -</p><pre><code class="language-julia-repl hljs">julia&gt; function train_custom_model()
           dLdW, dLdb, _, _ = gradient(custom_loss, W, b, x, y)
           @. W = W - 0.1 * dLdW
           @. b = b - 0.1 * dLdb
       end;

julia&gt; train_custom_model();

julia&gt; W, b, custom_loss(W, b, x, y)
(Float32[2.340657], Float32[0.7516814], 13.64972f0)</code></pre><p>It works, and the loss went down again! This was the second epoch of our training procedure. Let&#39;s plug this in a for loop and train the model for 30 epochs.</p><pre><code class="language-julia-repl hljs">julia&gt; for i = 1:40
          train_custom_model()
       end

julia&gt; W, b, custom_loss(W, b, x, y)
(Float32[4.2422233], Float32[2.2460847], 7.6680417f0)</code></pre><p>There was a significant reduction in loss, and the parameters were updated!</p><p>We can train the model even more or tweak the hyperparameters to achieve the desired result faster, but let&#39;s stop here. We trained our model for 42 epochs, and loss went down from <code>22.74856</code> to <code>7.6680417f</code>. Time for some visualization!</p><h2 id="Results"><a class="docs-heading-anchor" href="#Results">Results</a><a id="Results-1"></a><a class="docs-heading-anchor-permalink" href="#Results" title="Permalink"></a></h2><p>The main objective of this tutorial was to fit a line to our dataset using the linear regression algorithm. The training procedure went well, and the loss went down significantly! Let&#39;s see what the fitted line looks like. Remember, <code>Wx + b</code> is nothing more than a line&#39;s equation, with <code>slope = W[1]</code> and <code>y-intercept = b[1]</code> (indexing at <code>1</code> as <code>W</code> and <code>b</code> are iterable).</p><p>Plotting the line and the data points using <code>Plot.jl</code> -</p><pre><code class="language-julia-repl hljs">julia&gt; plot(reshape(x, (61, 1)), reshape(y, (61, 1)), lw = 3, seriestype = :scatter, label = &quot;&quot;, title = &quot;Simple Linear Regression&quot;, xlabel = &quot;x&quot;, ylabel= &quot;y&quot;);

julia&gt; plot!((x) -&gt; b[1] + W[1] * x, -3, 3, label=&quot;Custom model&quot;, lw=2);</code></pre><p><img src="https://user-images.githubusercontent.com/74055102/179050736-366bedcc-6990-40ee-83be-e11d07492e05.png" alt="linear-regression-line"/></p><p>The line fits well! There is room for improvement, but we leave that up to you! You can play with the optimisers, the number of epochs, learning rate, etc. to improve the fitting and reduce the loss!</p><h3 id="Linear-regression-model-on-a-real-dataset"><a class="docs-heading-anchor" href="#Linear-regression-model-on-a-real-dataset">Linear regression model on a real dataset</a><a id="Linear-regression-model-on-a-real-dataset-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-regression-model-on-a-real-dataset" title="Permalink"></a></h3><p>We now move on to a relatively complex linear regression model. Here we will use a real dataset from <a href="https://github.com/JuliaML/MLDatasets.jl"><code>MLDatasets.jl</code></a>, which will not confine our data points to have only one feature. Let&#39;s start by importing the required packages -</p><pre><code class="language-julia-repl hljs">julia&gt; using Flux, Statistics, MLDatasets, DataFrames</code></pre><h2 id="Gathering-real-data"><a class="docs-heading-anchor" href="#Gathering-real-data">Gathering real data</a><a id="Gathering-real-data-1"></a><a class="docs-heading-anchor-permalink" href="#Gathering-real-data" title="Permalink"></a></h2><p>Let&#39;s start by initializing our dataset. We will be using the <a href="https://juliaml.github.io/MLDatasets.jl/stable/datasets/misc/#MLDatasets.BostonHousing"><code>BostonHousing</code></a> dataset consisting of <code>506</code> data points. Each of these data points has <code>13</code> features and a corresponding label, the house&#39;s price. The <code>x</code>s are still mapped to a single <code>y</code>, but now, a single <code>x</code> data point has 13 features. </p><pre><code class="language-julia-repl hljs">julia&gt; dataset = BostonHousing();

julia&gt; x, y = BostonHousing(as_df=false)[:];

julia&gt; x, y = Float32.(x), Float32.(y);</code></pre><p>We can now split the obtained data into training and testing data -</p><pre><code class="language-julia-repl hljs">julia&gt; x_train, x_test, y_train, y_test = x[:, 1:400], x[:, 401:end], y[:, 1:400], y[:, 401:end];

julia&gt; x_train |&gt; size, x_test |&gt; size, y_train |&gt; size, y_test |&gt; size
((13, 400), (13, 106), (1, 400), (1, 106))</code></pre><p>This data contains a diverse number of features, which means that the features have different scales. A wise option here would be to <code>normalise</code> the data, making the training process more efficient and fast. Let&#39;s check the standard deviation of the training data before normalising it.</p><pre><code class="language-julia-repl hljs">julia&gt; std(x_train)
134.06786f0</code></pre><p>The data is indeed not normalised. We can use the <a href="../../models/layers/#Flux.normalise"><code>Flux.normalise</code></a> function to normalise the training data.</p><pre><code class="language-julia-repl hljs">julia&gt; x_train_n = Flux.normalise(x_train);

julia&gt; std(x_train_n)
1.0000844f0</code></pre><p>The standard deviation is now close to one! Our data is ready!</p><h2 id="Building-a-Flux-model"><a class="docs-heading-anchor" href="#Building-a-Flux-model">Building a Flux model</a><a id="Building-a-Flux-model-1"></a><a class="docs-heading-anchor-permalink" href="#Building-a-Flux-model" title="Permalink"></a></h2><p>We can now directly use <code>Flux</code> and let it do all the work internally! Let&#39;s define a model that takes in 13 inputs (13 features) and gives us a single output (the label). We will then pass our entire data through this model in one go, and <code>Flux</code> will handle everything for us! Remember, we could have declared a model in plain <code>Julia</code> as well. The model will have 14 parameters: 13 weights and 1 bias.</p><pre><code class="language-julia-repl hljs">julia&gt; model = Dense(13 =&gt; 1)
Dense(13 =&gt; 1)      # 14 parameters</code></pre><p>Same as before, our next step would be to define a loss function to quantify our accuracy somehow. The lower the loss, the better the model!</p><pre><code class="language-julia-repl hljs">julia&gt; function loss(model, x, y)
           ŷ = model(x)
           Flux.mse(ŷ, y)
       end;

julia&gt; loss(model, x_train_n, y_train)
676.1656f0</code></pre><p>We can now proceed to the training phase!</p><h2 id="Training-the-Flux-model"><a class="docs-heading-anchor" href="#Training-the-Flux-model">Training the Flux model</a><a id="Training-the-Flux-model-1"></a><a class="docs-heading-anchor-permalink" href="#Training-the-Flux-model" title="Permalink"></a></h2><p>The training procedure would make use of the same mathematics, but now we can pass in the model inside the <code>gradient</code> call and let <code>Flux</code> and <code>Zygote</code> handle the derivatives!</p><pre><code class="language-julia-repl hljs">julia&gt; function train_model()
           dLdm, _, _ = gradient(loss, model, x_train_n, y_train)
           @. model.weight = model.weight - 0.000001 * dLdm.weight
           @. model.bias = model.bias - 0.000001 * dLdm.bias
       end;</code></pre><p>Contrary to our last training procedure, let&#39;s say that this time we don&#39;t want to hardcode the number of epochs. We want the training procedure to stop when the loss converges, that is, when <code>change in loss &lt; δ</code>. The quantity <code>δ</code> can be altered according to a user&#39;s need, but let&#39;s fix it to <code>10⁻³</code> for this tutorial.</p><p>We can write such custom training loops effortlessly using <code>Flux</code> and plain <code>Julia</code>!</p><pre><code class="language-julia-repl hljs">julia&gt; loss_init = Inf;

julia&gt; while true
           train_model()
           if loss_init == Inf
               loss_init = loss(model, x_train_n, y_train)
               continue
           end
           if abs(loss_init - loss(model, x_train_n, y_train)) &lt; 1e-4
               break
           else
               loss_init = loss(model, x_train_n, y_train)
           end
       end;</code></pre><p>The code starts by initializing an initial value for the loss, <code>infinity</code>. Next, it runs an infinite loop that breaks if <code>change in loss &lt; 10⁻³</code>, or the code changes the value of <code>loss_init</code> to the current loss and moves on to the next iteration.</p><p>This custom loop works! This shows how easily a user can write down any custom training routine using Flux and Julia!</p><p>Let&#39;s have a look at the loss -</p><pre><code class="language-julia-repl hljs">julia&gt; loss(model, x_train_n, y_train)
27.1272f0</code></pre><p>The loss went down significantly! It can be minimized further by choosing an even smaller <code>δ</code>.</p><h2 id="Testing-the-Flux-model"><a class="docs-heading-anchor" href="#Testing-the-Flux-model">Testing the Flux model</a><a id="Testing-the-Flux-model-1"></a><a class="docs-heading-anchor-permalink" href="#Testing-the-Flux-model" title="Permalink"></a></h2><p>The last step of this tutorial would be to test our model using the testing data. We will first normalise the testing data and then calculate the corresponding loss.</p><pre><code class="language-julia-repl hljs">julia&gt; x_test_n = Flux.normalise(x_test);

julia&gt; loss(model, x_test_n, y_test)
66.91015f0</code></pre><p>The loss is not as small as the loss of the training data, but it looks good! This also shows that our model is not overfitting!</p><hr/><p>Summarising this tutorial, we started by generating a random yet correlated dataset for our <code>custom model</code>. We then saw how a simple linear regression model could be built with and without <code>Flux</code>, and how they were almost identical. </p><p>Next, we trained the model by manually writing down the Gradient Descent algorithm and optimising the loss. We also saw how <code>Flux</code> provides various wrapper functionalities and keeps the API extremely intuitive and simple for the users. </p><p>After getting familiar with the basics of <code>Flux</code> and <code>Julia</code>, we moved ahead to build a machine learning model for a real dataset. We repeated the exact same steps, but this time with a lot more features and data points, and by harnessing <code>Flux</code>&#39;s full capabilities. In the end, we developed a training loop that was smarter than the hardcoded one and ran the model on our normalised dataset to conclude the tutorial.</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Originally published on 21 November 2022, by Saransh Chopra.</p></div></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../models/functors/">« Nested Structures – Functors.jl</a><a class="docs-footer-nextpage" href="../logistic_regression/">Logistic Regression »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Thursday 7 September 2023 16:43">Thursday 7 September 2023</span>. Using Julia version 1.9.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
