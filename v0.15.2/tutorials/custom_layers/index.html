<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Custom Layers · Flux</title><meta name="title" content="Custom Layers · Flux"/><meta property="og:title" content="Custom Layers · Flux"/><meta property="twitter:title" content="Custom Layers · Flux"/><meta name="description" content="Documentation for Flux."/><meta property="og:description" content="Documentation for Flux."/><meta property="twitter:description" content="Documentation for Flux."/><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-36890222-9', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/flux.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="Flux logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="Flux logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Welcome</a></li><li><span class="tocitem">Guide</span><ul><li><a class="tocitem" href="../../guide/models/quickstart/">Quick Start</a></li><li><a class="tocitem" href="../../guide/models/overview/">Fitting a Line</a></li><li><a class="tocitem" href="../../guide/models/basics/">Gradients and Layers</a></li><li><a class="tocitem" href="../../guide/training/training/">Training</a></li><li><a class="tocitem" href="../../guide/models/recurrence/">Recurrence</a></li><li><a class="tocitem" href="../../guide/gpu/">GPU Support</a></li><li><a class="tocitem" href="../../guide/saving/">Saving &amp; Loading</a></li><li><a class="tocitem" href="../../guide/performance/">Performance Tips</a></li></ul></li><li><a class="tocitem" href="../../ecosystem/">Ecosystem</a></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../../reference/models/layers/">Built-in Layers</a></li><li><a class="tocitem" href="../../reference/models/activation/">Activation Functions</a></li><li><a class="tocitem" href="../../reference/utilities/">Weight Initialisation</a></li><li><a class="tocitem" href="../../reference/models/losses/">Loss Functions</a></li><li><a class="tocitem" href="../../reference/training/reference/">Training API</a></li><li><a class="tocitem" href="../../reference/training/optimisers/">Optimisation Rules</a></li><li><a class="tocitem" href="../../reference/outputsize/">Shape Inference</a></li><li><a class="tocitem" href="../../reference/destructure/">Flat vs. Nested</a></li><li><a class="tocitem" href="../../reference/training/callbacks/">Callback Helpers</a></li><li><a class="tocitem" href="../../reference/training/zygote/">Gradients – Zygote.jl</a></li><li><a class="tocitem" href="../../reference/training/enzyme/">Gradients – Enzyme.jl</a></li><li><a class="tocitem" href="../../reference/data/mldatadevices/">Transfer Data to GPU – MLDataDevices.jl</a></li><li><a class="tocitem" href="../../reference/data/mlutils/">Batching Data – MLUtils.jl</a></li><li><a class="tocitem" href="../../reference/data/onehot/">OneHotArrays.jl</a></li><li><a class="tocitem" href="../../reference/models/nnlib/">Low-level Operations – NNlib.jl</a></li><li><a class="tocitem" href="../../reference/models/functors/">Nested Structures – Functors.jl</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../linear_regression/">Linear Regression</a></li><li><a class="tocitem" href="../logistic_regression/">Logistic Regression</a></li><li class="is-active"><a class="tocitem" href>Custom Layers</a><ul class="internal"><li><a class="tocitem" href="#Custom-Model-Example"><span>Custom Model Example</span></a></li><li><a class="tocitem" href="#Customising-Parameter-Collection-for-a-Model"><span>Customising Parameter Collection for a Model</span></a></li><li><a class="tocitem" href="#Custom-multiple-input-or-output-layer"><span>Custom multiple input or output layer</span></a></li></ul></li><li><a class="tocitem" href="../model_zoo/">Model Zoo</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Custom Layers</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Custom Layers</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/FluxML/Flux.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/FluxML/Flux.jl/blob/master/docs/src/tutorials/custom_layers.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="man-advanced"><a class="docs-heading-anchor" href="#man-advanced">Defining Customised Layers</a><a id="man-advanced-1"></a><a class="docs-heading-anchor-permalink" href="#man-advanced" title="Permalink"></a></h1><p>Here we will try and describe usage of some more advanced features that Flux provides to give more control over model building.</p><h2 id="Custom-Model-Example"><a class="docs-heading-anchor" href="#Custom-Model-Example">Custom Model Example</a><a id="Custom-Model-Example-1"></a><a class="docs-heading-anchor-permalink" href="#Custom-Model-Example" title="Permalink"></a></h2><p>Here is a basic example of a custom model. It simply adds the input to the result from the neural network.</p><pre><code class="language-julia hljs">struct CustomModel{T &lt;: Chain} # Parameter to avoid type instability
  chain::T
end

function (m::CustomModel)(x)
  # Arbitrary code can go here, but note that everything will be differentiated.
  # Zygote does not allow some operations, like mutating arrays.

  return m.chain(x) + x
end

# This is optional but recommended for pretty printing and other niceties
Flux.@layer CustomModel</code></pre><p>Notice that we parameterized the type of the <code>chain</code> field. This is necessary for fast Julia code, so that that struct field can be given a concrete type. <code>Chain</code>s have a type parameter fully specifying the types of the layers they contain. By using a type parameter, we are freeing Julia to determine the correct concrete type, so that we do not need to specify the full, possibly quite long, type ourselves.</p><p>You can then use the model like:</p><pre><code class="language-julia hljs">chain = Chain(Dense(10 =&gt; 10, relu), Dense(10 =&gt; 10))
model = CustomModel(chain)
model(rand(Float32, 10))</code></pre><p>For an intro to Flux and automatic differentiation, see this <a href="https://fluxml.ai/tutorials/2020/09/15/deep-learning-flux.html">tutorial</a>.</p><h2 id="Customising-Parameter-Collection-for-a-Model"><a class="docs-heading-anchor" href="#Customising-Parameter-Collection-for-a-Model">Customising Parameter Collection for a Model</a><a id="Customising-Parameter-Collection-for-a-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Customising-Parameter-Collection-for-a-Model" title="Permalink"></a></h2><p>Taking reference from our example <code>Affine</code> layer from the <a href="../../guide/models/basics/#man-basics">basics</a>.</p><p>By default all the fields in the <code>Affine</code> type are collected as its parameters, however, in some cases it may be desired to hold other metadata in our &quot;layers&quot; that may not be needed for training, and are hence supposed to be ignored while the parameters are collected. With Flux, the way to mark some fields of our layer as trainable is through overloading the <code>trainable</code> function:</p><pre><code class="language-julia-repl hljs">julia&gt; struct Affine
        W
        b
      end

julia&gt; Affine(in::Int, out::Int) = Affine(randn(out, in), randn(out));

julia&gt; (m::Affine)(x) = m.W * x .+ m.b;

julia&gt; Flux.@layer Affine

julia&gt; a = Affine(Float32[1 2; 3 4; 5 6], Float32[7, 8, 9])
Affine(Float32[1.0 2.0; 3.0 4.0; 5.0 6.0], Float32[7.0, 8.0, 9.0])

julia&gt; Flux.trainable(a) # default behavior
(W = Float32[1.0 2.0; 3.0 4.0; 5.0 6.0], b = Float32[7.0, 8.0, 9.0])

julia&gt; Flux.trainable(a::Affine) = (; W = a.W)  # returns a NamedTuple using the field&#39;s name

julia&gt; Flux.trainable(a)
(W = Float32[1.0 2.0; 3.0 4.0; 5.0 6.0],)</code></pre><p>Only the fields returned by <code>trainable</code> will be seen by <code>Flux.setup</code> and <code>Flux.update!</code> for training. But all fields wil be seen by <code>gpu</code> and similar functions, for example:</p><pre><code class="language-julia-repl hljs">julia&gt; a |&gt; f16
Affine(Float16[1.0 2.0; 3.0 4.0; 5.0 6.0], Float16[7.0, 8.0, 9.0])</code></pre><p>Note that there is no need to overload <code>trainable</code> to hide fields which do not contain numerical array (for example, activation functions, or Boolean flags). These are always ignored by training.</p><p>The exact same method of <code>trainable</code> can also be defined using the macro, for convenience:</p><pre><code class="language-julia hljs">Flux.@layer Affine trainable=(W,)</code></pre><p>There is a second, more severe, kind of restriction possible. This is not recommended, but is included here for completeness. Calling <code>Functors.@functor Affine (W,)</code> means that no exploration of the model will ever visit the other fields: They will not be moved to the GPU by <a href="../../reference/models/functors/#Flux.gpu-Tuple{Any}"><code>gpu</code></a>, and their precision will not be changed by <code>f32</code>. This requires the <code>struct</code> to have a corresponding constructor that accepts only <code>W</code> as an argument.</p><h2 id="Custom-multiple-input-or-output-layer"><a class="docs-heading-anchor" href="#Custom-multiple-input-or-output-layer">Custom multiple input or output layer</a><a id="Custom-multiple-input-or-output-layer-1"></a><a class="docs-heading-anchor-permalink" href="#Custom-multiple-input-or-output-layer" title="Permalink"></a></h2><p>Sometimes a model needs to receive several separate inputs at once or produce several separate outputs at once. In other words, there multiple paths within this high-level layer, each processing a different input or producing a different output. A simple example of this in machine learning literature is the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Szegedy_Rethinking_the_Inception_CVPR_2016_paper.pdf">inception module</a>.</p><p>We could have a struct that stores the weights of along each path and implement the joining/splitting in the forward pass function. That would mean a new struct for each different block, e.g. one would have a <code>TransformerBlock</code> struct for a transformer block, and a <code>ResNetBlock</code> struct for a ResNet block, each block being composed by smaller sub-blocks. This is often the simplest and cleanest way to implement complex models.</p><p>This guide instead will show you how to construct a high-level layer (like <a href="../../reference/models/layers/#Flux.Chain"><code>Chain</code></a>) that is made of multiple sub-layers for each path. It may be the case that using the layers described as follows makes the definition of your model harder to read and to change. In that case, consider using the simpler approach of defining a custom structure described above.</p><h3 id="Multiple-inputs:-a-custom-Join-layer"><a class="docs-heading-anchor" href="#Multiple-inputs:-a-custom-Join-layer">Multiple inputs: a custom <code>Join</code> layer</a><a id="Multiple-inputs:-a-custom-Join-layer-1"></a><a class="docs-heading-anchor-permalink" href="#Multiple-inputs:-a-custom-Join-layer" title="Permalink"></a></h3><p>Our custom <code>Join</code> layer will accept multiple inputs at once, pass each input through a separate path, then combine the results together. Note that this layer can already be constructed using <a href="../../reference/models/layers/#Flux.Parallel"><code>Parallel</code></a>, but we will first walk through how do this manually.</p><p>We start by defining a new struct, <code>Join</code>, that stores the different paths and a combine operation as its fields.</p><pre><code class="language-julia hljs">using Flux
using CUDA

# custom join layer
struct Join{T, F}
  combine::F
  paths::T
end

# allow Join(op, m1, m2, ...) as a constructor
Join(combine, paths...) = Join(combine, paths)</code></pre><p>Notice again that we parameterized the type of the <code>combine</code> and <code>paths</code> fields. In addition to the performance considerations of concrete types, this allows either field to be <code>Vector</code>s, <code>Tuple</code>s, or one of each - we don&#39;t need to pay attention to which.</p><p>The next step is to use <a href="../../reference/models/functors/#Flux.@layer"><code>Flux.@layer</code></a> to make our struct behave like a Flux layer.  In Flux &lt; v0.15 this used to be important so that calling <code>Flux.setup</code> on a <code>Join</code> maps over the underlying trainable arrays on each path. Since Flux v0.15, this is no longer necessary, since now Functors.jl automatically traverses custom types. However, <a href="../../reference/models/functors/#Flux.@layer"><code>Flux.@layer</code></a> is still recommended for pretty printing and other niceties.</p><pre><code class="language-julia hljs">Flux.@layer Join</code></pre><p>Finally, we define the forward pass. For <code>Join</code>, this means applying each <code>path</code> in <code>paths</code> to each input array, then using <code>combine</code> to merge the results.</p><pre><code class="language-julia hljs">(m::Join)(xs::Tuple) = m.combine(map((f, x) -&gt; f(x), m.paths, xs)...)
(m::Join)(xs...) = m(xs)</code></pre><p>Lastly, we can test our new layer. Thanks to the proper abstractions in Julia, our layer works on GPU arrays out of the box!</p><pre><code class="language-julia hljs">model = Chain(
              Join(vcat,
                   Chain(Dense(1 =&gt; 5, relu), Dense(5 =&gt; 1)), # branch 1
                   Dense(1 =&gt; 2),                             # branch 2
                   Dense(1 =&gt; 1)                              # branch 3
                  ),
              Dense(4 =&gt; 1)
             ) |&gt; gpu

xs = map(gpu, (rand(1), rand(1), rand(1)))

model(xs)
# returns a single float vector with one value</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>This <code>Join</code> layer is available from the <a href="https://github.com/FluxML/Fluxperimental.jl">Fluxperimental.jl</a> package.</p></div></div><h4 id="Using-Parallel"><a class="docs-heading-anchor" href="#Using-Parallel">Using <code>Parallel</code></a><a id="Using-Parallel-1"></a><a class="docs-heading-anchor-permalink" href="#Using-Parallel" title="Permalink"></a></h4><p>Flux already provides <a href="../../reference/models/layers/#Flux.Parallel"><code>Parallel</code></a> that can offer the same functionality. In this case, <code>Join</code> is going to just be syntactic sugar for <code>Parallel</code>.</p><pre><code class="language-julia hljs">Join(combine, paths) = Parallel(combine, paths)
Join(combine, paths...) = Join(combine, paths)

# use vararg/tuple version of Parallel forward pass
model = Chain(
              Join(vcat,
                   Chain(Dense(1 =&gt; 5, relu), Dense(5 =&gt; 1)),
                   Dense(1 =&gt; 2),
                   Dense(1 =&gt; 1)
                  ),
              Dense(4 =&gt; 1)
             ) |&gt; gpu

xs = map(gpu, (rand(1), rand(1), rand(1)))

model(xs)
# returns a single float vector with one value</code></pre><h3 id="Multiple-outputs:-a-custom-Split-layer"><a class="docs-heading-anchor" href="#Multiple-outputs:-a-custom-Split-layer">Multiple outputs: a custom <code>Split</code> layer</a><a id="Multiple-outputs:-a-custom-Split-layer-1"></a><a class="docs-heading-anchor-permalink" href="#Multiple-outputs:-a-custom-Split-layer" title="Permalink"></a></h3><p>Our custom <code>Split</code> layer will accept a single input, then pass the input through a separate path to produce multiple outputs.</p><p>We start by following the same steps as the <code>Join</code> layer: define a struct, use <a href="../../reference/models/functors/#Flux.@layer"><code>Flux.@layer</code></a>, and define the forward pass.</p><pre><code class="language-julia hljs">using Flux
using CUDA

# custom split layer
struct Split{T}
  paths::T
end

Split(paths...) = Split(paths)

Flux.@layer Split

(m::Split)(x::AbstractArray) = map(f -&gt; f(x), m.paths)</code></pre><p>Now we can test to see that our <code>Split</code> does indeed produce multiple outputs.</p><pre><code class="language-julia hljs">model = Chain(
              Dense(10 =&gt; 5),
              Split(Dense(5 =&gt; 1, tanh), Dense(5 =&gt; 3, tanh), Dense(5 =&gt; 2))
             ) |&gt; gpu

model(gpu(rand(10)))
# returns a tuple with three float vectors</code></pre><p>A custom loss function for the multiple outputs may look like this:</p><pre><code class="language-julia hljs">using Statistics

# assuming model returns the output of a Split
# x is a single input
# ys is a tuple of outputs
function loss(x, ys, model)
  # rms over all the mse
  ŷs = model(x)
  return sqrt(mean(Flux.mse(y, ŷ) for (y, ŷ) in zip(ys, ŷs)))
end</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>This <code>Split</code> layer is available from the <a href="https://github.com/FluxML/Fluxperimental.jl">Fluxperimental.jl</a> package.</p></div></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../logistic_regression/">« Logistic Regression</a><a class="docs-footer-nextpage" href="../model_zoo/">Model Zoo »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Wednesday 11 December 2024 15:10">Wednesday 11 December 2024</span>. Using Julia version 1.11.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
