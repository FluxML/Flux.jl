<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Gradients and Layers · Flux</title><meta name="title" content="Gradients and Layers · Flux"/><meta property="og:title" content="Gradients and Layers · Flux"/><meta property="twitter:title" content="Gradients and Layers · Flux"/><meta name="description" content="Documentation for Flux."/><meta property="og:description" content="Documentation for Flux."/><meta property="twitter:description" content="Documentation for Flux."/><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-36890222-9', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../../assets/documenter.js"></script><script src="../../../search_index.js"></script><script src="../../../siteinfo.js"></script><script src="../../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../../assets/themeswap.js"></script><link href="../../../assets/flux.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../../"><img class="docs-light-only" src="../../../assets/logo.png" alt="Flux logo"/><img class="docs-dark-only" src="../../../assets/logo-dark.png" alt="Flux logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../../">Welcome</a></li><li><span class="tocitem">Guide</span><ul><li><a class="tocitem" href="../quickstart/">Quick Start</a></li><li><a class="tocitem" href="../overview/">Fitting a Line</a></li><li class="is-active"><a class="tocitem" href>Gradients and Layers</a><ul class="internal"><li><a class="tocitem" href="#Parameterised-Functions"><span>Parameterised Functions</span></a></li><li><a class="tocitem" href="#man-taking-gradients"><span>Structural Gradients</span></a></li><li><a class="tocitem" href="#Simple-Neural-Networks"><span>Simple Neural Networks</span></a></li><li><a class="tocitem" href="#Curve-Fitting"><span>Curve Fitting</span></a></li></ul></li><li><a class="tocitem" href="../../training/training/">Training</a></li><li><a class="tocitem" href="../recurrence/">Recurrence</a></li><li><a class="tocitem" href="../../gpu/">GPU Support</a></li><li><a class="tocitem" href="../../saving/">Saving &amp; Loading</a></li><li><a class="tocitem" href="../../performance/">Performance Tips</a></li></ul></li><li><a class="tocitem" href="../../../ecosystem/">Ecosystem</a></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../../../reference/models/layers/">Built-in Layers</a></li><li><a class="tocitem" href="../../../reference/models/activation/">Activation Functions</a></li><li><a class="tocitem" href="../../../reference/utilities/">Weight Initialisation</a></li><li><a class="tocitem" href="../../../reference/models/losses/">Loss Functions</a></li><li><a class="tocitem" href="../../../reference/training/reference/">Training API</a></li><li><a class="tocitem" href="../../../reference/training/optimisers/">Optimisation Rules</a></li><li><a class="tocitem" href="../../../reference/outputsize/">Shape Inference</a></li><li><a class="tocitem" href="../../../reference/destructure/">Flat vs. Nested</a></li><li><a class="tocitem" href="../../../reference/training/callbacks/">Callback Helpers</a></li><li><a class="tocitem" href="../../../reference/training/zygote/">Gradients – Zygote.jl</a></li><li><a class="tocitem" href="../../../reference/training/enzyme/">Gradients – Enzyme.jl</a></li><li><a class="tocitem" href="../../../reference/data/mldatadevices/">Transfer Data to GPU – MLDataDevices.jl</a></li><li><a class="tocitem" href="../../../reference/data/mlutils/">Batching Data – MLUtils.jl</a></li><li><a class="tocitem" href="../../../reference/data/onehot/">OneHotArrays.jl</a></li><li><a class="tocitem" href="../../../reference/models/nnlib/">Low-level Operations – NNlib.jl</a></li><li><a class="tocitem" href="../../../reference/models/functors/">Nested Structures – Functors.jl</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../../tutorials/linear_regression/">Linear Regression</a></li><li><a class="tocitem" href="../../../tutorials/logistic_regression/">Logistic Regression</a></li><li><a class="tocitem" href="../../../tutorials/custom_layers/">Custom Layers</a></li><li><a class="tocitem" href="../../../tutorials/model_zoo/">Model Zoo</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Guide</a></li><li class="is-active"><a href>Gradients and Layers</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Gradients and Layers</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/FluxML/Flux.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/FluxML/Flux.jl/blob/master/docs/src/guide/models/basics.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="man-basics"><a class="docs-heading-anchor" href="#man-basics">How Flux Works: Parameters, Gradients, and Layers</a><a id="man-basics-1"></a><a class="docs-heading-anchor-permalink" href="#man-basics" title="Permalink"></a></h1><p>A neural network is a function with <em>parameters</em>. That is, it takes some input <code>x</code> and gives you some output <code>y</code>, whose value also depends on some other numbers <code>θ</code>.</p><p>A sufficiently flexible function can, by adjusting the parameters just right, be made to do many things. And the one magic trick for adjusting parameters is to follow a <em>gradient</em>.</p><p>This page describes Flux&#39;s take on how to construct such flexible functions containing many parameters, and how to handle their gradients.</p><h2 id="Parameterised-Functions"><a class="docs-heading-anchor" href="#Parameterised-Functions">Parameterised Functions</a><a id="Parameterised-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Parameterised-Functions" title="Permalink"></a></h2><p>Let&#39;s start with very simple functions. This is a polynomial in <code>x::Real</code>, returning another real number <code>y</code> which depends on some coefficients stored in a vector:</p><pre><code class="language-julia hljs">θ = [10, 1, 0.1]

poly1(x::Real) = θ[1] + θ[2]*x + θ[3]*x^2

poly1(5) == 17.5  # true</code></pre><p>Here the parameters are a global variable <code>θ</code>. They could be handled in other ways, for instance by explicitly passing them as an additional argument to the function:</p><pre><code class="language-julia hljs">poly2(x::Real, θ2) = evalpoly(x, θ2)  # built-in, from Base.Math

poly2(5, θ) == 17.5  # true</code></pre><p>Flux chooses a third path, by <em>encapsulating</em> the parameters within the function. The simplest way to do this is a <em>closure</em>, an anonymous function which Julia knows to depend on some local variable <code>θ3</code>:</p><pre><code class="language-julia hljs">poly3 = let θ3 = [10, 1, 0.1]
    x -&gt; evalpoly(x, θ3)
end

poly3(5) == 17.5  # true</code></pre><p>An equivalent, but tidier, way is to construct a <code>struct</code> in which to store the parameters. Any struct can be made callable, allowing its instances to act just like function:</p><pre><code class="language-julia hljs">struct Poly3{T}  # container struct
    θ3::T
end
(p::Poly3)(x::Real) = evalpoly(x, p.θ3)  # make this callable

poly3s = Poly3([10, 1, 0.1])  # construct an instance

poly3s(5) == 17.5  # true</code></pre><p>Internally, there is little difference between a closure and a struct. They have the same fields, and equivalent methods:</p><pre><code class="language-julia hljs">dump(poly3), dump(poly3s)  # both contain θ3: Array
poly3s.θ3 == poly3.θ3 == θ  # field called :θ3 has same value
methods(poly3)
methods(poly3s)  # each has 1 method, accepting x</code></pre><p>The virtue of encapsulation is that it makes composition very easy. We can make more complicated functions by combining simple ones, and each will keep track of its own parameters. Juia writes function composition as <code>∘</code>, for instance <code>(inv ∘ sin)(pi/6) ≈ 2</code>, and we can use exactly this for our parameterised polynomials:</p><pre><code class="language-julia hljs">poly4 = Poly3([1, 0.5, 0]) ∘ Poly3([10, 1, 0.1])

poly4 isa ComposedFunction  # ∘ creates another struct...
poly4.outer.θ3 == θ         # which has fields :inner &amp; :outer

poly4(5) == 9.75  # true</code></pre><p>Flux models are precisely made by such function composition. In fact, <code>poly3</code>  and <code>poly4</code> are already valid Flux models.</p><h2 id="man-taking-gradients"><a class="docs-heading-anchor" href="#man-taking-gradients">Structural Gradients</a><a id="man-taking-gradients-1"></a><a class="docs-heading-anchor-permalink" href="#man-taking-gradients" title="Permalink"></a></h2><p>The derivative of a scalar function is its slope: how fast the output changes as the input is changed slightly. This may be found approximately by evaluating at two nearby points, and exactly by taking the limit in which the distance between them approaches zero:</p><pre><code class="language-julia-repl hljs">julia&gt; (poly1(5 + 0.1) - poly1(5)) / 0.1
2.010000000000005

julia&gt; (poly1(5 + 0.001) - poly1(5)) / 0.001  # answer is getting close to 2
2.000100000003613</code></pre><p>Flux&#39;s <code>gradient(f, x)</code> works this out for <code>f(x)</code>, and gives exactly <code>∂f/∂x = 2.0</code> here:</p><pre><code class="language-julia-repl hljs">julia&gt; using Flux

julia&gt; gradient(poly1, 5)
(2.0,)</code></pre><p>The reason <code>gradient</code> returns a tuple, not just the number <code>2.0</code>, is to allow for functions taking several arguments. (That&#39;s also why it&#39;s not called &quot;derivative&quot;.) For instance, this returns <code>∂f/∂x, ∂f/∂y, ∂f/∂z</code>:</p><pre><code class="language-julia-repl hljs">julia&gt; gradient((x,y,z) -&gt; (x*y)+z, 30, 40, 50)
(40.0, 30.0, 1.0)</code></pre><p>For our parameterised polynomial, we have <code>∂f/∂x</code> but we are really more interested in <code>∂f/∂θ</code>, as this will tell us about how the parameters are affecting the answer. It is not impossible to track gradients with respect to global <code>θ</code>, but much clearer to track explicit arguments. Here&#39;s how this works for <code>poly2</code> (which takes <code>θ</code> as a 2nd argument) and <code>poly3</code> (which encapsulates <code>θ</code>):</p><pre><code class="language-julia-repl hljs">julia&gt; grad2 = gradient(poly2, 5, θ)
(2.0, [1.0, 5.0, 25.0])

julia&gt; grad3 = gradient((x,p) -&gt; p(x), 5, poly3s)
(2.0, (θ3 = [1.0, 5.0, 25.0],))</code></pre><p>The first entry is <code>∂f/∂x</code> as before, but the second entry is more interesting. For <code>poly2</code>, we get <code>∂f/∂θ</code> as <code>grad2[2]</code> directly. It is a vector, because <code>θ</code> is a vector, and has elements <code>[∂f/∂θ[1], ∂f/∂θ[2], ∂f/∂θ[3]]</code>.</p><p>For <code>poly3s</code>, however, we get a <code>NamedTuple</code> whose fields correspond to those of the struct <code>Poly3</code>. This is called a <em>structural gradient</em>. And the nice thing about them is that they work for arbitrarily complicated structures, for instance:</p><pre><code class="language-julia-repl hljs">julia&gt; grad4 = gradient(|&gt;, 5, poly4)
(1.0, (outer = (θ3 = [1.0, 17.5, 306.25],), inner = (θ3 = [0.5, 2.5, 12.5],)))</code></pre><p>Here <code>grad4.inner.θ3</code> corresponds to <code>poly4.inner.θ3</code>. These matching nested structures are at the core of how Flux works.</p><div class="admonition is-info"><header class="admonition-header">Implicit gradients</header><div class="admonition-body"><p>Earlier versions of Flux used a different way to relate parameters and gradients, which looks like this:</p><pre><code class="language-julia hljs">g1 = gradient(() -&gt; poly1(5), Params([θ]))
g1[θ] == [1.0, 5.0, 25.0]</code></pre><p>Here <code>Params</code> is a set of references to global variables using <code>objectid</code>, and <code>g1 isa Grads</code> is a dictionary from these to their gradients. This method of <code>gradient</code> takes a zero-argument function, which only <em>implicitly</em> depends on <code>θ</code>.</p></div></div><h3><img src="../../../assets/zygote-crop.png" width="40px"/>&nbsp;<a href="https://github.com/FluxML/Zygote.jl">Zygote.jl</a></h3><p>Flux&#39;s <a href="@ref"><code>gradient</code></a> function by default calls a companion packages called <a href="https://github.com/FluxML/Zygote.jl">Zygote</a>. Zygote performs source-to-source automatic differentiation, meaning that <code>gradient(f, x)</code> hooks into Julia&#39;s compiler to find out what operations <code>f</code> contains, and transforms this to produce code for computing <code>∂f/∂x</code>.</p><p>Zygote can in principle differentiate almost any Julia code. However, it&#39;s not perfect, and you may eventually want to read its <a href="https://fluxml.ai/Zygote.jl/dev/limitations/">page about limitations</a>. In particular, a major limitation is that mutating an array is not allowed.</p><p>Flux can also be used with other automatic differentiation (AD) packages. It was originally written using <a href="https://github.com/FluxML/Tracker.jl">Tracker</a>, a more traditional operator-overloading approach. The future might be <a href="https://github.com/EnzymeAD/Enzyme.jl">Enzyme</a>, and Flux now builds in an easy way to use this instead, turned on by wrapping the model in <code>Duplicated</code>. (For details, see the <a href="../../../reference/training/enzyme/#autodiff-enzyme">Enzyme page</a> in the manual.)</p><pre><code class="language-julia hljs">julia&gt; using Enzyme: Const, Duplicated

julia&gt; grad3e = Flux.gradient((x,p) -&gt; p(x), Const(5.0), Duplicated(poly3s))
(nothing, (θ3 = [1.0, 5.0, 25.0],))</code></pre><p><code>Flux.gradient</code> follows Zygote&#39;s convention that arguments with no derivative are marked <code>nothing</code>. Here, this is because <code>Const(5.0)</code> is explicitly constant. Below, we will see an example where <code>nothing</code> shows up because the model struct has fields containing things other than parameters, such as an activation function. (It also adopts the convention that <code>gradient(f, x, y)</code> returns a tuple <code>(∂f/∂x, ∂f/∂y)</code>, without a &quot;<code>∂f/∂f</code>&quot; term for the function. This is why we had to write <code>gradient(|&gt;, 5, poly4)</code> above, not just <code>gradient(poly4, 5)</code>.)</p><p>Finally, the function <a href="../../../reference/training/zygote/#Zygote.withgradient-Tuple{Any, Vararg{Any}}"><code>withgradient</code></a> works the same way, but also returns the value of the function:</p><pre><code class="language-julia-repl hljs">julia&gt; Flux.withgradient((x,p) -&gt; p(x), 5.0, poly3s)
(val = 17.5, grad = (2.0, (θ3 = [1.0, 5.0, 25.0],)))</code></pre><h2 id="Simple-Neural-Networks"><a class="docs-heading-anchor" href="#Simple-Neural-Networks">Simple Neural Networks</a><a id="Simple-Neural-Networks-1"></a><a class="docs-heading-anchor-permalink" href="#Simple-Neural-Networks" title="Permalink"></a></h2><p>The polynomial functions above send a number <code>x</code> to another a number <code>y</code>. Neural networks typically take a vector of numbers, mix them all up, and return another vector. Here&#39;s a very simple one, which will take a vector like <code>x = [1.0, 2.0, 3.0]</code> and return another vector <code>y = layer1(x)</code> with <code>length(y) == 2</code>:</p><pre><code class="language-julia hljs">W = randn(2, 3)
b = zeros(2)

sigmoid(x::Real) = 1 / (1 + exp(-x))
layer1(x) = sigmoid.(W*x .+ b)</code></pre><p>Here <code>sigmoid</code> is a nonlinear function, applied element-wise because it is called with <code>.()</code>, called broadcasting.</p><p>Like <code>poly1</code> above, this <code>layer1</code> has as its parameters the global variables <code>W, b</code>. We can similarly define a version which takes these as arguments (like <code>poly2</code>), and a version which encapsulates them (like <code>poly3</code> above):</p><pre><code class="language-julia hljs">layer2(x, W2, b2) = sigmoid.(W2*x .+ b2)  # explicit parameter arguments

layer3 = let
    W3 = randn(2, 3)
    b3 = zeros(2)
    x -&gt; sigmoid.(W3*x .+ b3)  # closure over local variables
end

layer3([1.0, 2.0, 3.0]) isa Vector  # check that it runs</code></pre><p>This third way is precisely a Flux model. And we can again make a tidier version using a <code>struct</code> to hold the parameters:</p><pre><code class="language-julia hljs">struct Layer  # container struct
    W::Matrix
    b::Vector
    act::Function
end

(d::Layer)(x) = d.act.(d.W*x .+ d.b)  # make it callabale

Layer(in::Int, out::Int, act::Function=sigmoid) =
  Layer(randn(Float32, out, in), zeros(Float32, out), act)

layer3s = Layer(3, 2)  # instance with its own parameters</code></pre><p>The one new thing here is a friendly constructor <code>Layer(in, out, act)</code>. This is because we anticipate composing several instances of this thing, with independent parameter arrays, of different sizes and different random initial parameters.</p><p>Let&#39;s try this out, and look at its gradient:</p><pre><code class="language-julia hljs">x = Float32[0.1, 0.2, 0.3]  # input

layer3s(x)  # output, 2-element Vector{Float32}

Flux.gradient((x,d) -&gt; d(x)[1], x, layer3s)[2]  # NamedTuple{(:W, :b, :act)}</code></pre><p>This <code>∂f/∂layer3s</code> is a named tuple with the same fields as <code>Layer</code>. Within it, the gradient with respect to <code>W</code> is a matrix of seemingly random numbers. Notice that there is also an entry for <code>act</code>, which is <code>nothing</code>, as this field of the struct is not a smoothly adjustible parameter.</p><p>We can compose these layers just as we did the polynomials above, in <code>poly4</code>. Here&#39;s a composition of 3 functions, in which the last step is the function <code>only</code> which takes a 1-element vector and gives us the number inside:</p><pre><code class="language-julia hljs">model1 = only ∘ Layer(20, 1) ∘ Layer(1, 20)

y = model1(Float32[0.1])  # output is a Float32 number

grad = Flux.gradient(|&gt;, [1f0], model1)[2]</code></pre><p>This gradient is starting to be a complicated nested structure. But it works just like before: <code>grad.outer.inner.W</code> corresponds to <code>model1.outer.inner.W</code>.</p><p>We don&#39;t have to use <code>∘</code> (which makes a <code>ComposedFunction</code> struct) to combine layers. Instead, we could define our own container struct, or use a closure. This <code>model2</code> will work the same way (although its fields have different names):</p><pre><code class="language-julia hljs">model2 = let
    lay1 = Layer(1, 20)  # local variables containing layers
    lay2 = Layer(20, 1)
    function fwd(x)  # equivalent to x -&gt; only(lay2(lay1(x)))
        mid = lay1(x)
        lay2(mid) |&gt; only
    end
end

model2(Float32[0.1])

Flux.gradient(|&gt;, [1f0], model2)[2]</code></pre><h3><img src="https://github.com/FluxML/Optimisers.jl/blob/master/docs/src/assets/logo.png?raw=true" width="40px"/>&nbsp;<a href="../../../reference/models/layers/">Flux's layers</a></h3><p>Rather than define everything from scratch every time, Flux provides a library of commonly used layers. The same model could be defined:</p><pre><code class="language-julia hljs">model3 = Chain(Dense(1 =&gt; 20, σ), Dense(20 =&gt; 1), only)</code></pre><p>How does this <code>model3</code> differ from the <code>model1</code> we had before?</p><ul><li>Flux&#39;s <a href="../../../reference/models/layers/#Flux.Chain"><code>Chain</code></a> works left-to-right, the reverse of Base&#39;s <code>∘</code>. Its contents is stored in a tuple, thus <code>model3.layers[1].weight</code> is an array.</li><li>Flux&#39;s layer <a href="../../../reference/models/layers/#Flux.Dense"><code>Dense</code></a> has only minor differences from our <code>struct Layer</code>:<ul><li>Like <code>struct Poly3{T}</code> above, it has type parameters for its fields – the compiler does not know exactly what type <code>layer3s.W</code> will be, which costs speed.</li><li>Its initialisation uses not <code>randn</code> (normal distribution) but <a href="@ref"><code>glorot_uniform</code></a> by default.</li><li>It reshapes some inputs (to allow several batch dimensions), and produces more friendly errors on wrong-size input.</li><li>And it has some performance tricks: making sure element types match, and re-using some memory.</li></ul></li><li>The function <a href="../../../reference/models/activation/#NNlib.sigmoid"><code>σ</code></a> is calculated in a slightly better way, and has a rule telling Zygote how to differentiate it efficiently.</li><li>Flux overloads <code>Base.show</code> so to give pretty printing at the REPL prompt. Calling <a href="../../../reference/models/functors/#Flux.@layer"><code>Flux.@layer Layer</code></a> will add this, and some other niceties.</li></ul><p>All Flux layers accept a batch of samples: Instead of mapping one sample <code>x::Vector</code> to one output <code>y::Vector</code>, they map columns of a matrix <code>xs::Matrix</code> to columns of the output. This looks like <code>f(xs) ≈ stack(f(x) for x in eachcol(xs))</code> but is done more efficiently.</p><p>If what you need isn&#39;t covered by Flux&#39;s built-in layers, it&#39;s easy to write your own. There are more details <a href="../../../tutorials/custom_layers/#man-advanced">later</a>, but the steps are invariably those shown for <code>struct Layer</code> above:</p><ol><li>Define a <code>struct</code> which will hold the parameters.</li><li>Make it callable, to define how it uses them to transform the input <code>x</code></li><li>Define a constructor which initialises the parameters (if the default constructor doesn&#39;t do what you want).</li><li>Annotate with <code>@layer</code> to opt-in to pretty printing, and other enhacements.</li></ol><h3><img src="https://github.com/FluxML/Optimisers.jl/blob/master/docs/src/assets/logo.png?raw=true" width="40px"/>&nbsp;<a href="https://github.com/FluxML/Functors.jl">Functors.jl</a></h3><p>To deal with such nested structures, Flux relies heavily on an associated package called Functors. Its basic function is <a href="../../../reference/models/functors/#Functors.fmap"><code>fmap</code></a>, which generalises <code>map(f, x)</code> to work on almost anything.</p><p>For example, this is how <a href="../../../reference/models/functors/#Flux.gpu-Tuple{Any}">gpu</a> moves all arrays within a model to the GPU, reconstructing another <code>only ∘ Layer(...) ∘ Layer(...)</code> (or a <code>Chain</code> etc.) around the new <code>CuArray</code>s:</p><pre><code class="language-julia hljs">using CUDA, Functors
fmap(cu, model1)</code></pre><p>And this is a very simple gradient update of the parameters, walking over <code>model</code> and <code>grad</code> simultaneously:</p><pre><code class="language-julia hljs">fmap((x, dx) -&gt; x isa Array ? (x - dx/100) : x, model, grad)</code></pre><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Before Flux v0.15 (and Functors v0.5), this exploration of structs was opt-in. After defining <code>struct Layer</code> it was necessary to call <code>@functor Layer</code> (or <code>@layer Layer</code>) before Flux would look inside. This has now changed to be opt-out: Functors (and hence Flux) will explore arbitrary structs, unless told not to (using <code>Functors.@leaf</code>). This is why even &quot;anonymous structs&quot; created by closures, like <code>poly3</code> and <code>layer3</code> above, are now valid Flux models, although the use of named structs is still recommended practice.</p></div></div><h2 id="Curve-Fitting"><a class="docs-heading-anchor" href="#Curve-Fitting">Curve Fitting</a><a id="Curve-Fitting-1"></a><a class="docs-heading-anchor-permalink" href="#Curve-Fitting" title="Permalink"></a></h2><p>Above we took gradients of the output, or sometimes to the first element of the output – it must be a number, not a vector. Adjusting the parameters to make this smaller won&#39;t lead us anywhere interesting. Instead, we should minimise some <em>loss function</em> which compares the actual output to our desired output.</p><p>Perhaps the simplest example is curve fitting. The <a href="../overview/#man-overview">previous page</a> fitted a linear model to data. With out two-layer model, we can fit a nonlinear function. For example, let us use <code>f(x) = 2x - x^3</code> evaluated at some points <code>x in -2:0.1:2</code> as the data, and adjust the parameters of <code>model3</code> from above so that its output is similar.</p><pre><code class="language-julia hljs">data = [([x], 2x-x^3) for x in -2:0.1f0:2]  # training points (x, y)

for _ in 1:1000  # adjust parameters to minimise the error:
  Flux.train!((m,x,y) -&gt; (m(x) - y)^2, model3, data, Descent(0.01))
end</code></pre><p>The same code will also work with <code>model1</code> or <code>model2</code> instead. Here&#39;s how to plot the desired and actual outputs:</p><pre><code class="language-julia hljs">using Plots
plot(x -&gt; 2x-x^3, -2, 2, label=&quot;truth&quot;)
scatter!(x -&gt; model3([x]), -2:0.1f0:2, label=&quot;fitted&quot;)</code></pre><p>More detail about what exactly the function <code>train!</code> is doing, and how to use rules other than simple <a href="../../../reference/training/optimisers/#Optimisers.Descent"><code>Descent</code></a>, is what the next page in this guide is about: <a href="../../training/training/#man-training">training</a>.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../overview/">« Fitting a Line</a><a class="docs-footer-nextpage" href="../../training/training/">Training »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Wednesday 11 December 2024 15:10">Wednesday 11 December 2024</span>. Using Julia version 1.11.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
