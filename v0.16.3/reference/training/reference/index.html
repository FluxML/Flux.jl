<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Training API · Flux</title><meta name="title" content="Training API · Flux"/><meta property="og:title" content="Training API · Flux"/><meta property="twitter:title" content="Training API · Flux"/><meta name="description" content="Documentation for Flux."/><meta property="og:description" content="Documentation for Flux."/><meta property="twitter:description" content="Documentation for Flux."/><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-36890222-9', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../../assets/documenter.js"></script><script src="../../../search_index.js"></script><script src="../../../siteinfo.js"></script><script src="../../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../../assets/themeswap.js"></script><link href="../../../assets/flux.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../../"><img class="docs-light-only" src="../../../assets/logo.png" alt="Flux logo"/><img class="docs-dark-only" src="../../../assets/logo-dark.png" alt="Flux logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../../">Welcome</a></li><li><span class="tocitem">Guide</span><ul><li><a class="tocitem" href="../../../guide/models/quickstart/">Quick Start</a></li><li><a class="tocitem" href="../../../guide/models/overview/">Fitting a Line</a></li><li><a class="tocitem" href="../../../guide/models/basics/">Gradients and Layers</a></li><li><a class="tocitem" href="../../../guide/training/training/">Training</a></li><li><a class="tocitem" href="../../../guide/models/recurrence/">Recurrence</a></li><li><a class="tocitem" href="../../../guide/gpu/">GPU Support</a></li><li><a class="tocitem" href="../../../guide/saving/">Saving &amp; Loading</a></li><li><a class="tocitem" href="../../../guide/performance/">Performance Tips</a></li></ul></li><li><a class="tocitem" href="../../../ecosystem/">Ecosystem</a></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../../models/layers/">Built-in Layers</a></li><li><a class="tocitem" href="../../models/activation/">Activation Functions</a></li><li><a class="tocitem" href="../../utilities/">Weight Initialisation</a></li><li><a class="tocitem" href="../../models/losses/">Loss Functions</a></li><li class="is-active"><a class="tocitem" href>Training API</a><ul class="internal"><li><a class="tocitem" href="#Optimisation-Modifiers"><span>Optimisation Modifiers</span></a></li></ul></li><li><a class="tocitem" href="../optimisers/">Optimisation Rules</a></li><li><a class="tocitem" href="../../outputsize/">Shape Inference</a></li><li><a class="tocitem" href="../../destructure/">Flat vs. Nested</a></li><li><a class="tocitem" href="../callbacks/">Callback Helpers</a></li><li><a class="tocitem" href="../zygote/">Gradients – Zygote.jl</a></li><li><a class="tocitem" href="../enzyme/">Gradients – Enzyme.jl</a></li><li><a class="tocitem" href="../../data/mldatadevices/">Transfer Data to GPU – MLDataDevices.jl</a></li><li><a class="tocitem" href="../../data/mlutils/">Batching Data – MLUtils.jl</a></li><li><a class="tocitem" href="../../data/onehot/">OneHotArrays.jl</a></li><li><a class="tocitem" href="../../models/nnlib/">Low-level Operations – NNlib.jl</a></li><li><a class="tocitem" href="../../models/functors/">Nested Structures – Functors.jl</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../../tutorials/linear_regression/">Linear Regression</a></li><li><a class="tocitem" href="../../../tutorials/logistic_regression/">Logistic Regression</a></li><li><a class="tocitem" href="../../../tutorials/custom_layers/">Custom Layers</a></li><li><a class="tocitem" href="../../../tutorials/model_zoo/">Model Zoo</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Reference</a></li><li class="is-active"><a href>Training API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Training API</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/FluxML/Flux.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/FluxML/Flux.jl/blob/master/docs/src/reference/training/reference.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Training-API-Reference"><a class="docs-heading-anchor" href="#Training-API-Reference">Training API Reference</a><a id="Training-API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Training-API-Reference" title="Permalink"></a></h1><p>The new version of Flux&#39;s training code was written as an independent package, <a href="https://github.com/FluxML/Optimisers.jl">Optimisers.jl</a>. Only the function <code>train!</code> belongs to Flux itself.</p><p>The Optimisers package is designed to allow for immutable objects. But at present all Flux models contain parameter arrays (such as <code>Array</code>s and <code>CuArray</code>s) which can be updated in-place. Because of this:</p><ul><li>The objects returned by <code>Optimisers.update!</code> can be ignored.</li><li>Flux defines its own version of <code>setup</code> which checks this assumption. (Using instead <code>Optimisers.setup</code> will also work, they return the same thing.)</li></ul><p>The available optimization rules are listed the <a href="../optimisers/#man-optimisers">optimisation rules</a> page here. See the <a href="https://fluxml.ai/Optimisers.jl/dev/">Optimisers documentation</a> for details on how the rules work.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Flux.Train.setup" href="#Flux.Train.setup"><code>Flux.Train.setup</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">opt_state = setup(rule, model)</code></pre><p>This is a version of <code>Optimisers.setup</code>, and is the first step before using <a href="../enzyme/#Flux.Train.train!-Tuple{Any, EnzymeCore.Duplicated, Any, Any}"><code>train!</code></a>. It differs from <code>Optimisers.setup</code> in that it:</p><ul><li>has one extra check for mutability (since Flux expects to mutate the model in-place, while Optimisers.jl is designed to return an updated model)</li><li>has methods which accept Flux&#39;s old optimisers, and convert them. (The old <code>Flux.Optimise.Adam</code> and new <code>Optimisers.Adam</code> are distinct types.)</li></ul><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; model = Dense(2 =&gt; 1, leakyrelu; init=ones);

julia&gt; opt_state = Flux.setup(Momentum(0.1), model)  # this encodes the optimiser and its state
(weight = Leaf(Momentum(0.1, 0.9), [0.0 0.0]), bias = Leaf(Momentum(0.1, 0.9), [0.0]), σ = ())

julia&gt; x1, y1 = [0.2, -0.3], [0.4];  # use the same data for two steps:

julia&gt; Flux.train!(model, [(x1, y1), (x1, y1)], opt_state) do m, x, y
         sum(abs.(m(x) .- y)) * 100
       end

julia&gt; model.bias  # was zero, mutated by Flux.train!
1-element Vector{Float64}:
 10.19

julia&gt; opt_state  # mutated by Flux.train!
(weight = Leaf(Momentum(0.1, 0.9), [-2.018 3.027]), bias = Leaf(Momentum(0.1, 0.9), [-10.09]), σ = ())</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/9147e84bb1cde1dd1e789107422bb98bfd8a07b9/src/train.jl#L17-L47">source</a></section><section><div><pre><code class="language-julia hljs">opt_state = setup(rule, model::Duplicated) = setup(rule, model.val)</code></pre><p>Special method for use with Enzyme.jl, ignores the stored gradient.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/9147e84bb1cde1dd1e789107422bb98bfd8a07b9/src/train.jl#L58-L62">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Flux.Train.train!-NTuple{4, Any}" href="#Flux.Train.train!-NTuple{4, Any}"><code>Flux.Train.train!</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">train!(loss, model, data, opt_state)</code></pre><p>Uses a <code>loss</code> function and training <code>data</code> to improve the <code>model</code>&#39;s parameters according to a particular optimisation rule encoded in <code>opt_state</code>. Iterates through <code>data</code> once, evaluating for each <code>d in data</code> either <code>loss(model, d...)</code> if <code>d isa Tuple</code>, or else <code>loss(model, d)</code> for other <code>d</code>.</p><p>If <code>model</code> is an Enzyme.Duplicated and <code>Enzyme.jl</code> is loaded, gradients will be computed with Enzyme, otherwise they will be computed with Zygote.</p><p>For example, with these definitions...</p><pre><code class="nohighlight hljs">data = [(x1, y1), (x2, y2), (x3, y3)]

loss3(m, x, y) = norm(m(x) .- y)        # the model is the first argument

opt_state = Flux.setup(Adam(), model)   # explicit setup of optimiser momenta</code></pre><p>...calling <code>Flux.train!(loss3, model, data, opt_state)</code> runs a loop much like this:</p><pre><code class="nohighlight hljs">for d in data
    ∂L∂m = gradient(loss3, model, d...)[1]
    update!(opt_state, model, ∂L∂m)
end</code></pre><p>You can also write this loop yourself, if you need more flexibility. For this reason <code>train!</code> is not highly extensible. It adds only a few features to the loop above:</p><ul><li><p>Stop with a <code>DomainError</code> if the loss is infinite or <code>NaN</code> at any point.</p></li><li><p>Show a progress bar using <a href="https://github.com/JuliaLogging/ProgressLogging.jl"><code>@withprogress</code></a>.</p></li></ul><div class="admonition is-compat"><header class="admonition-header">New</header><div class="admonition-body"><p>This method was added in Flux 0.13.9. It has significant changes from the one used by Flux ≤ 0.13:</p><ul><li>It now takes the <code>model</code> itself, not the result of <code>Flux.params</code>. (This is to move away from Zygote&#39;s &quot;implicit&quot; parameter handling, with <code>Grads</code>.)</li><li>Instead of <code>loss</code> being a function which accepts only the data, now it must also accept the <code>model</code> itself, as the first argument.</li><li><code>opt_state</code> should be the result of <a href="#Flux.Train.setup"><code>Flux.setup</code></a>. Using an optimiser such as <code>Adam()</code> without this step should give you a warning.</li><li>Callback functions are not supported. (But any code can be included in the above <code>for</code> loop.)</li></ul></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/9147e84bb1cde1dd1e789107422bb98bfd8a07b9/src/train.jl#L65-L110">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.update" href="#Optimisers.update"><code>Optimisers.update</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Optimisers.update(tree, model, gradient) -&gt; (tree, model)</code></pre><p>Uses the optimiser and the gradient to change the trainable parameters in the model. Returns the improved model, and the optimiser states needed for the next update. The initial tree of states comes from <a href="#Optimisers.setup"><code>setup</code></a>.</p><p>See also <a href="#Optimisers.update!"><code>update!</code></a>, which will be faster for models of ordinary <code>Array</code>s or <code>CuArray</code>s.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; m = (x = Float32[1,2,3], y = tanh);

julia&gt; t = Optimisers.setup(Descent(0.1), m)
(x = Leaf(Descent(0.1), nothing), y = ())

julia&gt; g = (x = [1,1,1], y = nothing);  # fake gradient

julia&gt; Optimisers.update(t, m, g)
((x = Leaf(Descent(0.1), nothing), y = ()), (x = Float32[0.9, 1.9, 2.9], y = tanh))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.4.4/src/Optimisers.jl#L120-L141">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.update!" href="#Optimisers.update!"><code>Optimisers.update!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Optimisers.update!(tree, model, gradient) -&gt; (tree, model)</code></pre><p>Uses the optimiser and the gradient to change the trainable parameters in the model. Returns the improved model, and the optimiser states needed for the next update. The initial tree of states comes from <a href="#Optimisers.setup"><code>setup</code></a>.</p><p>This is used in exactly the same manner as <a href="#Optimisers.update"><code>update</code></a>, but because it may mutate arrays within the old model (and the old state), it will be faster for models of ordinary <code>Array</code>s or <code>CuArray</code>s. However, you should not rely on the old model being fully updated but rather use the returned model. (The original state tree is always mutated, as each <code>Leaf</code> is mutable.)</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using StaticArrays, Zygote, Optimisers

julia&gt; m = (x = [1f0, 2f0], y = SA[4f0, 5f0]);  # partly mutable model

julia&gt; t = Optimisers.setup(Momentum(1/30, 0.9), m)  # tree of states
(x = Leaf(Momentum(0.0333333, 0.9), Float32[0.0, 0.0]), y = Leaf(Momentum(0.0333333, 0.9), Float32[0.0, 0.0]))

julia&gt; g = gradient(m -&gt; sum(abs2.(m.x .+ m.y)), m)[1]  # structural gradient
(x = Float32[10.0, 14.0], y = Float32[10.0, 14.0])

julia&gt; t2, m2 = Optimisers.update!(t, m, g);

julia&gt; m2  # after update or update!, this is the new model
(x = Float32[0.6666666, 1.5333333], y = Float32[3.6666667, 4.5333333])

julia&gt; m2.x === m.x  # update! has re-used this array, for efficiency
true

julia&gt; m  # original should be discarded, may be mutated but no guarantee
(x = Float32[0.6666666, 1.5333333], y = Float32[4.0, 5.0])

julia&gt; t == t2  # original state tree is guaranteed to be mutated
true</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.4.4/src/Optimisers.jl#L144-L184">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.setup" href="#Optimisers.setup"><code>Optimisers.setup</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Optimisers.setup(rule, model) -&gt; state_tree</code></pre><p>Initialises the given optimiser for every trainable parameter within the model. Returns a tree of the relevant states, which must be passed to <a href="#Optimisers.update"><code>update</code></a> or <a href="#Optimisers.update!"><code>update!</code></a>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; m = (x = rand(3), y = (true, false), z = tanh);

julia&gt; Optimisers.setup(Momentum(), m)  # same field names as m
(x = Leaf(Momentum(0.01, 0.9), [0.0, 0.0, 0.0]), y = ((), ()), z = ())</code></pre><p>The recursion into structures uses Functors.jl, and any new <code>struct</code>s containing parameters need to be marked with <code>Functors.@functor</code> before use. See <a href="https://fluxml.ai/Flux.jl/stable/models/advanced/">the Flux docs</a> for more about this.</p><pre><code class="language-julia-repl hljs">julia&gt; struct Layer; mat; fun; end

julia&gt; model = (lay = Layer([1 2; 3 4f0], sin), vec = [5, 6f0]);

julia&gt; Optimisers.setup(Momentum(), model)  # new struct is by default ignored
(lay = (), vec = Leaf(Momentum(0.01, 0.9), Float32[0.0, 0.0]))

julia&gt; destructure(model)
(Float32[5.0, 6.0], Restructure(NamedTuple, ..., 2))

julia&gt; using Functors; @functor Layer  # annotate this type as containing parameters

julia&gt; Optimisers.setup(Momentum(), model)
(lay = (mat = Leaf(Momentum(0.01, 0.9), Float32[0.0 0.0; 0.0 0.0]), fun = ()), vec = Leaf(Momentum(0.01, 0.9), Float32[0.0, 0.0]))

julia&gt; destructure(model)
(Float32[1.0, 3.0, 2.0, 4.0, 5.0, 6.0], Restructure(NamedTuple, ..., 6))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.4.4/src/Optimisers.jl#L79-L117">source</a></section></article><p><code>train!</code> uses <a href="https://github.com/JuliaLogging/ProgressLogging.jl"><code>@progress</code></a> which should show a progress bar in VSCode automatically. To see one in a terminal, you will need to install <a href="https://github.com/JuliaLogging/TerminalLoggers.jl">TerminalLoggers.jl</a> and follow its setup instructions.</p><h2 id="Optimisation-Modifiers"><a class="docs-heading-anchor" href="#Optimisation-Modifiers">Optimisation Modifiers</a><a id="Optimisation-Modifiers-1"></a><a class="docs-heading-anchor-permalink" href="#Optimisation-Modifiers" title="Permalink"></a></h2><p>The state returned by <code>setup</code> can be modified to temporarily prevent training of some parts of the model, or to change the learning rate or other hyperparameter. The functions for doing so may be accessed as <code>Flux.freeze!</code>, <code>Flux.thaw!</code>, and <code>Flux.adjust!</code>. All mutate the state (or part of it) and return <code>nothing</code>.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.adjust!" href="#Optimisers.adjust!"><code>Optimisers.adjust!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Optimisers.adjust!(tree, η)</code></pre><p>Alters the state <code>tree = setup(rule, model)</code> to change the parameters of the optimisation rule, without destroying its stored state. Typically used mid-way through training.</p><p>Can be applied to part of a model, by acting only on the corresponding part of the state <code>tree</code>.</p><p>To change just the learning rate, provide a number <code>η::Real</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; m = (vec = rand(Float32, 2), fun = sin);

julia&gt; st = Optimisers.setup(Nesterov(), m)  # stored momentum is initialised to zero
(vec = Leaf(Nesterov(0.001, 0.9), Float32[0.0, 0.0]), fun = ())

julia&gt; st, m = Optimisers.update(st, m, (vec = [16, 88], fun = nothing));  # with fake gradient

julia&gt; st
(vec = Leaf(Nesterov(0.001, 0.9), Float32[-0.016, -0.088]), fun = ())

julia&gt; Optimisers.adjust!(st, 0.123)  # change learning rate, stored momentum untouched

julia&gt; st
(vec = Leaf(Nesterov(0.123, 0.9), Float32[-0.016, -0.088]), fun = ())</code></pre><p>To change other parameters, <code>adjust!</code> also accepts keyword arguments matching the field names of the optimisation rule&#39;s type.</p><pre><code class="language-julia-repl hljs">julia&gt; fieldnames(Adam)
(:eta, :beta, :epsilon)

julia&gt; st2 = Optimisers.setup(OptimiserChain(ClipGrad(), Adam()), m)
(vec = Leaf(OptimiserChain(ClipGrad(10.0), Adam(0.001, (0.9, 0.999), 1.0e-8)), (nothing, (Float32[0.0, 0.0], Float32[0.0, 0.0], (0.9, 0.999)))), fun = ())

julia&gt; Optimisers.adjust(st2; beta = (0.777, 0.909), delta = 11.1)  # delta acts on ClipGrad
(vec = Leaf(OptimiserChain(ClipGrad(11.1), Adam(0.001, (0.777, 0.909), 1.0e-8)), (nothing, (Float32[0.0, 0.0], Float32[0.0, 0.0], (0.9, 0.999)))), fun = ())

julia&gt; Optimisers.adjust(st; beta = &quot;no such field&quot;)  # silently ignored!
(vec = Leaf(Nesterov(0.123, 0.9), Float32[-0.016, -0.088]), fun = ())</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.4.4/src/adjust.jl#L58-L104">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.freeze!" href="#Optimisers.freeze!"><code>Optimisers.freeze!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Optimisers.freeze!(tree)</code></pre><p>Temporarily alters the state <code>tree = setup(rule, model)</code> so that parameters will not be updated. Un-done by <a href="#Optimisers.thaw!"><code>thaw!</code></a>.</p><p>Can be applied to the state corresponding to only part of a model, for instance with <code>model::Chain</code>, to freeze <code>model.layers[1]</code> you should call <code>freeze!(tree.layers[1])</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; m = (x = ([1.0], 2.0), y = [3.0]);

julia&gt; s = Optimisers.setup(Momentum(), m);

julia&gt; Optimisers.freeze!(s.x)

julia&gt; Optimisers.update!(s, m, (x = ([pi], 10pi), y = [100pi]));  # with fake gradient

julia&gt; m
(x = ([1.0], 2.0), y = [-0.14159265358979312])

julia&gt; s
(x = (Leaf(Momentum(0.01, 0.9), [0.0], frozen = true), ()), y = Leaf(Momentum(0.01, 0.9), [3.14159]))

julia&gt; Optimisers.thaw!(s)

julia&gt; s.x
(Leaf(Momentum(0.01, 0.9), [0.0]), ())</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.4.4/src/adjust.jl#L5-L36">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="Optimisers.thaw!" href="#Optimisers.thaw!"><code>Optimisers.thaw!</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Optimisers.thaw!(tree)</code></pre><p>The reverse of <a href="#Optimisers.freeze!"><code>freeze!</code></a>. Applies to all parameters, mutating every <code>Leaf(rule, state, frozen = true)</code> to <code>Leaf(rule, state, frozen = false)</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Optimisers.jl/blob/v0.4.4/src/adjust.jl#L40-L45">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../models/losses/">« Loss Functions</a><a class="docs-footer-nextpage" href="../optimisers/">Optimisation Rules »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Thursday 6 February 2025 21:19">Thursday 6 February 2025</span>. Using Julia version 1.11.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body><div data-docstringscollapsed="true"></div></html>
