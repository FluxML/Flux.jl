<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Optimisation Rules · Flux</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-36890222-9', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/flux.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="Flux logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="Flux logo"/></a><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Welcome</a></li><li><span class="tocitem">Guide</span><ul><li><a class="tocitem" href="../../models/quickstart/">Quick Start</a></li><li><a class="tocitem" href="../../models/overview/">Fitting a Line</a></li><li><a class="tocitem" href="../../models/basics/">Gradients and Layers</a></li><li><a class="tocitem" href="../training/">Training</a></li><li><a class="tocitem" href="../../models/recurrence/">Recurrence</a></li><li><a class="tocitem" href="../../gpu/">GPU Support</a></li><li><a class="tocitem" href="../../saving/">Saving &amp; Loading</a></li><li><a class="tocitem" href="../../performance/">Performance Tips</a></li></ul></li><li><a class="tocitem" href="../../ecosystem/">Ecosystem</a></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../../models/layers/">Built-in Layers</a></li><li><a class="tocitem" href="../../models/activation/">Activation Functions</a></li><li><a class="tocitem" href="../../utilities/">Weight Initialisation</a></li><li><a class="tocitem" href="../../models/losses/">Loss Functions</a></li><li><a class="tocitem" href="../reference/">Training API</a></li><li class="is-active"><a class="tocitem" href>Optimisation Rules</a><ul class="internal"><li><a class="tocitem" href="#Optimiser-Reference"><span>Optimiser Reference</span></a></li><li><a class="tocitem" href="#Composing-Optimisers"><span>Composing Optimisers</span></a></li><li><a class="tocitem" href="#Scheduling-Optimisers"><span>Scheduling Optimisers</span></a></li><li><a class="tocitem" href="#Decays"><span>Decays</span></a></li><li><a class="tocitem" href="#Gradient-Clipping"><span>Gradient Clipping</span></a></li></ul></li><li><a class="tocitem" href="../../outputsize/">Shape Inference</a></li><li><a class="tocitem" href="../../destructure/">Flat vs. Nested</a></li><li><a class="tocitem" href="../callbacks/">Callback Helpers</a></li><li><a class="tocitem" href="../zygote/">Gradients – Zygote.jl</a></li><li><a class="tocitem" href="../../data/mlutils/">Batching Data – MLUtils.jl</a></li><li><a class="tocitem" href="../../data/onehot/">OneHotArrays.jl</a></li><li><a class="tocitem" href="../../models/nnlib/">Low-level Operations – NNlib.jl</a></li><li><a class="tocitem" href="../../models/functors/">Nested Structures – Functors.jl</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/linear_regression/">Linear Regression</a></li><li><a class="tocitem" href="../../tutorials/logistic_regression/">Logistic Regression</a></li><li><a class="tocitem" href="../../models/advanced/">Custom Layers</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Reference</a></li><li class="is-active"><a href>Optimisation Rules</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Optimisation Rules</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/FluxML/Flux.jl/blob/master/docs/src/training/optimisers.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="man-optimisers"><a class="docs-heading-anchor" href="#man-optimisers">Optimisation Rules</a><a id="man-optimisers-1"></a><a class="docs-heading-anchor-permalink" href="#man-optimisers" title="Permalink"></a></h1><p>Flux builds in many optimisation rules for use with <a href="../reference/#Flux.Optimise.train!-NTuple{4, Any}"><code>train!</code></a> and other training functions.</p><p>The mechanism by which these work is gradually being replaced as part of the change from &quot;implicit&quot; dictionary-based to &quot;explicit&quot; tree-like structures. At present, the same struct (such as <code>Adam</code>) can be used with either form, and will be automatically translated.</p><p>For full details of how the new interface works, see the <a href="https://fluxml.ai/Optimisers.jl/dev/">Optimisers.jl documentation</a>.</p><p>For full details on how the old &quot;implicit&quot; interface worked, see the <a href="https://fluxml.ai/Flux.jl/v0.13.6/training/optimisers/#Optimiser-Interface">Flux 0.13.6 manual</a>.</p><h2 id="Optimiser-Reference"><a class="docs-heading-anchor" href="#Optimiser-Reference">Optimiser Reference</a><a id="Optimiser-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Optimiser-Reference" title="Permalink"></a></h2><p>All optimisers return an object that, when passed to <code>train!</code>, will update the parameters passed to it.</p><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.Descent" href="#Flux.Optimise.Descent"><code>Flux.Optimise.Descent</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Descent(η = 0.1)</code></pre><p>Classic gradient descent optimiser with learning rate <code>η</code>. For each parameter <code>p</code> and its gradient <code>δp</code>, this runs <code>p -= η*δp</code></p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">opt = Descent()

opt = Descent(0.3)

ps = Flux.params(model)

gs = gradient(ps) do
    loss(x, y)
end

Flux.Optimise.update!(opt, ps, gs)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/91e25e1e4c0bbee19dccce656285fc34c51f49f9/src/optimise/optimisers.jl#L10-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.Momentum" href="#Flux.Optimise.Momentum"><code>Flux.Optimise.Momentum</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Momentum(η = 0.01, ρ = 0.9)</code></pre><p>Gradient descent optimiser with learning rate <code>η</code> and momentum <code>ρ</code>.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Momentum (<code>ρ</code>): Controls the acceleration of gradient descent in the                 prominent direction, in effect damping oscillations.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">opt = Momentum()

opt = Momentum(0.01, 0.99)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/91e25e1e4c0bbee19dccce656285fc34c51f49f9/src/optimise/optimisers.jl#L45-L62">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.Nesterov" href="#Flux.Optimise.Nesterov"><code>Flux.Optimise.Nesterov</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Nesterov(η = 0.001, ρ = 0.9)</code></pre><p>Gradient descent optimiser with learning rate <code>η</code> and Nesterov momentum <code>ρ</code>.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Nesterov momentum (<code>ρ</code>): Controls the acceleration of gradient descent in the                          prominent direction, in effect damping oscillations.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">opt = Nesterov()

opt = Nesterov(0.003, 0.95)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/91e25e1e4c0bbee19dccce656285fc34c51f49f9/src/optimise/optimisers.jl#L78-L95">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.RMSProp" href="#Flux.Optimise.RMSProp"><code>Flux.Optimise.RMSProp</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RMSProp(η = 0.001, ρ = 0.9, ϵ = 1.0e-8)</code></pre><p>Optimizer using the <a href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">RMSProp</a> algorithm. Often a good choice for recurrent networks. Parameters other than learning rate generally don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Momentum (<code>ρ</code>): Controls the acceleration of gradient descent in the                 prominent direction, in effect damping oscillations.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">opt = RMSProp()

opt = RMSProp(0.002, 0.95)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/91e25e1e4c0bbee19dccce656285fc34c51f49f9/src/optimise/optimisers.jl#L112-L132">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.Adam" href="#Flux.Optimise.Adam"><code>Flux.Optimise.Adam</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Adam(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = 1.0e-8)</code></pre><p><a href="https://arxiv.org/abs/1412.6980">Adam</a> optimiser.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">opt = Adam()

opt = Adam(0.001, (0.9, 0.8))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/91e25e1e4c0bbee19dccce656285fc34c51f49f9/src/optimise/optimisers.jl#L149-L166">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.RAdam" href="#Flux.Optimise.RAdam"><code>Flux.Optimise.RAdam</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RAdam(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = 1.0e-8)</code></pre><p><a href="https://arxiv.org/abs/1908.03265">Rectified Adam</a> optimiser.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">opt = RAdam()

opt = RAdam(0.001, (0.9, 0.8))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/91e25e1e4c0bbee19dccce656285fc34c51f49f9/src/optimise/optimisers.jl#L191-L208">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.AdaMax" href="#Flux.Optimise.AdaMax"><code>Flux.Optimise.AdaMax</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaMax(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = 1.0e-8)</code></pre><p><a href="https://arxiv.org/abs/1412.6980">AdaMax</a> is a variant of Adam based on the ∞-norm.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">opt = AdaMax()

opt = AdaMax(0.001, (0.9, 0.995))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/91e25e1e4c0bbee19dccce656285fc34c51f49f9/src/optimise/optimisers.jl#L241-L258">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.AdaGrad" href="#Flux.Optimise.AdaGrad"><code>Flux.Optimise.AdaGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaGrad(η = 0.1, ϵ = 1.0e-8)</code></pre><p><a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">AdaGrad</a> optimiser. It has parameter specific learning rates based on how frequently it is updated. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">opt = AdaGrad()

opt = AdaGrad(0.001)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/91e25e1e4c0bbee19dccce656285fc34c51f49f9/src/optimise/optimisers.jl#L328-L345">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.AdaDelta" href="#Flux.Optimise.AdaDelta"><code>Flux.Optimise.AdaDelta</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaDelta(ρ = 0.9, ϵ = 1.0e-8)</code></pre><p><a href="https://arxiv.org/abs/1212.5701">AdaDelta</a> is a version of AdaGrad adapting its learning rate based on a window of past gradient updates. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Rho (<code>ρ</code>): Factor by which the gradient is decayed at each time step.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">opt = AdaDelta()

opt = AdaDelta(0.89)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/91e25e1e4c0bbee19dccce656285fc34c51f49f9/src/optimise/optimisers.jl#L361-L377">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.AMSGrad" href="#Flux.Optimise.AMSGrad"><code>Flux.Optimise.AMSGrad</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AMSGrad(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = 1.0e-8)</code></pre><p>The <a href="https://openreview.net/forum?id=ryQu7f-RZ">AMSGrad</a> version of the Adam optimiser. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">opt = AMSGrad()

opt = AMSGrad(0.001, (0.89, 0.995))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/91e25e1e4c0bbee19dccce656285fc34c51f49f9/src/optimise/optimisers.jl#L397-L415">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.NAdam" href="#Flux.Optimise.NAdam"><code>Flux.Optimise.NAdam</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">NAdam(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = 1.0e-8)</code></pre><p><a href="https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ">NAdam</a> is a Nesterov variant of Adam. Parameters don&#39;t need tuning.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">opt = NAdam()

opt = NAdam(0.002, (0.89, 0.995))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/91e25e1e4c0bbee19dccce656285fc34c51f49f9/src/optimise/optimisers.jl#L438-L456">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.AdamW" href="#Flux.Optimise.AdamW"><code>Flux.Optimise.AdamW</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">AdamW(η = 0.001, β::Tuple = (0.9, 0.999), decay = 0)</code></pre><p><a href="https://arxiv.org/abs/1711.05101">AdamW</a> is a variant of Adam fixing (as in repairing) its weight decay regularization.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li><li><code>decay</code>: Decay applied to weights during optimisation.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">opt = AdamW()

opt = AdamW(0.001, (0.89, 0.995), 0.1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/91e25e1e4c0bbee19dccce656285fc34c51f49f9/src/optimise/optimisers.jl#L482-L501">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.OAdam" href="#Flux.Optimise.OAdam"><code>Flux.Optimise.OAdam</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">OAdam(η = 0.0001, β::Tuple = (0.5, 0.9), ϵ = 1.0e-8)</code></pre><p><a href="https://arxiv.org/abs/1711.00141">OAdam</a> (Optimistic Adam) is a variant of Adam adding an &quot;optimistic&quot; term suitable for adversarial training.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">opt = OAdam()

opt = OAdam(0.001, (0.9, 0.995))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/91e25e1e4c0bbee19dccce656285fc34c51f49f9/src/optimise/optimisers.jl#L283-L301">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.AdaBelief" href="#Flux.Optimise.AdaBelief"><code>Flux.Optimise.AdaBelief</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">AdaBelief(η = 0.001, β::Tuple = (0.9, 0.999), ϵ = 1.0e-8)</code></pre><p>The <a href="https://arxiv.org/abs/2010.07468">AdaBelief</a> optimiser is a variant of the well-known Adam optimiser.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li>Decay of momentums (<code>β::Tuple</code>): Exponential decay for the first (β1) and the                                  second (β2) momentum estimate.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">opt = AdaBelief()

opt = AdaBelief(0.001, (0.9, 0.8))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/91e25e1e4c0bbee19dccce656285fc34c51f49f9/src/optimise/optimisers.jl#L505-L523">source</a></section></article><h2 id="Composing-Optimisers"><a class="docs-heading-anchor" href="#Composing-Optimisers">Composing Optimisers</a><a id="Composing-Optimisers-1"></a><a class="docs-heading-anchor-permalink" href="#Composing-Optimisers" title="Permalink"></a></h2><p>Flux defines a special kind of optimiser simply called <code>Optimiser</code> which takes in arbitrary optimisers as input. Its behaviour is similar to the usual optimisers, but differs in that it acts by calling the optimisers listed in it sequentially. Each optimiser produces a modified gradient that will be fed into the next, and the resultant update will be applied to the parameter as usual. A classic use case is where adding decays is desirable. Flux defines some basic decays including <code>ExpDecay</code>, <code>InvDecay</code> etc.</p><pre><code class="language-julia hljs">opt = Optimiser(ExpDecay(1, 0.1, 1000, 1e-4), Descent())</code></pre><p>Here we apply exponential decay to the <code>Descent</code> optimiser. The defaults of <code>ExpDecay</code> say that its learning rate will be decayed every 1000 steps. It is then applied like any optimiser.</p><pre><code class="language-julia hljs">w = randn(10, 10)
w1 = randn(10,10)
ps = Params([w, w1])

loss(x) = Flux.Losses.mse(w * x, w1 * x)

loss(rand(10)) # around 9

for t = 1:10^5
  θ = Params([w, w1])
  θ̄ = gradient(() -&gt; loss(rand(10)), θ)
  Flux.Optimise.update!(opt, θ, θ̄)
end

loss(rand(10)) # around 0.9</code></pre><p>It is possible to compose optimisers for some added flexibility.</p><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.Optimiser" href="#Flux.Optimise.Optimiser"><code>Flux.Optimise.Optimiser</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Optimiser(a, b, c...)</code></pre><p>Combine several optimisers into one; each optimiser produces a modified gradient that will be fed into the next, and this is finally applied to the parameter as usual.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>This will be replaced by <code>Optimisers.OptimiserChain</code> in Flux 0.15.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/91e25e1e4c0bbee19dccce656285fc34c51f49f9/src/optimise/optimisers.jl#L561-L570">source</a></section></article><h2 id="Scheduling-Optimisers"><a class="docs-heading-anchor" href="#Scheduling-Optimisers">Scheduling Optimisers</a><a id="Scheduling-Optimisers-1"></a><a class="docs-heading-anchor-permalink" href="#Scheduling-Optimisers" title="Permalink"></a></h2><p>In practice, it is fairly common to schedule the learning rate of an optimiser to obtain faster convergence. There are a variety of popular scheduling policies, and you can find implementations of them in <a href="http://fluxml.ai/ParameterSchedulers.jl/dev/README.html">ParameterSchedulers.jl</a>. The documentation for ParameterSchedulers.jl provides a more detailed overview of the different scheduling policies, and how to use them with Flux optimisers. Below, we provide a brief snippet illustrating a <a href="https://arxiv.org/pdf/1608.03983.pdf">cosine annealing</a> schedule with a momentum optimiser.</p><p>First, we import ParameterSchedulers.jl and initialize a cosine annealing schedule to vary the learning rate between <code>1e-4</code> and <code>1e-2</code> every 10 steps. We also create a new <a href="#Flux.Optimise.Momentum"><code>Momentum</code></a> optimiser.</p><pre><code class="language-julia hljs">using ParameterSchedulers

opt = Momentum()
schedule = Cos(λ0 = 1e-4, λ1 = 1e-2, period = 10)
for (eta, epoch) in zip(schedule, 1:100)
  opt.eta = eta
  # your training code here
end</code></pre><p><code>schedule</code> can also be indexed (e.g. <code>schedule(100)</code>) or iterated like any iterator in Julia.</p><p>ParameterSchedulers.jl schedules are stateless (they don&#39;t store their iteration state). If you want a <em>stateful</em> schedule, you can use <code>ParameterSchedulers.Stateful</code>:</p><pre><code class="language-julia hljs">using ParameterSchedulers: Stateful, next!

schedule = Stateful(Cos(λ0 = 1e-4, λ1 = 1e-2, period = 10))
for epoch in 1:100
  opt.eta = next!(schedule)
  # your training code here
end</code></pre><p>ParameterSchedulers.jl allows for many more scheduling policies including arbitrary functions, looping any function with a given period, or sequences of many schedules. See the ParameterSchedulers.jl documentation for more info.</p><h2 id="Decays"><a class="docs-heading-anchor" href="#Decays">Decays</a><a id="Decays-1"></a><a class="docs-heading-anchor-permalink" href="#Decays" title="Permalink"></a></h2><p>Similar to optimisers, Flux also defines some simple decays that can be used in conjunction with other optimisers, or standalone.</p><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.ExpDecay" href="#Flux.Optimise.ExpDecay"><code>Flux.Optimise.ExpDecay</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ExpDecay(η = 0.001, decay = 0.1, decay_step = 1000, clip = 1e-4, start = 1)</code></pre><p>Discount the learning rate <code>η</code> by the factor <code>decay</code> every <code>decay_step</code> steps till a minimum of <code>clip</code>.</p><p><strong>Parameters</strong></p><ul><li>Learning rate (<code>η</code>): Amount by which gradients are discounted before updating                      the weights.</li><li><code>decay</code>: Factor by which the learning rate is discounted.</li><li><code>decay_step</code>: Schedule decay operations by setting the number of steps between               two decay operations.</li><li><code>clip</code>: Minimum value of learning rate.</li><li>&#39;start&#39;: Step at which the decay starts.</li></ul><p>See also the <a href="#Scheduling-Optimisers">Scheduling Optimisers</a> section of the docs for more general scheduling techniques.</p><p><strong>Examples</strong></p><p><code>ExpDecay</code> is typically composed  with other optimisers  as the last transformation of the gradient:</p><pre><code class="language-julia hljs">opt = Optimiser(Adam(), ExpDecay(1.0))</code></pre><p>Note: you may want to start with <code>η=1</code> in <code>ExpDecay</code> when combined with other optimisers (<code>Adam</code> in this case) that have their own learning rate.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/91e25e1e4c0bbee19dccce656285fc34c51f49f9/src/optimise/optimisers.jl#L625-L653">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.InvDecay" href="#Flux.Optimise.InvDecay"><code>Flux.Optimise.InvDecay</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">InvDecay(γ = 0.001)</code></pre><p>Apply inverse time decay to an optimiser, so that the effective step size at iteration <code>n</code> is <code>eta / (1 + γ * n)</code> where <code>eta</code> is the initial step size. The wrapped optimiser&#39;s step size is not modified.</p><p>See also the <a href="#Scheduling-Optimisers">Scheduling Optimisers</a> section of the docs for more general scheduling techniques.</p><p><strong>Examples</strong></p><p><code>InvDecay</code> is typically composed  with other optimisers  as the last transformation of the gradient:</p><pre><code class="language-julia hljs"># Inverse decay of the learning rate
# with starting value 0.001 and decay coefficient 0.01.
opt = Optimiser(Adam(1f-3), InvDecay(1f-2))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/91e25e1e4c0bbee19dccce656285fc34c51f49f9/src/optimise/optimisers.jl#L589-L609">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.WeightDecay" href="#Flux.Optimise.WeightDecay"><code>Flux.Optimise.WeightDecay</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">WeightDecay(λ = 0)</code></pre><p>Decay weights by <span>$λ$</span>.  Typically composed  with other optimisers as the first transformation to the gradient, making it equivalent to adding <span>$L_2$</span> regularization  with coefficient  <span>$λ$</span> to the loss.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">opt = Optimiser(WeightDecay(1f-4), Adam())</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/91e25e1e4c0bbee19dccce656285fc34c51f49f9/src/optimise/optimisers.jl#L676-L689">source</a></section></article><h2 id="Gradient-Clipping"><a class="docs-heading-anchor" href="#Gradient-Clipping">Gradient Clipping</a><a id="Gradient-Clipping-1"></a><a class="docs-heading-anchor-permalink" href="#Gradient-Clipping" title="Permalink"></a></h2><p>Gradient clipping is useful for training recurrent neural networks, which have a tendency to suffer from the exploding gradient problem. An example usage is</p><pre><code class="language-julia hljs">opt = Optimiser(ClipValue(1e-3), Adam(1e-3))</code></pre><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.ClipValue" href="#Flux.Optimise.ClipValue"><code>Flux.Optimise.ClipValue</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ClipValue(thresh)</code></pre><p>Clip gradients when their absolute value exceeds <code>thresh</code>.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>This will be replaced by <code>Optimisers.ClipGrad</code> in Flux 0.15.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/91e25e1e4c0bbee19dccce656285fc34c51f49f9/src/optimise/optimisers.jl#L701-L708">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.ClipNorm" href="#Flux.Optimise.ClipNorm"><code>Flux.Optimise.ClipNorm</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ClipNorm(thresh)</code></pre><p>Clip gradients when their L2 norm exceeds <code>thresh</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/91e25e1e4c0bbee19dccce656285fc34c51f49f9/src/optimise/optimisers.jl#L715-L719">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../reference/">« Training API</a><a class="docs-footer-nextpage" href="../../outputsize/">Shape Inference »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Monday 7 August 2023 15:23">Monday 7 August 2023</span>. Using Julia version 1.9.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
