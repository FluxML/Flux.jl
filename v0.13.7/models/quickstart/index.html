<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Quick Start Â· Flux</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-36890222-9', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/flux.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="Flux logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="Flux logo"/></a><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><span class="tocitem">Getting Started</span><ul><li><a class="tocitem" href="../../">Welcome</a></li><li class="is-active"><a class="tocitem" href>Quick Start</a><ul class="internal"><li><a class="tocitem" href="#Features-of-Note"><span>Features of Note</span></a></li></ul></li><li><a class="tocitem" href="../overview/">Fitting a Line</a></li><li><a class="tocitem" href="../basics/">Gradients and Layers</a></li></ul></li><li><span class="tocitem">Building Models</span><ul><li><a class="tocitem" href="../layers/">Built-in Layers ðŸ“š</a></li><li><a class="tocitem" href="../recurrence/">Recurrence</a></li><li><a class="tocitem" href="../activation/">Activation Functions ðŸ“š</a></li><li><a class="tocitem" href="../nnlib/">NNlib.jl ðŸ“š (<code>softmax</code>, <code>conv</code>, ...)</a></li></ul></li><li><span class="tocitem">Handling Data</span><ul><li><a class="tocitem" href="../../data/mlutils/">MLUtils.jl ðŸ“š (<code>DataLoader</code>, ...)</a></li><li><a class="tocitem" href="../../data/onehot/">OneHotArrays.jl ðŸ“š (<code>onehot</code>, ...)</a></li></ul></li><li><span class="tocitem">Training Models</span><ul><li><a class="tocitem" href="../../training/training/">Training</a></li><li><a class="tocitem" href="../regularisation/">Regularisation</a></li><li><a class="tocitem" href="../losses/">Loss Functions ðŸ“š</a></li><li><a class="tocitem" href="../../training/optimisers/">Optimisation Rules ðŸ“š</a></li><li><a class="tocitem" href="../../training/callbacks/">Callback Helpers ðŸ“š</a></li><li><a class="tocitem" href="../../training/zygote/">Zygote.jl ðŸ“š (<code>gradient</code>, ...)</a></li></ul></li><li><span class="tocitem">Model Tools</span><ul><li><a class="tocitem" href="../../gpu/">GPU Support</a></li><li><a class="tocitem" href="../../saving/">Saving &amp; Loading</a></li><li><a class="tocitem" href="../../outputsize/">Shape Inference ðŸ“š</a></li><li><a class="tocitem" href="../../utilities/">Weight Initialisation ðŸ“š</a></li><li><a class="tocitem" href="../../destructure/">Flat vs. Nested ðŸ“š</a></li><li><a class="tocitem" href="../functors/">Functors.jl ðŸ“š (<code>fmap</code>, ...)</a></li></ul></li><li><a class="tocitem" href="../../performance/">Performance Tips</a></li><li><a class="tocitem" href="../../ecosystem/">Flux&#39;s Ecosystem</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../advanced/">Custom Layers</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Getting Started</a></li><li class="is-active"><a href>Quick Start</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Quick Start</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/FluxML/Flux.jl/blob/master/docs/src/models/quickstart.md" title="Edit on GitHub"><span class="docs-icon fab">ï‚›</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="man-quickstart"><a class="docs-heading-anchor" href="#man-quickstart">A Neural Network in One Minute</a><a id="man-quickstart-1"></a><a class="docs-heading-anchor-permalink" href="#man-quickstart" title="Permalink"></a></h1><p>If you have used neural networks before, then this simple example might be helpful for seeing how the major parts of Flux work together. Try pasting the code into the REPL prompt.</p><p>If you haven&#39;t, then you might prefer the <a href="../overview/">Fitting a Straight Line</a> page.</p><pre><code class="language-julia hljs"># With Julia 1.7+, this will prompt if neccessary to install everything, including CUDA:
using Flux, Statistics

# Generate some data for the XOR problem: vectors of length 2, as columns of a matrix:
noisy = rand(Float32, 2, 1000)                                    # 2Ã—1000 Matrix{Float32}
truth = map(col -&gt; xor(col...), eachcol(noisy .&gt; 0.5))            # 1000-element Vector{Bool}

# Define our model, a multi-layer perceptron with one hidden layer of size 3:
model = Chain(Dense(2 =&gt; 3, tanh), BatchNorm(3), Dense(3 =&gt; 2), softmax)

# The model encapsulates parameters, randomly initialised. Its initial output is:
out1 = model(noisy)                                               # 2Ã—1000 Matrix{Float32}

# To train the model, we use batches of 64 samples:
mat = Flux.onehotbatch(truth, [true, false])                      # 2Ã—1000 OneHotMatrix
data = Flux.DataLoader((noisy, mat), batchsize=64, shuffle=true);
first(data) .|&gt; summary                                           # (&quot;2Ã—64 Matrix{Float32}&quot;, &quot;2Ã—64 Matrix{Bool}&quot;)

pars = Flux.params(model)  # contains references to arrays in model
opt = Flux.Adam(0.01)      # will store optimiser momentum, etc.

# Training loop, using the whole data set 1000 times:
for epoch in 1:1_000
    Flux.train!(pars, data, opt) do x, y
        # First argument of train! is a loss function, here defined by a `do` block.
        # This gets x and y, each a 2Ã—64 Matrix, from data, and compares:
        Flux.crossentropy(model(x), y)
    end
end

pars  # has changed!
opt
out2 = model(noisy)

mean((out2[1,:] .&gt; 0.5) .== truth)  # accuracy 94% so far!</code></pre><p><img src="../../assets/oneminute.png" alt/></p><pre><code class="nohighlight hljs">using Plots  # to draw the above figure

p_true = scatter(noisy[1,:], noisy[2,:], zcolor=truth, title=&quot;True classification&quot;, legend=false)
p_raw =  scatter(noisy[1,:], noisy[2,:], zcolor=out1[1,:], title=&quot;Untrained network&quot;, label=&quot;&quot;, clims=(0,1))
p_done = scatter(noisy[1,:], noisy[2,:], zcolor=out2[1,:], title=&quot;Trained network&quot;, legend=false)

plot(p_true, p_raw, p_done, layout=(1,3), size=(1000,330))</code></pre><p>This XOR (&quot;exclusive or&quot;) problem is a variant of the famous one which drove Minsky and Papert to invent deep neural networks in 1969. For small values of &quot;deep&quot; â€“ this has one hidden layer, while earlier perceptrons had none. (What they call a hidden layer, Flux calls the output of the first layer, <code>model[1](noisy)</code>.)</p><p>Since then things have developed a little. </p><h2 id="Features-of-Note"><a class="docs-heading-anchor" href="#Features-of-Note">Features of Note</a><a id="Features-of-Note-1"></a><a class="docs-heading-anchor-permalink" href="#Features-of-Note" title="Permalink"></a></h2><p>Some things to notice in this example are:</p><ul><li><p>The batch dimension of data is always the last one. Thus a <code>2Ã—1000 Matrix</code> is a thousand observations, each a column of length 2.</p></li><li><p>The <code>model</code> can be called like a function, <code>y = model(x)</code>. It encapsulates the parameters (and state).</p></li><li><p>But the model does not contain the loss function, nor the optimisation rule. Instead the <a href="../../training/optimisers/#Flux.Optimise.Adam"><code>Adam()</code></a> object stores between iterations the momenta it needs.</p></li><li><p>The function <a href="../../training/training/#Flux.Optimise.train!"><code>train!</code></a> likes data as an iterator generating <code>Tuple</code>s, here produced by <a href="../../data/mlutils/#MLUtils.DataLoader"><code>DataLoader</code></a>. This mutates both the <code>model</code> and the optimiser state inside <code>opt</code>.</p></li></ul><p>There are other ways to train Flux models, for more control than <code>train!</code> provides:</p><ul><li><p>Within Flux, you can easily write a training loop, calling <a href="../../training/zygote/#Zygote.gradient"><code>gradient</code></a> and <a href="../../training/optimisers/#Flux.Optimise.update!"><code>update!</code></a>.</p></li><li><p>For a lower-level way, see the package <a href="https://github.com/FluxML/Optimisers.jl">Optimisers.jl</a>.</p></li><li><p>For higher-level ways, see <a href="https://github.com/FluxML/FluxTraining.jl">FluxTraining.jl</a> and <a href="https://github.com/FluxML/FastAI.jl">FastAI.jl</a>.</p></li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">Â« Welcome</a><a class="docs-footer-nextpage" href="../overview/">Fitting a Line Â»</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Saturday 29 October 2022 08:22">Saturday 29 October 2022</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
