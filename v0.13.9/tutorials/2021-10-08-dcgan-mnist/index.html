<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Deep Convolutional GAN Â· Flux</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-36890222-9', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/flux.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="Flux logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="Flux logo"/></a><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><span class="tocitem">Getting Started</span><ul><li><a class="tocitem" href="../../">Welcome</a></li><li><a class="tocitem" href="../../models/quickstart/">Quick Start</a></li><li><a class="tocitem" href="../../models/overview/">Fitting a Line</a></li><li><a class="tocitem" href="../../models/basics/">Gradients and Layers</a></li></ul></li><li><span class="tocitem">Building Models</span><ul><li><a class="tocitem" href="../../models/layers/">Built-in Layers ðŸ“š</a></li><li><a class="tocitem" href="../../models/recurrence/">Recurrence</a></li><li><a class="tocitem" href="../../models/activation/">Activation Functions ðŸ“š</a></li><li><a class="tocitem" href="../../models/nnlib/">NNlib.jl ðŸ“š (<code>softmax</code>, <code>conv</code>, ...)</a></li></ul></li><li><span class="tocitem">Handling Data</span><ul><li><a class="tocitem" href="../../data/mlutils/">MLUtils.jl ðŸ“š (<code>DataLoader</code>, ...)</a></li><li><a class="tocitem" href="../../data/onehot/">OneHotArrays.jl ðŸ“š (<code>onehot</code>, ...)</a></li></ul></li><li><span class="tocitem">Training Models</span><ul><li><a class="tocitem" href="../../training/training/">Training</a></li><li><a class="tocitem" href="../../models/regularisation/">Regularisation</a></li><li><a class="tocitem" href="../../models/losses/">Loss Functions ðŸ“š</a></li><li><a class="tocitem" href="../../training/optimisers/">Optimisation Rules ðŸ“š</a></li><li><a class="tocitem" href="../../training/callbacks/">Callback Helpers ðŸ“š</a></li><li><a class="tocitem" href="../../training/zygote/">Zygote.jl ðŸ“š (<code>gradient</code>, ...)</a></li></ul></li><li><span class="tocitem">Model Tools</span><ul><li><a class="tocitem" href="../../gpu/">GPU Support</a></li><li><a class="tocitem" href="../../saving/">Saving &amp; Loading</a></li><li><a class="tocitem" href="../../outputsize/">Shape Inference ðŸ“š</a></li><li><a class="tocitem" href="../../utilities/">Weight Initialisation ðŸ“š</a></li><li><a class="tocitem" href="../../destructure/">Flat vs. Nested ðŸ“š</a></li><li><a class="tocitem" href="../../models/functors/">Functors.jl ðŸ“š (<code>fmap</code>, ...)</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../linear_regression/">Linear Regression</a></li><li><a class="tocitem" href="../2020-09-15-deep-learning-flux/">Julia &amp; Flux: 60 Minute Blitz</a></li><li><a class="tocitem" href="../2021-01-26-mlp/">Multi-layer Perceptron</a></li><li><a class="tocitem" href="../2021-02-07-convnet/">Simple ConvNet</a></li><li><a class="tocitem" href="../2021-10-14-vanilla-gan/">Generative Adversarial Net</a></li><li class="is-active"><a class="tocitem" href>Deep Convolutional GAN</a><ul class="internal"><li><a class="tocitem" href="#What-are-GANs?"><span>What are GANs?</span></a></li><li><a class="tocitem" href="#Setup"><span>Setup</span></a></li><li><a class="tocitem" href="#Loading-the-data"><span>Loading the data</span></a></li><li><a class="tocitem" href="#Create-the-models"><span>Create the models</span></a></li><li><a class="tocitem" href="#Loss-functions-for-GAN"><span>Loss functions for GAN</span></a></li><li><a class="tocitem" href="#Utility-functions"><span>Utility functions</span></a></li><li><a class="tocitem" href="#Training"><span>Training</span></a></li><li><a class="tocitem" href="#Output"><span>Output</span></a></li><li><a class="tocitem" href="#Resources-and-References"><span>Resources &amp; References</span></a></li></ul></li><li><a class="tocitem" href="../../models/advanced/">Custom Layers</a></li></ul></li><li><a class="tocitem" href="../../performance/">Performance Tips</a></li><li><a class="tocitem" href="../../ecosystem/">Flux&#39;s Ecosystem</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Deep Convolutional GAN</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Deep Convolutional GAN</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/FluxML/Flux.jl/blob/master/docs/src/tutorials/2021-10-08-dcgan-mnist.md" title="Edit on GitHub"><span class="docs-icon fab">ï‚›</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Deep-Convolutional-Generative-Adversarial-Network-(DCGAN)"><a class="docs-heading-anchor" href="#Deep-Convolutional-Generative-Adversarial-Network-(DCGAN)">Deep Convolutional Generative Adversarial Network (DCGAN)</a><a id="Deep-Convolutional-Generative-Adversarial-Network-(DCGAN)-1"></a><a class="docs-heading-anchor-permalink" href="#Deep-Convolutional-Generative-Adversarial-Network-(DCGAN)" title="Permalink"></a></h1><p>This is a beginner level tutorial for generating images of handwritten digits using a <a href="https://arxiv.org/pdf/1511.06434.pdf">Deep Convolutional Generative Adversarial Network</a> inspired by the <a href="https://www.tensorflow.org/tutorials/generative/dcgan">TensorFlow tutorial on DCGAN</a>.</p><h2 id="What-are-GANs?"><a class="docs-heading-anchor" href="#What-are-GANs?">What are GANs?</a><a id="What-are-GANs?-1"></a><a class="docs-heading-anchor-permalink" href="#What-are-GANs?" title="Permalink"></a></h2><p><a href="https://arxiv.org/abs/1406.2661">Generative Adversarial Neural Networks or simply GANs</a> introduced by Goodfellow et al. is one of the most innovative ideas in modern-day machine learning. GANs are used extensively in the field of image and audio processing to generate high-quality synthetic data that can easily be passed off as real data.</p><p>A GAN is composed of two sub-models - the <strong>generator</strong> and the <strong>discriminator</strong> acting against one another. The generator can be considered as an artist who draws (generates) new images that look real, whereas the discriminator is a critic who learns to tell real images apart from fakes.</p><p><img src="https://fluxml.ai/assets/tutorialposts/2021-10-08-dcgan-mnist/cat_gan.png" alt/></p><p>The GAN starts with a generator and discriminator which have very little or no idea about the underlying data. During training, the generator progressively becomes better at creating images that look real, while the discriminator becomes better at telling them apart. The process reaches equilibrium when the discriminator can no longer distinguish real images from fakes.</p><p><img src="https://www.tensorflow.org/tutorials/generative/images/gan2.png" alt/></p><p><a href="https://www.tensorflow.org/tutorials/generative/dcgan">[source]</a></p><p>This tutorial demonstrates the process of training a DC-GAN on the <a href="http://yann.lecun.com/exdb/mnist/">MNIST dataset for handwritten digits</a>. The following animation shows a series of images produced by the generator as it was trained for 25 epochs. The images begin as random noise, but over time, the images become increasingly similar to handwritten numbers.</p><p><img src="https://fluxml.ai/assets/tutorialposts/2021-10-08-dcgan-mnist/output.gif" alt/></p><h2 id="Setup"><a class="docs-heading-anchor" href="#Setup">Setup</a><a id="Setup-1"></a><a class="docs-heading-anchor-permalink" href="#Setup" title="Permalink"></a></h2><p>We need to install some Julia packages before we start with our implementation of DCGAN.</p><pre><code class="language-julia hljs">using Pkg

# Activate a new project environment in the current directory
Pkg.activate(&quot;.&quot;)
# Add the required packages to the environment
Pkg.add([&quot;Images&quot;, &quot;Flux&quot;, &quot;MLDatasets&quot;, &quot;CUDA&quot;, &quot;Parameters&quot;])</code></pre><p><em>Note: Depending on your internet speed, it may take a few minutes for the packages install.</em></p><p>After installing the libraries, load the required packages and functions:</p><pre><code class="language-julia hljs">using Base.Iterators: partition
using Printf
using Statistics
using Random
using Images
using Flux: params, DataLoader
using Flux.Optimise: update!
using Flux.Losses: logitbinarycrossentropy
using MLDatasets: MNIST
using CUDA</code></pre><p>Now we set default values for the learning rates, batch size, epochs, the usage of a GPU (if available) and other hyperparameters for our model.</p><pre><code class="language-julia hljs">Base.@kwdef struct HyperParams
    batch_size::Int = 128
    latent_dim::Int = 100
    epochs::Int = 25
    verbose_freq::Int = 1000
    output_dim::Int = 5
    disc_lr::Float64 = 0.0002
    gen_lr::Float64 = 0.0002
    device::Function = gpu
end</code></pre><h2 id="Loading-the-data"><a class="docs-heading-anchor" href="#Loading-the-data">Loading the data</a><a id="Loading-the-data-1"></a><a class="docs-heading-anchor-permalink" href="#Loading-the-data" title="Permalink"></a></h2><p>As mentioned before, we will be using the MNIST dataset for handwritten digits. So we begin with a simple function for loading and pre-processing the MNIST images:</p><pre><code class="language-julia hljs">function load_MNIST_images(hparams)
    images = MNIST.traintensor(Float32)

    # Normalize the images to (-1, 1)
    normalized_images = @. 2f0 * images - 1f0
    image_tensor = reshape(normalized_images, 28, 28, 1, :)

    # Create a dataloader that iterates over mini-batches of the image tensor
    dataloader = DataLoader(image_tensor, batchsize=hparams.batch_size, shuffle=true)

    return dataloader
end</code></pre><p>To learn more about loading images in Flux, you can check out <a href="https://fluxml.ai/tutorials/2021/01/21/data-loader.html">this tutorial</a>.</p><p><em>Note: The data returned from the dataloader is loaded is on the CPU. To train on the GPU, we need to transfer the data to the GPU beforehand.</em></p><h2 id="Create-the-models"><a class="docs-heading-anchor" href="#Create-the-models">Create the models</a><a id="Create-the-models-1"></a><a class="docs-heading-anchor-permalink" href="#Create-the-models" title="Permalink"></a></h2><h3 id="Generator"><a class="docs-heading-anchor" href="#Generator">Generator</a><a id="Generator-1"></a><a class="docs-heading-anchor-permalink" href="#Generator" title="Permalink"></a></h3><p>Our generator, a.k.a. the artist, is a neural network that maps low dimensional data to a high dimensional form.</p><ul><li>This low dimensional data (seed) is generally a vector of random values sampled from a normal distribution.</li><li>The high dimensional data is the generated image.</li></ul><p>The <code>Dense</code> layer is used for taking the seed as an input which is upsampled several times using the <a href="https://fluxml.ai/Flux.jl/stable/models/layers/#Flux.ConvTranspose">ConvTranspose</a> layer until we reach the desired output size (in our case, 28x28x1). Furthermore, after each <code>ConvTranspose</code> layer, we apply the Batch Normalization to stabilize the learning process.</p><p>We will be using the <a href="https://fluxml.ai/Flux.jl/stable/models/nnlib/#NNlib.relu">relu</a> activation function for each layer except the output layer, where we use <code>tanh</code> activation.</p><p>We will also apply the weight initialization method mentioned in the original DCGAN paper.</p><pre><code class="language-julia hljs"># Function for intializing the model weights with values 
# sampled from a Gaussian distribution with Î¼=0 and Ïƒ=0.02
dcgan_init(shape...) = randn(Float32, shape) * 0.02f0</code></pre><pre><code class="language-julia hljs">function Generator(latent_dim)
    Chain(
        Dense(latent_dim, 7*7*256, bias=false),
        BatchNorm(7*7*256, relu),

        x -&gt; reshape(x, 7, 7, 256, :),

        ConvTranspose((5, 5), 256 =&gt; 128; stride = 1, pad = 2, init = dcgan_init, bias=false),
        BatchNorm(128, relu),

        ConvTranspose((4, 4), 128 =&gt; 64; stride = 2, pad = 1, init = dcgan_init, bias=false),
        BatchNorm(64, relu),

        # The tanh activation ensures that output is in range of (-1, 1)
        ConvTranspose((4, 4), 64 =&gt; 1, tanh; stride = 2, pad = 1, init = dcgan_init, bias=false),
    )
end</code></pre><p>Time for a small test!! We create a dummy generator and feed a random vector as a seed to the generator. If our generator is initialized correctly it will return an array of size (28, 28, 1, <code>batch_size</code>). The <code>@assert</code> macro in Julia will raise an exception for the wrong output size.</p><pre><code class="language-julia hljs"># Create a dummy generator of latent dim 100
generator = Generator(100)
noise = randn(Float32, 100, 3) # The last axis is the batch size

# Feed the random noise to the generator
gen_image = generator(noise)
@assert size(gen_image) == (28, 28, 1, 3)</code></pre><p>Our generator model is yet to learn the correct weights, so it does not produce a recognizable image for now. To train our poor generator we need its equal rival, the <em>discriminator</em>.</p><h3 id="Discriminator"><a class="docs-heading-anchor" href="#Discriminator">Discriminator</a><a id="Discriminator-1"></a><a class="docs-heading-anchor-permalink" href="#Discriminator" title="Permalink"></a></h3><p>The Discriminator is a simple CNN based image classifier. The <code>Conv</code> layer a is used with a <a href="https://fluxml.ai/Flux.jl/stable/models/nnlib/#NNlib.leakyrelu">leakyrelu</a> activation function. </p><pre><code class="language-julia hljs">function Discriminator()
    Chain(
        Conv((4, 4), 1 =&gt; 64; stride = 2, pad = 1, init = dcgan_init),
        x-&gt;leakyrelu.(x, 0.2f0),
        Dropout(0.3),

        Conv((4, 4), 64 =&gt; 128; stride = 2, pad = 1, init = dcgan_init),
        x-&gt;leakyrelu.(x, 0.2f0),
        Dropout(0.3),

        # The output is now of the shape (7, 7, 128, batch_size)
        flatten,
        Dense(7 * 7 * 128, 1) 
    )
end</code></pre><p>For a more detailed implementation of a CNN-based image classifier, you can refer to <a href="https://fluxml.ai/tutorials/2021/02/07/convnet.html">this tutorial</a>.</p><p>Now let us check if our discriminator is working:</p><pre><code class="language-julia hljs"># Dummy Discriminator
discriminator = Discriminator()
# We pass the generated image to the discriminator
logits = discriminator(gen_image)
@assert size(logits) == (1, 3)</code></pre><p>Just like our dummy generator, the untrained discriminator has no idea about what is a real or fake image. It needs to be trained alongside the generator to output positive values for real images, and negative values for fake images.</p><h2 id="Loss-functions-for-GAN"><a class="docs-heading-anchor" href="#Loss-functions-for-GAN">Loss functions for GAN</a><a id="Loss-functions-for-GAN-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-functions-for-GAN" title="Permalink"></a></h2><p>In a GAN problem, there are only two labels involved: fake and real. So Binary CrossEntropy is an easy choice for a preliminary loss function. </p><p>But even if Flux&#39;s <code>binarycrossentropy</code> does the job for us, due to numerical stability it is always preferred to compute cross-entropy using logits. Flux provides <a href="https://fluxml.ai/Flux.jl/stable/models/losses/#Flux.Losses.logitbinarycrossentropy">logitbinarycrossentropy</a> specifically for this purpose. Mathematically it is equivalent to <code>binarycrossentropy(Ïƒ(Å·), y, kwargs...).</code></p><h3 id="Discriminator-Loss"><a class="docs-heading-anchor" href="#Discriminator-Loss">Discriminator Loss</a><a id="Discriminator-Loss-1"></a><a class="docs-heading-anchor-permalink" href="#Discriminator-Loss" title="Permalink"></a></h3><p>The discriminator loss quantifies how well the discriminator can distinguish real images from fakes. It compares </p><ul><li>discriminator&#39;s predictions on real images to an array of 1s, and</li><li>discriminator&#39;s predictions on fake (generated) images to an array of 0s.</li></ul><p>These two losses are summed together to give a scalar loss. So we can write the loss function of the discriminator as:</p><pre><code class="language-julia hljs">function discriminator_loss(real_output, fake_output)
    real_loss = logitbinarycrossentropy(real_output, 1)
    fake_loss = logitbinarycrossentropy(fake_output, 0)
    return real_loss + fake_loss
end</code></pre><h3 id="Generator-Loss"><a class="docs-heading-anchor" href="#Generator-Loss">Generator Loss</a><a id="Generator-Loss-1"></a><a class="docs-heading-anchor-permalink" href="#Generator-Loss" title="Permalink"></a></h3><p>The generator&#39;s loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1).</p><pre><code class="language-julia hljs">generator_loss(fake_output) = logitbinarycrossentropy(fake_output, 1)</code></pre><p>We also need optimizers for our network. Why you may ask? Read more <a href="https://towardsdatascience.com/overview-of-various-optimizers-in-neural-networks-17c1be2df6d5">here</a>. For both the generator and discriminator, we will use the <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.ADAM">ADAM optimizer</a>.</p><h2 id="Utility-functions"><a class="docs-heading-anchor" href="#Utility-functions">Utility functions</a><a id="Utility-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Utility-functions" title="Permalink"></a></h2><p>The output of the generator ranges from (-1, 1), so it needs to be de-normalized before we can display it as an image. To make things a bit easier, we define a function to visualize the output of the generator as a grid of images. </p><pre><code class="language-julia hljs">function create_output_image(gen, fixed_noise, hparams)
    fake_images = cpu(gen.(fixed_noise))
    image_array = reduce(vcat, reduce.(hcat, partition(fake_images, hparams.output_dim)))
    image_array = permutedims(dropdims(image_array; dims=(3, 4)), (2, 1))
    image_array = @. Gray(image_array + 1f0) / 2f0
    return image_array
end</code></pre><h2 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h2><p>For the sake of simplifying our training problem, we will divide the generator and discriminator training into two separate functions. </p><pre><code class="language-julia hljs">function train_discriminator!(gen, disc, real_img, fake_img, opt, ps, hparams)

    disc_loss, grads = Flux.withgradient(ps) do
        discriminator_loss(disc(real_img), disc(fake_img))
    end

    # Update the discriminator parameters
    update!(opt, ps, grads)
    return disc_loss
end</code></pre><p>We define a similar function for the generator.</p><pre><code class="language-julia hljs">function train_generator!(gen, disc, fake_img, opt, ps, hparams)

    gen_loss, grads = Flux.withgradient(ps) do
        generator_loss(disc(fake_img))
    end

    update!(opt, ps, grads)
    return gen_loss
end</code></pre><p>Now that we have defined every function we need, we integrate everything into a single <code>train</code> function where we first set up all the models and optimizers and then train the GAN for a specified number of epochs.</p><pre><code class="language-julia hljs">function train(hparams)

    dev = hparams.device
    # Check if CUDA is actually present
    if hparams.device == gpu
        if !CUDA.has_cuda()
        dev = cpu
        @warn &quot;No gpu found, falling back to CPU&quot;
        end
    end

    # Load the normalized MNIST images
    dataloader = load_MNIST_images(hparams)

    # Initialize the models and pass them to correct device
    disc = Discriminator() |&gt; dev
    gen =  Generator(hparams.latent_dim) |&gt; dev

    # Collect the generator and discriminator parameters
    disc_ps = params(disc)
    gen_ps = params(gen)

    # Initialize the ADAM optimizers for both the sub-models
    # with respective learning rates
    disc_opt = ADAM(hparams.disc_lr)
    gen_opt = ADAM(hparams.gen_lr)

    # Create a batch of fixed noise for visualizing the training of generator over time
    fixed_noise = [randn(Float32, hparams.latent_dim, 1) |&gt; dev for _=1:hparams.output_dim^2]

    # Training loop
    train_steps = 0
    for ep in 1:hparams.epochs
        @info &quot;Epoch $ep&quot;
        for real_img in dataloader

            # Transfer the data to the GPU
            real_img = real_img |&gt; dev

            # Create a random noise
            noise = randn!(similar(real_img, (hparams.latent_dim, hparams.batch_size)))
            # Pass the noise to the generator to create a fake imagae
            fake_img = gen(noise)

            # Update discriminator and generator
            loss_disc = train_discriminator!(gen, disc, real_img, fake_img, disc_opt, disc_ps, hparams)
            loss_gen = train_generator!(gen, disc, fake_img, gen_opt, gen_ps, hparams)

            if train_steps % hparams.verbose_freq == 0
                @info(&quot;Train step $(train_steps), Discriminator loss = $(loss_disc), Generator loss = $(loss_gen)&quot;)
                # Save generated fake image
                output_image = create_output_image(gen, fixed_noise, hparams)
                save(@sprintf(&quot;output/dcgan_steps_%06d.png&quot;, train_steps), output_image)
            end
            train_steps += 1
        end
    end

    output_image = create_output_image(gen, fixed_noise, hparams)
    save(@sprintf(&quot;output/dcgan_steps_%06d.png&quot;, train_steps), output_image)

    return nothing
end</code></pre><p>Now we finally get to train the GAN:</p><pre><code class="language-julia hljs"># Define the hyper-parameters (here, we go with the default ones)
hparams = HyperParams()
train(hparams)</code></pre><h2 id="Output"><a class="docs-heading-anchor" href="#Output">Output</a><a id="Output-1"></a><a class="docs-heading-anchor-permalink" href="#Output" title="Permalink"></a></h2><p>The generated images are stored inside the <code>output</code> folder. To visualize the output of the generator over time, we create a gif of the generated images.</p><pre><code class="language-julia hljs">folder = &quot;output&quot;
# Get the image filenames from the folder
img_paths = readdir(folder, join=true)
# Load all the images as an array
images = load.(img_paths)
# Join all the images in the array to create a matrix of images
gif_mat = cat(images..., dims=3)
save(&quot;./output.gif&quot;, gif_mat)</code></pre><p><img src="https://fluxml.ai/assets/tutorialposts/2021-10-08-dcgan-mnist/output.gif" alt/></p><h2 id="Resources-and-References"><a class="docs-heading-anchor" href="#Resources-and-References">Resources &amp; References</a><a id="Resources-and-References-1"></a><a class="docs-heading-anchor-permalink" href="#Resources-and-References" title="Permalink"></a></h2><ul><li><a href="https://github.com/FluxML/model-zoo/blob/master/vision/dcgan_mnist/dcgan_mnist.jl">The DCGAN implementation in the Model Zoo.</a></li></ul><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Originally published at <a href="https://fluxml.ai/tutorials/">fluxml.ai</a> on 8 October 2021, by Deeptendu Santra</p></div></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../2021-10-14-vanilla-gan/">Â« Generative Adversarial Net</a><a class="docs-footer-nextpage" href="../../models/advanced/">Custom Layers Â»</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Wednesday 30 November 2022 19:41">Wednesday 30 November 2022</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
