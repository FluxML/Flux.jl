<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Logistic Regression · Flux</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-36890222-9', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/flux.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="Flux logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="Flux logo"/></a><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Welcome</a></li><li><span class="tocitem">Guide</span><ul><li><a class="tocitem" href="../../models/quickstart/">Quick Start</a></li><li><a class="tocitem" href="../../models/overview/">Fitting a Line</a></li><li><a class="tocitem" href="../../models/basics/">Gradients and Layers</a></li><li><a class="tocitem" href="../../training/training/">Training</a></li><li><a class="tocitem" href="../../models/recurrence/">Recurrence</a></li><li><a class="tocitem" href="../../gpu/">GPU Support</a></li><li><a class="tocitem" href="../../saving/">Saving &amp; Loading</a></li><li><a class="tocitem" href="../../performance/">Performance Tips</a></li></ul></li><li><a class="tocitem" href="../../ecosystem/">Ecosystem</a></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../../models/layers/">Built-in Layers</a></li><li><a class="tocitem" href="../../models/activation/">Activation Functions</a></li><li><a class="tocitem" href="../../utilities/">Weight Initialisation</a></li><li><a class="tocitem" href="../../models/losses/">Loss Functions</a></li><li><a class="tocitem" href="../../training/reference/">Training API</a></li><li><a class="tocitem" href="../../training/optimisers/">Optimisation Rules</a></li><li><a class="tocitem" href="../../outputsize/">Shape Inference</a></li><li><a class="tocitem" href="../../destructure/">Flat vs. Nested</a></li><li><a class="tocitem" href="../../training/callbacks/">Callback Helpers</a></li><li><a class="tocitem" href="../../training/zygote/">Gradients – Zygote.jl</a></li><li><a class="tocitem" href="../../data/mlutils/">Batching Data – MLUtils.jl</a></li><li><a class="tocitem" href="../../data/onehot/">OneHotArrays.jl</a></li><li><a class="tocitem" href="../../models/nnlib/">Low-level Operations – NNlib.jl</a></li><li><a class="tocitem" href="../../models/functors/">Nested Structures – Functors.jl</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../linear_regression/">Linear Regression</a></li><li class="is-active"><a class="tocitem" href>Logistic Regression</a><ul class="internal"><li><a class="tocitem" href="#Dataset"><span>Dataset</span></a></li><li><a class="tocitem" href="#Building-a-model"><span>Building a model</span></a></li><li><a class="tocitem" href="#Loss-and-accuracy"><span>Loss and accuracy</span></a></li><li><a class="tocitem" href="#Training-the-model"><span>Training the model</span></a></li></ul></li><li><a class="tocitem" href="../../models/advanced/">Custom Layers</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Logistic Regression</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Logistic Regression</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/FluxML/Flux.jl/blob/master/docs/src/tutorials/logistic_regression.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Logistic-Regression"><a class="docs-heading-anchor" href="#Logistic-Regression">Logistic Regression</a><a id="Logistic-Regression-1"></a><a class="docs-heading-anchor-permalink" href="#Logistic-Regression" title="Permalink"></a></h1><p>The following page contains a step-by-step walkthrough of the logistic regression algorithm in Julia using Flux. We will then create a simple logistic regression model without any usage of Flux and compare the different working parts with Flux&#39;s implementation. </p><p>Let&#39;s start by importing the required Julia packages.</p><pre><code class="language-julia-repl hljs">julia&gt; using Flux, Statistics, MLDatasets, DataFrames, OneHotArrays</code></pre><h2 id="Dataset"><a class="docs-heading-anchor" href="#Dataset">Dataset</a><a id="Dataset-1"></a><a class="docs-heading-anchor-permalink" href="#Dataset" title="Permalink"></a></h2><p>Let&#39;s start by importing a dataset from MLDatasets.jl. We will use the <code>Iris</code> dataset that contains the data of three different <code>Iris</code> species. The data consists of 150 data points (<code>x</code>s), each having four features. Each of these <code>x</code> is mapped to <code>y</code>, the name of a particular <code>Iris</code> specie. The following code will download the <code>Iris</code> dataset when run for the first time.</p><pre><code class="language-julia-repl hljs">julia&gt; Iris()
dataset Iris:
  metadata   =&gt;    Dict{String, Any} with 4 entries
  features   =&gt;    150×4 DataFrame
  targets    =&gt;    150×1 DataFrame
  dataframe  =&gt;    150×5 DataFrame

julia&gt; x, y = Iris(as_df=false)[:];</code></pre><p>Let&#39;s have a look at our dataset -</p><pre><code class="language-julia-repl hljs">julia&gt; y
1×150 Matrix{InlineStrings.String15}:
 &quot;Iris-setosa&quot;  &quot;Iris-setosa&quot;  …  &quot;Iris-virginica&quot;  &quot;Iris-virginica&quot;

julia&gt; x |&gt; summary
&quot;4×150 Matrix{Float64}&quot;</code></pre><p>The <code>y</code> values here corresponds to a type of iris plant, with a total of 150 data points. The <code>x</code> values depict the sepal length, sepal width, petal length, and petal width (all in <code>cm</code>) of 150 iris plant (hence the matrix size <code>4×150</code>). Different type of iris plants have different lengths and widths of sepals and petals associated with them, and there is a definitive pattern for this in nature. We can leverage this to train a simple classifier that outputs the type of iris plant using the length and width of sepals and petals as inputs.</p><p>Our next step would be to convert this data into a form that can be fed to a machine learning model. The <code>x</code> values are arranged in a matrix and should ideally be converted to <code>Float32</code> type (see <a href="tutorials/@ref man-performance-tips">Performance tips</a>), but the labels must be one hot encoded. <a href="https://discourse.julialang.org/t/all-the-ways-to-do-one-hot-encoding/64807">Here</a> is a great discourse thread on different techniques that can be used to one hot encode data with or without using any external Julia package.</p><pre><code class="language-julia-repl hljs">julia&gt; x = Float32.(x);

julia&gt; y = vec(y);

julia&gt; custom_y_onehot = unique(y) .== permutedims(y)
3×150 BitMatrix:
 1  1  1  1  1  1  1  1  1  1  1  1  1  …  0  0  0  0  0  0  0  0  0  0  0  0
 0  0  0  0  0  0  0  0  0  0  0  0  0     0  0  0  0  0  0  0  0  0  0  0  0
 0  0  0  0  0  0  0  0  0  0  0  0  0     1  1  1  1  1  1  1  1  1  1  1  1</code></pre><p>This same operation can also be performed using <a href="https://github.com/FluxML/OneHotArrays.jl">OneHotArrays</a>&#39; <code>onehotbatch</code> function. We will use both of these outputs parallelly to show how intuitive FluxML is!</p><pre><code class="language-julia-repl hljs">julia&gt; const classes = [&quot;Iris-setosa&quot;, &quot;Iris-versicolor&quot;, &quot;Iris-virginica&quot;];

julia&gt; flux_y_onehot = onehotbatch(y, classes)
3×150 OneHotMatrix(::Vector{UInt32}) with eltype Bool:
 1  1  1  1  1  1  1  1  1  1  1  1  1  …  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅
 ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅
 ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅  ⋅     1  1  1  1  1  1  1  1  1  1  1  1</code></pre><p>Our data is ready. The next step would be to build a classifier for the same.</p><h2 id="Building-a-model"><a class="docs-heading-anchor" href="#Building-a-model">Building a model</a><a id="Building-a-model-1"></a><a class="docs-heading-anchor-permalink" href="#Building-a-model" title="Permalink"></a></h2><p>A logistic regression model is defined mathematically as -</p><p class="math-container">\[model(x) = σ(Wx + b)\]</p><p>where <code>W</code> is the weight matrix, <code>b</code> is the bias vector, and <code>σ</code> is any activation function. For our case, let&#39;s use the <code>softmax</code> activation function as we will be performing a multiclass classification task.</p><pre><code class="language-julia-repl hljs">julia&gt; m(W, b, x) = W*x .+ b
m (generic function with 1 method)</code></pre><p>Note that this model lacks an activation function, but we will come back to that.</p><p>We can now move ahead to initialize the parameters of our model. Given that our model has four inputs (4 features in every data point), and three outputs (3 different classes), the parameters can be initialized in the following way -</p><pre><code class="language-julia-repl hljs">julia&gt; W = rand(Float32, 3, 4);

julia&gt; b = [0.0f0, 0.0f0, 0.0f0];</code></pre><p>Now our model can take in the complete dataset and predict the class of each <code>x</code> in one go. But, we need to ensure that our model outputs the probabilities of an input belonging to the respective classes. As our model has three outputs, each would denote the probability of the input belonging to a particular class.</p><p>We will use an activation function to map our outputs to a probability value. It would make sense to use a <code>softmax</code> activation function here, which is defined mathematically as -</p><p class="math-container">\[σ(\vec{x}) = \frac{\\e^{z_{i}}}{\\sum_{j=1}^{k} \\e^{z_{j}}}\]</p><p>The <code>softmax</code> function scales down the outputs to probability values such that the sum of all the final outputs equals <code>1</code>. Let&#39;s implement this in Julia.</p><pre><code class="language-julia-repl hljs">julia&gt; custom_softmax(x) = exp.(x) ./ sum(exp.(x), dims=1)
custom_softmax (generic function with 1 method)</code></pre><p>The implementation looks straightforward enough! Note that we specify <code>dims=1</code> in the <code>sum</code> function to calculate the sum of probabilities in each column. Remember, we will have a 3×150 matrix (predicted <code>y</code>s) as the output of our model, where each column would be an output of a corresponding input.</p><p>Let&#39;s combine this <code>softmax</code> function with our model to construct the complete <code>custom_model</code>.</p><pre><code class="language-julia-repl hljs">julia&gt; custom_model(W, b, x) = m(W, b, x) |&gt; custom_softmax
custom_model (generic function with 1 method)</code></pre><p>Let&#39;s check if our model works.</p><pre><code class="language-julia-repl hljs">julia&gt; custom_model(W, b, x) |&gt; size
(3, 150)</code></pre><p>It works! Let&#39;s check if the <code>softmax</code> function is working.</p><pre><code class="language-julia-repl hljs">julia&gt; all(0 .&lt;= custom_model(W, b, x) .&lt;= 1)
true

julia&gt; sum(custom_model(W, b, x), dims=1)
1×150 Matrix{Float32}:
 1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  …  1.0  1.0  1.0  1.0  1.0  1.0  1.0</code></pre><p>Every output value is between <code>0</code> and <code>1</code>, and every column adds to <code>1</code>!</p><p>Let&#39;s convert our <code>custom_model</code> to a Flux model. Flux provides the users with a very elegant API that almost feels like writing your code!</p><p>Note, all the <code>flux_*</code> variables in this tutorial would be general, that is, they can be used as it is with some other similar-looking dataset, but the <code>custom_*</code> variables will remain specific to this tutorial.</p><pre><code class="language-julia-repl hljs">julia&gt; flux_model = Chain(Dense(4 =&gt; 3), softmax)
Chain(
  Dense(4 =&gt; 3),                        # 15 parameters
  NNlib.softmax,
)</code></pre><p>A <a href="../../models/layers/#Flux.Dense"><code>Dense(4 =&gt; 3)</code></a> layer denotes a layer with four inputs (four features in every data point) and three outputs (three classes or labels). This layer is the same as the mathematical model defined by us above. Under the hood, Flux too calculates the output using the same expression, but we don&#39;t have to initialize the parameters ourselves this time, instead Flux does it for us.</p><p>The <code>softmax</code> function provided by NNLib.jl is re-exported by Flux, which has been used here. Lastly, Flux provides users with a <code>Chain</code> struct which makes stacking layers seamless.</p><p>A model&#39;s weights and biases can be accessed as follows -</p><pre><code class="language-julia-repl hljs">julia&gt; flux_model[1].weight, flux_model[1].bias
(Float32[0.78588694 -0.45968163 -0.77409476 0.2358028; -0.9049773 -0.58643705 0.466441 -0.79523873; 0.82426906 0.4143493 0.7630932 0.020588955], Float32[0.0, 0.0, 0.0])</code></pre><p>We can now pass the complete data in one go, with each data point having four features (four inputs)!</p><h2 id="Loss-and-accuracy"><a class="docs-heading-anchor" href="#Loss-and-accuracy">Loss and accuracy</a><a id="Loss-and-accuracy-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-and-accuracy" title="Permalink"></a></h2><p>Our next step should be to define some quantitative values for our model, which we will maximize or minimize during the complete training procedure. These values will be the loss function and the accuracy metric.</p><p>Let&#39;s start by defining a loss function, a <code>logitcrossentropy</code> function.</p><pre><code class="language-julia-repl hljs">julia&gt; custom_logitcrossentropy(ŷ, y) = mean(.-sum(y .* logsoftmax(ŷ; dims = 1); dims = 1));</code></pre><p>Now we can wrap the <code>custom_logitcrossentropy</code> inside a function that takes in the model parameters, <code>x</code>s, and <code>y</code>s, and returns the loss value.</p><pre><code class="language-julia-repl hljs">julia&gt; function custom_loss(W, b, x, y)
           ŷ = custom_model(W, b, x)
           custom_logitcrossentropy(ŷ, y)
       end;

julia&gt; custom_loss(W, b, x, custom_y_onehot)
1.1714406827505623</code></pre><p>The loss function works!</p><p>Flux provides us with many minimal yet elegant loss functions. In fact, the <code>custom_logitcrossentropy</code> defined above has been taken directly from Flux. The functions present in Flux includes sanity checks, ensures efficient performance, and behaves well with the overall FluxML ecosystem.</p><pre><code class="language-julia-repl hljs">julia&gt; function flux_loss(flux_model, x, y)
           ŷ = flux_model(x)
           Flux.logitcrossentropy(ŷ, y)
       end;

julia&gt; flux_loss(flux_model, x, flux_y_onehot)
1.2156688659673647</code></pre><p>Next, let&#39;s define an accuracy function, which we will try to maximize during our training procedure. Before jumping to accuracy, let&#39;s define a <code>onecold</code> function. The <code>onecold</code> function would convert our output, which remember, are probability values, to the actual class names.</p><p>We can divide this task into two parts -</p><ol><li>Identify the index of the maximum element of each column in the output matrix</li><li>Convert this index to a class name</li></ol><p>The maximum index should be calculated along the columns (remember, each column is the output of a single <code>x</code> data point). We can use Julia&#39;s <code>argmax</code> function to achieve this.</p><pre><code class="language-julia-repl hljs">julia&gt; argmax(custom_y_onehot, dims=1)  # calculate the cartesian index of max element column-wise
1×150 Matrix{CartesianIndex{2}}:
 CartesianIndex(1, 1)  CartesianIndex(1, 2)  …  CartesianIndex(3, 150)

julia&gt; max_idx = [x[1] for x in argmax(custom_y_onehot; dims=1)]
1×150 Matrix{Int64}:
 1  1  1  1  1  1  1  1  1  1  1  1  1  …  3  3  3  3  3  3  3  3  3  3  3  3</code></pre><p>Now we can write a function that calculates the indices of the maximum element in each column, and maps them to a class name.</p><pre><code class="language-julia-repl hljs">julia&gt; function custom_onecold(custom_y_onehot)
           max_idx = [x[1] for x in argmax(custom_y_onehot; dims=1)]
           vec(classes[max_idx])
       end;

julia&gt; custom_onecold(custom_y_onehot)
150-element Vector{String}:
 &quot;Iris-setosa&quot;
 &quot;Iris-setosa&quot;
 &quot;Iris-setosa&quot;
 &quot;Iris-setosa&quot;
 &quot;Iris-setosa&quot;
 &quot;Iris-setosa&quot;
 &quot;Iris-setosa&quot;
 &quot;Iris-setosa&quot;
 &quot;Iris-setosa&quot;
 &quot;Iris-setosa&quot;
 ⋮
 &quot;Iris-virginica&quot;
 &quot;Iris-virginica&quot;
 &quot;Iris-virginica&quot;
 &quot;Iris-virginica&quot;
 &quot;Iris-virginica&quot;
 &quot;Iris-virginica&quot;
 &quot;Iris-virginica&quot;
 &quot;Iris-virginica&quot;
 &quot;Iris-virginica&quot;</code></pre><p>It works!</p><p>Flux provides users with the <code>onecold</code> function so that we don&#39;t have to write it on our own. Let&#39;s see how our <code>custom_onecold</code> function compares to <code>Flux.onecold</code>.</p><pre><code class="language-julia-repl hljs">julia&gt; istrue = Flux.onecold(flux_y_onehot, classes) .== custom_onecold(custom_y_onehot);

julia&gt; all(istrue)
true</code></pre><p>Both the functions act identically!</p><p>We now move to the <code>accuracy</code> metric and run it with the untrained <code>custom_model</code>.</p><pre><code class="language-julia-repl hljs">julia&gt; custom_accuracy(W, b, x, y) = mean(custom_onecold(custom_model(W, b, x)) .== y);

julia&gt; custom_accuracy(W, b, x, y)
0.3333333333333333</code></pre><p>We could also have used Flux&#39;s built-in functionality to define this accuracy function.</p><pre><code class="language-julia-repl hljs">julia&gt; flux_accuracy(x, y) = mean(Flux.onecold(flux_model(x), classes) .== y);

julia&gt; flux_accuracy(x, y)
0.24</code></pre><h2 id="Training-the-model"><a class="docs-heading-anchor" href="#Training-the-model">Training the model</a><a id="Training-the-model-1"></a><a class="docs-heading-anchor-permalink" href="#Training-the-model" title="Permalink"></a></h2><p>Let&#39;s train our model using the classic Gradient Descent algorithm. According to the gradient descent algorithm, the weights and biases should be iteratively updated using the following mathematical equations -</p><p class="math-container">\[\begin{aligned}
W &amp;= W - \eta * \frac{dL}{dW} \\
b &amp;= b - \eta * \frac{dL}{db}
\end{aligned}\]</p><p>Here, <code>W</code> is the weight matrix, <code>b</code> is the bias vector, <span>$\eta$</span> is the learning rate, <span>$\frac{dL}{dW}$</span> is the derivative of the loss function with respect to the weight, and <span>$\frac{dL}{db}$</span> is the derivative of the loss function with respect to the bias.</p><p>The derivatives are calculated using an Automatic Differentiation tool, and Flux uses <a href="https://github.com/FluxML/Zygote.jl"><code>Zygote.jl</code></a> for the same. Since Zygote.jl is an independent Julia package, it can be used outside of Flux as well! Refer to the documentation of Zygote.jl for more information on the same.</p><p>Our first step would be to obtain the gradient of the loss function with respect to the weights and the biases. Flux re-exports Zygote&#39;s <code>gradient</code> function; hence, we don&#39;t need to import Zygote explicitly to use the functionality. <code>gradient</code> takes in a function and its arguments, and returns a tuple containing <code>∂f/∂x</code> for each argument x. Let&#39;s pass in <code>custom_loss</code> and the arguments required by <code>custom_loss</code> to <code>gradient</code>. We will require the derivatives of the loss function (<code>custom_loss</code>) with respect to the weights (<code>∂f/∂w</code>) and the bias (<code>∂f/∂b</code>) to carry out gradient descent, but we can ignore the partial derivatives of the loss function (<code>custom_loss</code>) with respect to <code>x</code> (<code>∂f/∂x</code>) and one hot encoded <code>y</code> (<code>∂f/∂y</code>).</p><pre><code class="language-julia-repl hljs">julia&gt; dLdW, dLdb, _, _ = gradient(custom_loss, W, b, x, custom_y_onehot);</code></pre><p>We can now update the parameters, following the gradient descent algorithm -</p><pre><code class="language-julia-repl hljs">julia&gt; W .= W .- 0.1 .* dLdW;

julia&gt; b .= b .- 0.1 .* dLdb;</code></pre><p>The parameters have been updated! We can now check the value of our custom loss function -</p><pre><code class="language-julia-repl hljs">julia&gt; custom_loss(W, b, x, custom_y_onehot)
1.164742997664842</code></pre><p>The loss went down! Let&#39;s plug our super training logic inside a function.</p><pre><code class="language-julia-repl hljs">julia&gt; function train_custom_model()
           dLdW, dLdb, _, _ = gradient(custom_loss, W, b, x, custom_y_onehot)
           W .= W .- 0.1 .* dLdW
           b .= b .- 0.1 .* dLdb
       end;</code></pre><p>We can plug the training function inside a loop and train the model for more epochs. The loop can be tailored to suit the user&#39;s needs, and the conditions can be specified in plain Julia. Here we will train the model for a maximum of <code>500</code> epochs, but to ensure that the model does not overfit, we will break as soon as our accuracy value crosses or becomes equal to <code>0.98</code>.</p><pre><code class="language-julia-repl hljs">julia&gt; for i = 1:500
            train_custom_model();
            custom_accuracy(W, b, x, y) &gt;= 0.98 &amp;&amp; break
       end
    
julia&gt; @show custom_accuracy(W, b, x, y);
custom_accuracy(W, b, x, y) = 0.98</code></pre><p>Everything works! Our model achieved an accuracy of <code>0.98</code>! Let&#39;s have a look at the loss.</p><pre><code class="language-julia-repl hljs">julia&gt; custom_loss(W, b, x, custom_y_onehot)
0.6520349798243569</code></pre><p>As expected, the loss went down too! Now, let&#39;s repeat the same steps with our <code>flux_model</code>.</p><p>We can write a similar-looking training loop for our <code>flux_model</code> and train it similarly.</p><pre><code class="language-julia-repl hljs">julia&gt; flux_loss(flux_model, x, flux_y_onehot)
1.215731131385928

julia&gt; function train_flux_model()
           dLdm, _, _ = gradient(flux_loss, flux_model, x, flux_y_onehot)
           @. flux_model[1].weight = flux_model[1].weight - 0.1 * dLdm[:layers][1][:weight]
           @. flux_model[1].bias = flux_model[1].bias - 0.1 * dLdm[:layers][1][:bias]
       end;

julia&gt; for i = 1:500
            train_flux_model();
            flux_accuracy(x, y) &gt;= 0.98 &amp;&amp; break
       end</code></pre><p>Looking at the accuracy and loss value -</p><pre><code class="language-julia-repl hljs">julia&gt; @show flux_accuracy(x, y);
flux_accuracy(x, y) = 0.98

julia&gt; flux_loss(flux_model, x, flux_y_onehot)
0.6952386604624324</code></pre><p>We see a very similar final loss and accuracy.</p><hr/><p>Summarising this tutorial, we saw how we can run a logistic regression algorithm in Julia with and without using Flux. We started by importing the classic <code>Iris</code> dataset, and one hot encoded the labels. Next, we defined our model, the loss function, and the accuracy, all by ourselves.</p><p>Finally, we trained the model by manually writing down the Gradient Descent algorithm and optimising the loss. Interestingly, we implemented most of the functions on our own, and then parallelly compared them with the functionalities provided by Flux!</p><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Originally published on 1st April 2023, by Saransh Chopra.</p></div></div></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../linear_regression/">« Linear Regression</a><a class="docs-footer-nextpage" href="../../models/advanced/">Custom Layers »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Thursday 28 December 2023 18:12">Thursday 28 December 2023</span>. Using Julia version 1.10.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
