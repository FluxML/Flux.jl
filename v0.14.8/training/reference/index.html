<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Training API · Flux</title><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-36890222-9', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/flux.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img class="docs-light-only" src="../../assets/logo.png" alt="Flux logo"/><img class="docs-dark-only" src="../../assets/logo-dark.png" alt="Flux logo"/></a><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Welcome</a></li><li><span class="tocitem">Guide</span><ul><li><a class="tocitem" href="../../models/quickstart/">Quick Start</a></li><li><a class="tocitem" href="../../models/overview/">Fitting a Line</a></li><li><a class="tocitem" href="../../models/basics/">Gradients and Layers</a></li><li><a class="tocitem" href="../training/">Training</a></li><li><a class="tocitem" href="../../models/recurrence/">Recurrence</a></li><li><a class="tocitem" href="../../gpu/">GPU Support</a></li><li><a class="tocitem" href="../../saving/">Saving &amp; Loading</a></li><li><a class="tocitem" href="../../performance/">Performance Tips</a></li></ul></li><li><a class="tocitem" href="../../ecosystem/">Ecosystem</a></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../../models/layers/">Built-in Layers</a></li><li><a class="tocitem" href="../../models/activation/">Activation Functions</a></li><li><a class="tocitem" href="../../utilities/">Weight Initialisation</a></li><li><a class="tocitem" href="../../models/losses/">Loss Functions</a></li><li class="is-active"><a class="tocitem" href>Training API</a><ul class="internal"><li><a class="tocitem" href="#Optimisation-Modifiers"><span>Optimisation Modifiers</span></a></li><li><a class="tocitem" href="#Implicit-style-(Flux-0.14)"><span>Implicit style (Flux ≤ 0.14)</span></a></li><li><a class="tocitem" href="#Callbacks"><span>Callbacks</span></a></li></ul></li><li><a class="tocitem" href="../optimisers/">Optimisation Rules</a></li><li><a class="tocitem" href="../../outputsize/">Shape Inference</a></li><li><a class="tocitem" href="../../destructure/">Flat vs. Nested</a></li><li><a class="tocitem" href="../callbacks/">Callback Helpers</a></li><li><a class="tocitem" href="../zygote/">Gradients – Zygote.jl</a></li><li><a class="tocitem" href="../../data/mlutils/">Batching Data – MLUtils.jl</a></li><li><a class="tocitem" href="../../data/onehot/">OneHotArrays.jl</a></li><li><a class="tocitem" href="../../models/nnlib/">Low-level Operations – NNlib.jl</a></li><li><a class="tocitem" href="../../models/functors/">Nested Structures – Functors.jl</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../tutorials/linear_regression/">Linear Regression</a></li><li><a class="tocitem" href="../../tutorials/logistic_regression/">Logistic Regression</a></li><li><a class="tocitem" href="../../models/advanced/">Custom Layers</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Reference</a></li><li class="is-active"><a href>Training API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Training API</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/FluxML/Flux.jl/blob/master/docs/src/training/reference.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Training-API-Reference"><a class="docs-heading-anchor" href="#Training-API-Reference">Training API Reference</a><a id="Training-API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Training-API-Reference" title="Permalink"></a></h1><p>The new version of Flux&#39;s training code was written as an independent package, <a href="https://github.com/FluxML/Optimisers.jl">Optimisers.jl</a>. Only the function <code>train!</code> belongs to Flux itself.</p><p>The Optimisers package is designed to allow for immutable objects. But at present all Flux models contain parameter arrays (such as <code>Array</code>s and <code>CuArray</code>s) which can be updated in-place. Because of this:</p><ul><li>The objects returned by <code>Optimisers.update!</code> can be ignored.</li><li>Flux defines its own version of <code>setup</code> which checks this assumption. (Using instead <code>Optimisers.setup</code> will also work, they return the same thing.)</li></ul><p>The new implementation of rules such as Adam in the Optimisers is quite different from the old one in <code>Flux.Optimise</code>. In Flux 0.14, <code>Flux.Adam()</code> returns the old one, with supertype <code>Flux.Optimise.AbstractOptimiser</code>, but <code>setup</code> will silently translate it to its new counterpart. The available rules are listed the <a href="../optimisers/#man-optimisers">optimisation rules</a> page here; see the <a href="https://fluxml.ai/Optimisers.jl/dev/">Optimisers documentation</a> for details on how the new rules work.</p><article class="docstring"><header><a class="docstring-binding" id="Flux.Train.setup" href="#Flux.Train.setup"><code>Flux.Train.setup</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">opt_state = setup(rule, model)</code></pre><p>This is a version of <code>Optimisers.setup</code>, and is the first step before using <a href="#Flux.Optimise.train!-NTuple{4, Any}"><code>train!</code></a>. It differs from <code>Optimisers.setup</code> in that it:</p><ul><li>has one extra check for mutability (since Flux expects to mutate the model in-place, while Optimisers.jl is designed to return an updated model)</li><li>has methods which accept Flux&#39;s old optimisers, and convert them. (The old <code>Flux.Optimise.Adam</code> and new <code>Optimisers.Adam</code> are distinct types.)</li></ul><div class="admonition is-compat"><header class="admonition-header">New</header><div class="admonition-body"><p>This function was added in Flux 0.13.9. It was not used by the old &quot;implicit&quot; interface, using <code>Flux.Optimise</code> module and <a href="#Flux.params"><code>Flux.params</code></a>.</p></div></div><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; model = Dense(2=&gt;1, leakyrelu; init=ones);

julia&gt; opt_state = Flux.setup(Momentum(0.1), model)  # this encodes the optimiser and its state
(weight = Leaf(Momentum{Float64}(0.1, 0.9), [0.0 0.0]), bias = Leaf(Momentum{Float64}(0.1, 0.9), [0.0]), σ = ())

julia&gt; x1, y1 = [0.2, -0.3], [0.4];  # use the same data for two steps:

julia&gt; Flux.train!(model, [(x1, y1), (x1, y1)], opt_state) do m, x, y
         sum(abs.(m(x) .- y)) * 100
       end

julia&gt; model.bias  # was zero, mutated by Flux.train!
1-element Vector{Float64}:
 10.19

julia&gt; opt_state  # mutated by Flux.train!
(weight = Leaf(Momentum{Float64}(0.1, 0.9), [-2.018 3.027]), bias = Leaf(Momentum{Float64}(0.1, 0.9), [-10.09]), σ = ())</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/df468ba0a10d86b660eebed98d1b484cbce113a7/src/train.jl#L14-L48">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.train!-NTuple{4, Any}" href="#Flux.Optimise.train!-NTuple{4, Any}"><code>Flux.Optimise.train!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">train!(loss, model, data, opt_state)</code></pre><p>Uses a <code>loss</code> function and training <code>data</code> to improve the <code>model</code>&#39;s parameters according to a particular optimisation rule encoded in <code>opt_state</code>.  Iterates through <code>data</code> once, evaluating for each <code>d in data</code> either <code>loss(model, d...)</code> if <code>d isa Tuple</code>, or else <code>loss(model, d)</code> for other <code>d</code>.</p><p>For example, with these definitions...</p><pre><code class="nohighlight hljs">data = [(x1, y1), (x2, y2), (x3, y3)]

loss3(m, x, y) = norm(m(x) .- y)        # the model is the first argument

opt_state = Flux.setup(Adam(), model)   # explicit setup of optimiser momenta</code></pre><p>...calling <code>Flux.train!(loss3, model, data, opt_state)</code> runs a loop much like this:</p><pre><code class="nohighlight hljs">for d in data
    ∂L∂m = gradient(loss3, model, d...)[1]
    update!(opt_state, model, ∂L∂m)
end</code></pre><p>You can also write this loop yourself, if you need more flexibility. For this reason <code>train!</code> is not highly extensible. It adds only a few features to the loop above:</p><ul><li><p>Stop with a <code>DomainError</code> if the loss is infinite or <code>NaN</code> at any point.</p></li><li><p>Show a progress bar using <a href="https://github.com/JuliaLogging/ProgressLogging.jl"><code>@withprogress</code></a>.</p></li></ul><div class="admonition is-compat"><header class="admonition-header">New</header><div class="admonition-body"><p>This method was added in Flux 0.13.9. It has significant changes from the one used by Flux ≤ 0.13:</p><ul><li>It now takes the <code>model</code> itself, not the result of <a href="#Flux.params"><code>Flux.params</code></a>. (This is to move away from Zygote&#39;s &quot;implicit&quot; parameter handling, with <code>Grads</code>.)</li><li>Instead of <code>loss</code> being a function which accepts only the data, now it must also accept the <code>model</code> itself, as the first argument.</li><li><code>opt_state</code> should be the result of <a href="#Flux.Train.setup"><code>Flux.setup</code></a>. Using an optimiser such as <code>Adam()</code> without this step should give you a warning.</li><li>Callback functions are not supported. (But any code can be included in the above <code>for</code> loop.)</li></ul></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/df468ba0a10d86b660eebed98d1b484cbce113a7/src/train.jl#L59-L101">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.update!" href="#Optimisers.update!"><code>Optimisers.update!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.update!(tree, model, gradient) -&gt; (tree, model)</code></pre><p>Uses the optimiser and the gradient to change the trainable parameters in the model. Returns the improved model, and the optimiser states needed for the next update. The initial tree of states comes from <a href="training/@ref"><code>setup</code></a>.</p><p>This is used in exactly the same manner as <a href="training/@ref"><code>update</code></a>, but because it may mutate arrays within the old model (and the old state), it will be faster for models of ordinary <code>Array</code>s or <code>CuArray</code>s. However, you should not rely on the old model being fully updated but rather use the returned model. (The original state tree is always mutated, as each <code>Leaf</code> is mutable.)</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using StaticArrays, Zygote, Optimisers

julia&gt; m = (x = [1f0, 2f0], y = SA[4f0, 5f0]);  # partly mutable model

julia&gt; t = Optimisers.setup(Momentum(1/30, 0.9), m)  # tree of states
(x = Leaf(Momentum(0.0333333, 0.9), Float32[0.0, 0.0]), y = Leaf(Momentum(0.0333333, 0.9), Float32[0.0, 0.0]))

julia&gt; g = gradient(m -&gt; sum(abs2.(m.x .+ m.y)), m)[1]  # structural gradient
(x = Float32[10.0, 14.0], y = Float32[10.0, 14.0])

julia&gt; t2, m2 = Optimisers.update!(t, m, g);

julia&gt; m2  # after update or update!, this is the new model
(x = Float32[0.6666666, 1.5333333], y = Float32[3.6666667, 4.5333333])

julia&gt; m2.x === m.x  # update! has re-used this array, for efficiency
true

julia&gt; m  # original should be discarded, may be mutated but no guarantee
(x = Float32[0.6666666, 1.5333333], y = Float32[4.0, 5.0])

julia&gt; t == t2  # original state tree is guaranteed to be mutated
true</code></pre></div></section></article><p><code>train!</code> uses <a href="https://github.com/JuliaLogging/ProgressLogging.jl"><code>@progress</code></a> which should show a progress bar in VSCode automatically. To see one in a terminal, you will need to install <a href="https://github.com/JuliaLogging/TerminalLoggers.jl">TerminalLoggers.jl</a> and follow its setup instructions.</p><h2 id="Optimisation-Modifiers"><a class="docs-heading-anchor" href="#Optimisation-Modifiers">Optimisation Modifiers</a><a id="Optimisation-Modifiers-1"></a><a class="docs-heading-anchor-permalink" href="#Optimisation-Modifiers" title="Permalink"></a></h2><p>The state returned by <code>setup</code> can be modified to temporarily prevent training of some parts of the model, or to change the learning rate or other hyperparameter. The functions for doing so may be accessed as <code>Flux.freeze!</code>, <code>Flux.thaw!</code>, and <code>Flux.adjust!</code>. All mutate the state (or part of it) and return <code>nothing</code>.</p><article class="docstring"><header><a class="docstring-binding" id="Optimisers.adjust!" href="#Optimisers.adjust!"><code>Optimisers.adjust!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.adjust!(tree, η)</code></pre><p>Alters the state <code>tree = setup(rule, model)</code> to change the parameters of the optimisation rule, without destroying its stored state. Typically used mid-way through training.</p><p>Can be applied to part of a model, by acting only on the corresponding part of the state <code>tree</code>.</p><p>To change just the learning rate, provide a number <code>η::Real</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; m = (vec = rand(Float32, 2), fun = sin);

julia&gt; st = Optimisers.setup(Nesterov(), m)  # stored momentum is initialised to zero
(vec = Leaf(Nesterov(0.001, 0.9), Float32[0.0, 0.0]), fun = ())

julia&gt; st, m = Optimisers.update(st, m, (vec = [16, 88], fun = nothing));  # with fake gradient

julia&gt; st
(vec = Leaf(Nesterov(0.001, 0.9), Float32[-0.016, -0.088]), fun = ())

julia&gt; Optimisers.adjust!(st, 0.123)  # change learning rate, stored momentum untouched

julia&gt; st
(vec = Leaf(Nesterov(0.123, 0.9), Float32[-0.016, -0.088]), fun = ())</code></pre><p>To change other parameters, <code>adjust!</code> also accepts keyword arguments matching the field names of the optimisation rule&#39;s type.</p><pre><code class="language-julia-repl hljs">julia&gt; fieldnames(Adam)
(:eta, :beta, :epsilon)

julia&gt; st2 = Optimisers.setup(OptimiserChain(ClipGrad(), Adam()), m)
(vec = Leaf(OptimiserChain(ClipGrad(10.0), Adam(0.001, (0.9, 0.999), 1.0e-8)), (nothing, (Float32[0.0, 0.0], Float32[0.0, 0.0], (0.9, 0.999)))), fun = ())

julia&gt; Optimisers.adjust(st2; beta = (0.777, 0.909), delta = 11.1)  # delta acts on ClipGrad
(vec = Leaf(OptimiserChain(ClipGrad(11.1), Adam(0.001, (0.777, 0.909), 1.0e-8)), (nothing, (Float32[0.0, 0.0], Float32[0.0, 0.0], (0.9, 0.999)))), fun = ())

julia&gt; Optimisers.adjust(st; beta = &quot;no such field&quot;)  # silently ignored!
(vec = Leaf(Nesterov(0.123, 0.9), Float32[-0.016, -0.088]), fun = ())</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.freeze!" href="#Optimisers.freeze!"><code>Optimisers.freeze!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.freeze!(tree)</code></pre><p>Temporarily alters the state <code>tree = setup(rule, model)</code> so that parameters will not be updated. Un-done by <a href="#Optimisers.thaw!"><code>thaw!</code></a>.</p><p>Can be applied to the state corresponding to only part of a model, for instance with <code>model::Chain</code>, to freeze <code>model.layers[1]</code> you should call <code>freeze!(tree.layers[1])</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia-repl hljs">julia&gt; m = (x = ([1.0], 2.0), y = [3.0]);

julia&gt; s = Optimisers.setup(Momentum(), m);

julia&gt; Optimisers.freeze!(s.x)

julia&gt; Optimisers.update!(s, m, (x = ([pi], 10pi), y = [100pi]));  # with fake gradient

julia&gt; m
(x = ([1.0], 2.0), y = [-0.14159265358979312])

julia&gt; s
(x = (Leaf(Momentum(0.01, 0.9), [0.0], frozen = true), ()), y = Leaf(Momentum(0.01, 0.9), [3.14159]))

julia&gt; Optimisers.thaw!(s)

julia&gt; s.x
(Leaf(Momentum(0.01, 0.9), [0.0]), ())</code></pre></div></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.thaw!" href="#Optimisers.thaw!"><code>Optimisers.thaw!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Optimisers.thaw!(tree)</code></pre><p>The reverse of <a href="#Optimisers.freeze!"><code>freeze!</code></a>. Applies to all parameters, mutating every <code>Leaf(rule, state, frozen = true)</code> to <code>Leaf(rule, state, frozen = false)</code>.</p></div></section></article><h2 id="Implicit-style-(Flux-0.14)"><a class="docs-heading-anchor" href="#Implicit-style-(Flux-0.14)">Implicit style (Flux ≤ 0.14)</a><a id="Implicit-style-(Flux-0.14)-1"></a><a class="docs-heading-anchor-permalink" href="#Implicit-style-(Flux-0.14)" title="Permalink"></a></h2><p>Flux used to handle gradients, training, and optimisation rules quite differently. The new style described above is called &quot;explicit&quot; by Zygote, and the old style &quot;implicit&quot;. Flux 0.13 and 0.14 are the transitional versions which support both; Flux 0.15 will remove the old.</p><div class="admonition is-compat"><header class="admonition-header">How to upgrade</header><div class="admonition-body"><p>The blue-green boxes in the <a href="../training/#man-training">training section</a> describe the changes needed to upgrade old code.</p></div></div><p>For full details on the interface for implicit-style optimisers, see the <a href="https://fluxml.ai/Flux.jl/v0.13.6/training/training/">Flux 0.13.6 manual</a>.</p><div class="admonition is-compat"><header class="admonition-header">Flux ≤ 0.12</header><div class="admonition-body"><p>Earlier versions of Flux exported <code>params</code>, thus allowing unqualified <code>params(model)</code> after <code>using Flux</code>. This conflicted with too many other packages, and was removed in Flux 0.13. If you get an error <code>UndefVarError: params not defined</code>, this probably means that you are following code for Flux 0.12 or earlier on a more recent version.</p></div></div><article class="docstring"><header><a class="docstring-binding" id="Flux.params" href="#Flux.params"><code>Flux.params</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">params(model)
params(layers...)</code></pre><p>Given a model or specific layers from a model, create a <code>Params</code> object pointing to its trainable parameters.</p><p>This can be used with the <code>gradient</code> function, see the <a href="../training/#man-training">training section of the manual</a>, or as input to the <a href="#Flux.Optimise.train!-NTuple{4, Any}"><code>Flux.train!</code></a> function.</p><p>The behaviour of <code>params</code> on custom types can be customized using <a href="../../models/functors/#Functors.@functor"><code>Functors.@functor</code></a> or <a href="../../destructure/#Optimisers.trainable"><code>Flux.trainable</code></a>.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; using Flux: params

julia&gt; params(Chain(Dense(ones(2,3)), softmax))  # unpacks Flux models
Params([[1.0 1.0 1.0; 1.0 1.0 1.0], [0.0, 0.0]])

julia&gt; bn = BatchNorm(2, relu)
BatchNorm(2, relu)  # 4 parameters, plus 4 non-trainable

julia&gt; params(bn)  # only the trainable parameters
Params([Float32[0.0, 0.0], Float32[1.0, 1.0]])

julia&gt; params([1, 2, 3], [4])  # one or more arrays of numbers
Params([[1, 2, 3], [4]])

julia&gt; params([[1, 2, 3], [4]])  # unpacks array of arrays
Params([[1, 2, 3], [4]])

julia&gt; params(1, [2 2], (alpha=[3,3,3], beta=Ref(4), gamma=sin))  # ignores scalars, unpacks NamedTuples
Params([[2 2], [3, 3, 3]])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/df468ba0a10d86b660eebed98d1b484cbce113a7/src/functor.jl#L91-L123">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Optimisers.update!-Tuple{Flux.Optimise.AbstractOptimiser, AbstractArray, Any}" href="#Optimisers.update!-Tuple{Flux.Optimise.AbstractOptimiser, AbstractArray, Any}"><code>Optimisers.update!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">update!(opt, p, g)
update!(opt, ps::Params, gs)</code></pre><p>Perform an update step of the parameters <code>ps</code> (or the single parameter <code>p</code>) according to optimiser <code>opt::AbstractOptimiser</code>  and the gradients <code>gs</code> (the gradient <code>g</code>).</p><p>As a result, the parameters are mutated and the optimiser&#39;s internal state may change. The gradient could be mutated as well.</p><div class="admonition is-compat"><header class="admonition-header">Deprecated</header><div class="admonition-body"><p>This method for implicit <code>Params</code> (and <code>AbstractOptimiser</code>) will be removed from Flux 0.15. The explicit method <code>update!(opt, model, grad)</code> from Optimisers.jl will remain.</p></div></div></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/df468ba0a10d86b660eebed98d1b484cbce113a7/src/optimise/train.jl#L8-L21">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Flux.Optimise.train!-Tuple{Any, Params, Any, Flux.Optimise.AbstractOptimiser}" href="#Flux.Optimise.train!-Tuple{Any, Params, Any, Flux.Optimise.AbstractOptimiser}"><code>Flux.Optimise.train!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">train!(loss, pars::Params, data, opt::AbstractOptimiser; [cb])</code></pre><p>Uses a <code>loss</code> function and training <code>data</code> to improve the  model&#39;s parameters according to a particular optimisation rule <code>opt</code>.</p><div class="admonition is-compat"><header class="admonition-header">Deprecated</header><div class="admonition-body"><p>This method with implicit <code>Params</code> will be removed from Flux 0.15. It should be replaced with the explicit method <code>train!(loss, model, data, opt)</code>.</p></div></div><p>For each <code>d in data</code>, first the gradient of the <code>loss</code> is computed like this:</p><pre><code class="nohighlight hljs">    gradient(() -&gt; loss(d...), pars)  # if d isa Tuple
    gradient(() -&gt; loss(d), pars)     # otherwise</code></pre><p>Here <code>pars</code> is produced by calling <a href="#Flux.params"><code>Flux.params</code></a> on your model. (Or just on the layers you want to train, like <code>train!(loss, params(model[1:end-2]), data, opt)</code>.) This is the &quot;implicit&quot; style of parameter handling.</p><p>This gradient is then used by optimiser <code>opt</code> to update the parameters:</p><pre><code class="nohighlight hljs">    update!(opt, pars, grads)</code></pre><p>The optimiser should be from the <code>Flux.Optimise</code> module (see <a href="training/@ref">Optimisers</a>). Different optimisers can be combined using <a href="../optimisers/#Flux.Optimise.Optimiser"><code>Flux.Optimise.Optimiser</code></a>.</p><p>This training loop iterates through <code>data</code> once. It will stop with a <code>DomainError</code> if the loss is <code>NaN</code> or infinite.</p><p>You can use use <code>train!</code> inside a for loop to do this several times, or  use for instance <code>Itertools.ncycle</code> to make a longer <code>data</code> iterator.</p><p><strong>Callbacks</strong></p><p><a href="#Callbacks">Callbacks</a> are given with the keyword argument <code>cb</code>. For example, this will print &quot;training&quot; every 10 seconds (using <a href="../callbacks/#Flux.throttle"><code>Flux.throttle</code></a>):</p><pre><code class="nohighlight hljs">    train!(loss, params, data, opt, cb = throttle(() -&gt; println(&quot;training&quot;), 10))</code></pre><p>Multiple callbacks can be passed to <code>cb</code> as array.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/FluxML/Flux.jl/blob/df468ba0a10d86b660eebed98d1b484cbce113a7/src/optimise/train.jl#L44-L85">source</a></section></article><h2 id="Callbacks"><a class="docs-heading-anchor" href="#Callbacks">Callbacks</a><a id="Callbacks-1"></a><a class="docs-heading-anchor-permalink" href="#Callbacks" title="Permalink"></a></h2><p>Implicit <code>train!</code> takes an additional argument, <code>cb</code>, that&#39;s used for callbacks so that you can observe the training process. For example:</p><pre><code class="language-julia hljs">train!(objective, ps, data, opt, cb = () -&gt; println(&quot;training&quot;))</code></pre><p>Callbacks are called for every batch of training data. You can slow this down using <code>Flux.throttle(f, timeout)</code> which prevents <code>f</code> from being called more than once every <code>timeout</code> seconds.</p><p>A more typical callback might look like this:</p><pre><code class="language-julia hljs">test_x, test_y = # ... create single batch of test data ...
evalcb() = @show(loss(test_x, test_y))
throttled_cb = throttle(evalcb, 5)
for epoch in 1:20
  @info &quot;Epoch $epoch&quot;
  Flux.train!(objective, ps, data, opt, cb = throttled_cb)
end</code></pre><p>See the page about <a href="../callbacks/#man-callback-helpers">callback helpers</a> for more.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../models/losses/">« Loss Functions</a><a class="docs-footer-nextpage" href="../optimisers/">Optimisation Rules »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Thursday 28 December 2023 18:12">Thursday 28 December 2023</span>. Using Julia version 1.10.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
