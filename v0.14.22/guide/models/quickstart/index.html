<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Quick Start · Flux</title><meta name="title" content="Quick Start · Flux"/><meta property="og:title" content="Quick Start · Flux"/><meta property="twitter:title" content="Quick Start · Flux"/><meta name="description" content="Documentation for Flux."/><meta property="og:description" content="Documentation for Flux."/><meta property="twitter:description" content="Documentation for Flux."/><script async src="https://www.googletagmanager.com/gtag/js?id=UA-36890222-9"></script><script>  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'UA-36890222-9', {'page_path': location.pathname + location.search + location.hash});
</script><script data-outdated-warner src="../../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../../assets/documenter.js"></script><script src="../../../search_index.js"></script><script src="../../../siteinfo.js"></script><script src="../../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../../assets/themeswap.js"></script><link href="../../../assets/flux.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../../"><img class="docs-light-only" src="../../../assets/logo.png" alt="Flux logo"/><img class="docs-dark-only" src="../../../assets/logo-dark.png" alt="Flux logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../../">Welcome</a></li><li><span class="tocitem">Guide</span><ul><li class="is-active"><a class="tocitem" href>Quick Start</a><ul class="internal"><li><a class="tocitem" href="#Features-to-Note"><span>Features to Note</span></a></li></ul></li><li><a class="tocitem" href="../overview/">Fitting a Line</a></li><li><a class="tocitem" href="../basics/">Gradients and Layers</a></li><li><a class="tocitem" href="../custom_layers/">Custom Layers</a></li><li><a class="tocitem" href="../../training/training/">Training</a></li><li><a class="tocitem" href="../recurrence/">Recurrence</a></li><li><a class="tocitem" href="../../gpu/">GPU Support</a></li><li><a class="tocitem" href="../../saving/">Saving &amp; Loading</a></li><li><a class="tocitem" href="../../performance/">Performance Tips</a></li></ul></li><li><a class="tocitem" href="../../../ecosystem/">Ecosystem</a></li><li><span class="tocitem">Reference</span><ul><li><a class="tocitem" href="../../../reference/models/layers/">Built-in Layers</a></li><li><a class="tocitem" href="../../../reference/models/activation/">Activation Functions</a></li><li><a class="tocitem" href="../../../reference/utilities/">Weight Initialisation</a></li><li><a class="tocitem" href="../../../reference/models/losses/">Loss Functions</a></li><li><a class="tocitem" href="../../../reference/training/reference/">Training API</a></li><li><a class="tocitem" href="../../../reference/training/optimisers/">Optimisation Rules</a></li><li><a class="tocitem" href="../../../reference/outputsize/">Shape Inference</a></li><li><a class="tocitem" href="../../../reference/destructure/">Flat vs. Nested</a></li><li><a class="tocitem" href="../../../reference/training/callbacks/">Callback Helpers</a></li><li><a class="tocitem" href="../../../reference/training/zygote/">Gradients – Zygote.jl</a></li><li><a class="tocitem" href="../../../reference/data/mlutils/">Batching Data – MLUtils.jl</a></li><li><a class="tocitem" href="../../../reference/data/onehot/">OneHotArrays.jl</a></li><li><a class="tocitem" href="../../../reference/models/nnlib/">Low-level Operations – NNlib.jl</a></li><li><a class="tocitem" href="../../../reference/models/functors/">Nested Structures – Functors.jl</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../../tutorials/linear_regression/">Linear Regression</a></li><li><a class="tocitem" href="../../../tutorials/logistic_regression/">Logistic Regression</a></li><li><a class="tocitem" href="../../../tutorials/model_zoo/">Model Zoo</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Guide</a></li><li class="is-active"><a href>Quick Start</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Quick Start</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/FluxML/Flux.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/FluxML/Flux.jl/blob/master/docs/src/guide/models/quickstart.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="man-quickstart"><a class="docs-heading-anchor" href="#man-quickstart">A Neural Network in One Minute</a><a id="man-quickstart-1"></a><a class="docs-heading-anchor-permalink" href="#man-quickstart" title="Permalink"></a></h1><p>If you have used neural networks before, then this simple example might be helpful for seeing how the major parts of Flux work together. Try pasting the code into the REPL prompt.</p><p>If you haven&#39;t, then you might prefer the <a href="../overview/">Fitting a Straight Line</a> page.</p><pre><code class="language-julia hljs"># This will prompt if neccessary to install everything, including CUDA:
using Flux, CUDA, Statistics, ProgressMeter

# Generate some data for the XOR problem: vectors of length 2, as columns of a matrix:
noisy = rand(Float32, 2, 1000)                                    # 2×1000 Matrix{Float32}
truth = [xor(col[1]&gt;0.5, col[2]&gt;0.5) for col in eachcol(noisy)]   # 1000-element Vector{Bool}

# Define our model, a multi-layer perceptron with one hidden layer of size 3:
model = Chain(
    Dense(2 =&gt; 3, tanh),   # activation function inside layer
    BatchNorm(3),
    Dense(3 =&gt; 2)) |&gt; gpu        # move model to GPU, if available

# The model encapsulates parameters, randomly initialised. Its initial output is:
out1 = model(noisy |&gt; gpu) |&gt; cpu                                 # 2×1000 Matrix{Float32}
probs1 = softmax(out1)      # normalise to get probabilities

# To train the model, we use batches of 64 samples, and one-hot encoding:
target = Flux.onehotbatch(truth, [true, false])                   # 2×1000 OneHotMatrix
loader = Flux.DataLoader((noisy, target) |&gt; gpu, batchsize=64, shuffle=true);
# 16-element DataLoader with first element: (2×64 Matrix{Float32}, 2×64 OneHotMatrix)

optim = Flux.setup(Flux.Adam(0.01), model)  # will store optimiser momentum, etc.

# Training loop, using the whole data set 1000 times:
losses = []
@showprogress for epoch in 1:1_000
    for (x, y) in loader
        loss, grads = Flux.withgradient(model) do m
            # Evaluate model and loss inside gradient context:
            y_hat = m(x)
            Flux.logitcrossentropy(y_hat, y)
        end
        Flux.update!(optim, model, grads[1])
        push!(losses, loss)  # logging, outside gradient context
    end
end

optim # parameters, momenta and output have all changed
out2 = model(noisy |&gt; gpu) |&gt; cpu  # first row is prob. of true, second row p(false)
probs2 = softmax(out2)      # normalise to get probabilities
mean((probs2[1,:] .&gt; 0.5) .== truth)  # accuracy 94% so far!</code></pre><p><img src="../../../assets/quickstart/oneminute.png" alt/></p><pre><code class="language-julia hljs">using Plots  # to draw the above figure

p_true = scatter(noisy[1,:], noisy[2,:], zcolor=truth, title=&quot;True classification&quot;, legend=false)
p_raw =  scatter(noisy[1,:], noisy[2,:], zcolor=probs1[1,:], title=&quot;Untrained network&quot;, label=&quot;&quot;, clims=(0,1))
p_done = scatter(noisy[1,:], noisy[2,:], zcolor=probs2[1,:], title=&quot;Trained network&quot;, legend=false)

plot(p_true, p_raw, p_done, layout=(1,3), size=(1000,330))</code></pre><img align="right" width="300px" src="../../assets/quickstart/loss.png"><p>Here&#39;s the loss during training:</p><pre><code class="language-julia hljs">plot(losses; xaxis=(:log10, &quot;iteration&quot;),
    yaxis=&quot;loss&quot;, label=&quot;per batch&quot;)
n = length(loader)
plot!(n:n:length(losses), mean.(Iterators.partition(losses, n)),
    label=&quot;epoch mean&quot;, dpi=200)</code></pre><p>This XOR (&quot;exclusive or&quot;) problem is a variant of the famous one which drove Minsky and Papert to invent deep neural networks in 1969. For small values of &quot;deep&quot; – this has one hidden layer, while earlier perceptrons had none. (What they call a hidden layer, Flux calls the output of the first layer, <code>model[1](noisy)</code>.)</p><p>Since then things have developed a little. </p><h2 id="Features-to-Note"><a class="docs-heading-anchor" href="#Features-to-Note">Features to Note</a><a id="Features-to-Note-1"></a><a class="docs-heading-anchor-permalink" href="#Features-to-Note" title="Permalink"></a></h2><p>Some things to notice in this example are:</p><ul><li><p>The batch dimension of data is always the last one. Thus a <code>2×1000 Matrix</code> is a thousand observations, each a column of length 2. Flux defaults to <code>Float32</code>, but most of Julia to <code>Float64</code>.</p></li><li><p>The <code>model</code> can be called like a function, <code>y = model(x)</code>. Each layer like <a href="../../../reference/models/layers/#Flux.Dense"><code>Dense</code></a> is an ordinary <code>struct</code>, which encapsulates some arrays of parameters (and possibly other state, as for <a href="../../../reference/models/layers/#Flux.BatchNorm"><code>BatchNorm</code></a>).</p></li><li><p>But the model does not contain the loss function, nor the optimisation rule. The momenta needed by <a href="../../../reference/training/optimisers/#Optimisers.Adam"><code>Adam</code></a> are stored in the object returned by <a href="../../../reference/training/reference/#Flux.Train.setup">setup</a>. And <a href="../../../reference/models/losses/#Flux.Losses.logitcrossentropy"><code>Flux.logitcrossentropy</code></a> is an ordinary function that combines the <a href="../../../reference/models/nnlib/#NNlib.softmax"><code>softmax</code></a> and <a href="../../../reference/models/losses/#Flux.Losses.crossentropy"><code>crossentropy</code></a> functions.</p></li><li><p>The <code>do</code> block creates an anonymous function, as the first argument of <code>gradient</code>. Anything executed within this is differentiated.</p></li></ul><p>Instead of calling <a href="../../../reference/training/zygote/#Zygote.gradient-Tuple{Any, Vararg{Any}}"><code>gradient</code></a> and <a href="../../../reference/training/reference/#Optimisers.update!"><code>update!</code></a> separately, there is a convenience function <a href="../../../reference/training/reference/#Flux.Optimise.train!-NTuple{4, Any}"><code>train!</code></a>. If we didn&#39;t want anything extra (like logging the loss), we could replace the training loop with the following:</p><pre><code class="language-julia hljs">for epoch in 1:1_000
    Flux.train!(model, loader, optim) do m, x, y
        y_hat = m(x)
        Flux.logitcrossentropy(y_hat, y)
    end
end</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../../">« Welcome</a><a class="docs-footer-nextpage" href="../overview/">Fitting a Line »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.7.0 on <span class="colophon-date" title="Saturday 12 October 2024 14:11">Saturday 12 October 2024</span>. Using Julia version 1.11.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
